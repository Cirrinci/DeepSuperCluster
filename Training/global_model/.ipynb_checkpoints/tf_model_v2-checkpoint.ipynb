{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing setGPU\n",
      "Could not import setGPU, please make sure you configure CUDA_VISIBLE_DEVICES manually\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import glob\n",
    "try:\n",
    "    if not (\"CUDA_VISIBLE_DEVICES\" in os.environ):\n",
    "        os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "        print(\"importing setGPU\")\n",
    "        import setGPU\n",
    "except:\n",
    "    print(\"Could not import setGPU, please make sure you configure CUDA_VISIBLE_DEVICES manually\")\n",
    "    pass\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import io\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import tensorflow as tf\n",
    "from numpy.lib.recfunctions import append_fields\n",
    "\n",
    "import scipy\n",
    "import scipy.special\n",
    "\n",
    "from mpnn import MessagePassing, ReadoutGraph, Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version=2.1.0, CUDA=True, GPU=True, TPU=False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json, os\n",
    "import numpy as np\n",
    "\n",
    "# Tested with TensorFlow 2.1.0\n",
    "print('version={}, CUDA={}, GPU={}, TPU={}'.format(\n",
    "    tf.__version__, tf.test.is_built_with_cuda(),\n",
    "    # GPU attached?\n",
    "    len(tf.config.list_physical_devices('GPU')) > 0,\n",
    "    # TPU accessible? (only works on Colab)\n",
    "    'COLAB_TPU_ADDR' in os.environ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpus= 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    num_gpus = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\"))\n",
    "    print(\"num_gpus=\", num_gpus)\n",
    "    if num_gpus > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(\"gpu:0\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"fallback to CPU\")\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(A,B):\n",
    "    na = tf.reduce_sum(tf.square(A), -1)\n",
    "    nb = tf.reduce_sum(tf.square(B), -1)\n",
    " \n",
    "    na = tf.reshape(na, [tf.shape(na)[0], -1, 1])\n",
    "    nb = tf.reshape(nb, [tf.shape(na)[0], 1, -1])\n",
    "    Dsq = tf.clip_by_value(na - 2*tf.linalg.matmul(A, B, transpose_a=False, transpose_b=True) + nb, 1e-12, 1e12)\n",
    "    D = tf.sqrt(Dsq)\n",
    "    return D\n",
    "\n",
    "\n",
    "#Given a list of [Nbatch, Nelem, Nfeat] input nodes, computes the dense [Nbatch, Nelem, Nelem] adjacency matrices\n",
    "class Distance(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, dist_shape, *args, **kwargs):\n",
    "        super(Distance, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs1, inputs2):\n",
    "        #compute the pairwise distance matrix between the vectors defined by the first two components of the input array\n",
    "        #inputs1, inputs2: [Nbatch, Nelem, distance_dim] embedded coordinates used for element-to-element distance calculation\n",
    "        D = dist(inputs1, inputs2)\n",
    "      \n",
    "        #adjacency between two elements should be high if the distance is small.\n",
    "        #this is equivalent to radial basis functions. \n",
    "        #self-loops adj_{i,i}=1 are included, as D_{i,i}=0 by construction\n",
    "        adj = tf.math.exp(-1.0*D)\n",
    "\n",
    "        #optionally set the adjacency matrix to 0 for low values in order to make the matrix sparse.\n",
    "        #need to test if this improves the result.\n",
    "        #adj = tf.keras.activations.relu(adj, threshold=0.01)\n",
    "\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_classes):\n",
    "        super(InputEncoding, self).__init__()\n",
    "        self.num_input_classes = num_input_classes\n",
    "        \n",
    "    def call(self, X):\n",
    "        #X: [Nbatch, Nelem, Nfeat] array of all the input detector element feature data\n",
    "\n",
    "        #X[:, :, 0] - categorical index of the element type\n",
    "        Xid = tf.one_hot(tf.cast(X[:, :, 0], tf.int32), self.num_input_classes)\n",
    "\n",
    "        #X[:, :, 1:] - all the other non-categorical features\n",
    "        Xprop = X[:, :, 1:]\n",
    "        return tf.concat([Xid, Xprop], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph Highway network\n",
    "# https://arxiv.org/pdf/2004.04635.pdf\n",
    "#https://github.com/gcucurull/jax-ghnet/blob/master/models.py \n",
    "class GHConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, k, *args, **kwargs):\n",
    "        self.activation = kwargs.pop(\"activation\")\n",
    "        self.hidden_dim = args[0]\n",
    "        self.k = k\n",
    "\n",
    "        super(GHConv, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.W_t = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"w_t\", initializer=\"random_normal\")\n",
    "        self.b_t = self.add_weight(shape=(self.hidden_dim, ), name=\"b_t\", initializer=\"zeros\")\n",
    "        self.W_h = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"w_h\", initializer=\"random_normal\")\n",
    "        self.theta = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"theta\", initializer=\"random_normal\")\n",
    " \n",
    "    def call(self, x, adj):\n",
    "        #compute the normalization of the adjacency matrix\n",
    "        in_degrees = tf.reduce_sum(adj, axis=-1)\n",
    "        #add epsilon to prevent numerical issues from 1/sqrt(x)\n",
    "        norm = tf.expand_dims(tf.pow(in_degrees + 1e-6, -0.5), -1)\n",
    "        norm_k = tf.pow(norm, self.k)\n",
    "        adj_k = tf.pow(adj, self.k)\n",
    "\n",
    "        f_hom = tf.linalg.matmul(x, self.theta)\n",
    "        f_hom = tf.linalg.matmul(adj_k, f_hom*norm_k)*norm_k\n",
    "\n",
    "        f_het = tf.linalg.matmul(x, self.W_h)\n",
    "        gate = tf.nn.sigmoid(tf.linalg.matmul(x, self.W_t) + self.b_t)\n",
    "        #tf.print(tf.reduce_mean(f_hom), tf.reduce_mean(f_het), tf.reduce_mean(gate))\n",
    "\n",
    "        out = gate*f_hom + (1-gate)*f_het\n",
    "        return out\n",
    "\n",
    "## Simple Graph Conv layer\n",
    "class SGConv(tf.keras.layers.Dense):\n",
    "    def __init__(self, k, *args, **kwargs):\n",
    "        super(SGConv, self).__init__(*args, **kwargs)\n",
    "        self.k = k\n",
    "    \n",
    "    def call(self, inputs, adj):\n",
    "        W = self.weights[0]\n",
    "        b = self.weights[1]\n",
    "\n",
    "        #compute the normalization of the adjacency matrix\n",
    "        in_degrees = tf.reduce_sum(adj, axis=-1)\n",
    "        #add epsilon to prevent numerical issues from 1/sqrt(x)\n",
    "        norm = tf.expand_dims(tf.pow(in_degrees + 1e-6, -0.5), -1)\n",
    "        norm_k = tf.pow(norm, self.k)\n",
    "\n",
    "        support = (tf.linalg.matmul(inputs, W))\n",
    "     \n",
    "        #k-th power of the normalized adjacency matrix is nearly equivalent to k consecutive GCN layers\n",
    "        adj_k = tf.pow(adj, self.k)\n",
    "        out = tf.linalg.matmul(adj_k, support*norm_k)*norm_k\n",
    "\n",
    "        return self.activation(out + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple message passing based on a matrix multiplication\n",
    "class DNNSuperCluster(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, activation=tf.nn.selu, nclass_labels=2, \n",
    "                     hidden_dim_coord=256, hidden_dim_input=256, hidden_dim_id=256,     \n",
    "                     n_layers_input=2, n_layers_id=3, n_layers_coord=2,\n",
    "                     distance_dim=256, num_conv=4, convlayer=\"ghconv\", dropout=0.1):\n",
    "        super(DNNSuperCluster, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.nclass_labels = nclass_labels\n",
    "\n",
    "        #self.enc = InputEncoding(3)\n",
    "        \n",
    "        # layers for distance coordinate extraction\n",
    "        self.layers_coord = [ ]\n",
    "        for i in range(n_layers_coord):\n",
    "            layer_coord_i = tf.keras.layers.Dense(hidden_dim_coord, activation=activation, name=\"disctcoords_\"+str(i))\n",
    "            self.layers_coord.append(layer_coord_i)\n",
    "\n",
    "        self.layer_distcoords = tf.keras.layers.Dense(distance_dim, activation=\"linear\", name=\"distcoords_final\")\n",
    "        self.layer_distance = Distance(distance_dim, name=\"distance\")\n",
    "\n",
    "        # layers for feature extraction \n",
    "        self.layers_input = [ ]\n",
    "        for i in range(n_layers_input):\n",
    "            layer_input_i = tf.keras.layers.Dense(hidden_dim_input, activation=activation, name=\"input_\"+str(i))\n",
    "            layer_input_i_do = tf.keras.layers.Dropout(dropout)\n",
    "            self.layers_input.append((layer_input_i, layer_input_i_do))\n",
    "        \n",
    "       \n",
    "\n",
    "        # Graph convolutions\n",
    "        if convlayer == \"sgconv\":\n",
    "            self.layer_conv1 = SGConv(num_conv, hidden_dim_input, activation=activation, name=\"conv1\")\n",
    "            #self.layer_conv2 = SGConv(num_conv, 2*hidden_dim+len(class_labels), activation=activation, name=\"conv2\")\n",
    "        elif convlayer == \"ghconv\":\n",
    "            self.layer_conv1 = GHConv(num_conv, hidden_dim_input, activation=activation, name=\"conv1\")\n",
    "            #self.layer_conv2 = GHConv(num_conv, 2*hidden_dim+len(class_labels), activation=activation, name=\"conv2\")\n",
    "\n",
    "        # Output layers\n",
    "        self.layers_id = [ ]\n",
    "        for i in range(n_layers_id):\n",
    "            layer_id_i = tf.keras.layers.Dense(hidden_dim_id, activation=activation, name=\"id_\"+str(i))\n",
    "            #layer_id_i_do = tf.keras.layers.Dropout(dropout)\n",
    "            self.layers_id.append(layer_id_i)\n",
    "            \n",
    "        #self.layer_id3 = tf.keras.layers.Dense(hidden_dim, activation=activation, name=\"id3\")\n",
    "        self.layer_id = tf.keras.layers.Dense(nclass_labels, activation=\"linear\", name=\"out_id\")\n",
    "        \n",
    " \n",
    "    def predict_distancematrix(self, inputs, msk_elem, training=True):\n",
    "        x = inputs\n",
    "        for layer_coord in self.layers_coord:\n",
    "            x = layer_coord(x)\n",
    "\n",
    "        distcoords = self.layer_distcoords(x)\n",
    "\n",
    "        dm = self.layer_distance(distcoords, distcoords)\n",
    "        \n",
    "        dm = dm*msk_elem\n",
    "\n",
    "        return dm\n",
    "\n",
    "    #@tf.function(input_signature=[tf.TensorSpec(shape=[None, 15], dtype=tf.float32)])\n",
    "    def call(self, inputs, training=True):\n",
    "        # separate cluster energies from rescaled inputs\n",
    "        X = inputs[:,:,1:]\n",
    "        cl_energies = inputs[:,:,0]\n",
    "        \n",
    "        # the 4th element in the scaled input is the en_cluster. used to check for the mask\n",
    "        msk_input = tf.expand_dims(tf.cast(X[:, :, 3] != 0., tf.float32), -1)\n",
    "\n",
    "        dm = self.predict_distancematrix(X, msk_elem=msk_input, training=training)\n",
    "        \n",
    "        x = X\n",
    "        for layer_input, layer_input_do in self.layers_input:\n",
    "            x = layer_input(x)\n",
    "            x = layer_input_do(x, training)\n",
    "            \n",
    "        x = self.layer_conv1(x, dm)\n",
    "        \n",
    "        for layer_id in self.layers_id:\n",
    "            x = layer_id(x)\n",
    "            #x = layer_id_do(x, training)\n",
    "            \n",
    "        out_id_logits = self.layer_id(x)\n",
    "        \n",
    "        energies = tf.expand_dims(cl_energies, axis=-1)\n",
    "        # add the cluster energies in the output, in the future we can add here corrections\n",
    "        output = tf.concat([out_id_logits,energies], axis=-1)\n",
    "        # mask to 0 the padded output\n",
    "        output_masked = output * msk_input\n",
    "        \n",
    "        #return masked output logits and the predicted total energy\n",
    "        return output_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def separate_true(y, nclass_labels):\n",
    "    # one-hot encoding for true label (signal,PU,noise)\n",
    "    # the padded elements have -1 so they are one_hot to (0,0)\n",
    "    y_onehot = tf.one_hot(tf.cast(y[:,1:], tf.int32), nclass_labels)\n",
    "    true_en = y[:,0]\n",
    "    return y_onehot, true_en\n",
    "\n",
    "#@tf.function\n",
    "def true_mask(y):\n",
    "    # mask for elements that should be included in supercluster\n",
    "    in_sc = tf.cast(y[:,1:] == 1., tf.float32)\n",
    "    # number of padding elements\n",
    "    padded = tf.reduce_sum(tf.cast(y[:,1:] == -1., tf.float32), axis=-1)\n",
    "    return in_sc, padded\n",
    "    \n",
    "#@tf.function\n",
    "def separate_pred(ypred):\n",
    "    ens = ypred[:,:,2]\n",
    "    ypred_onehot = ypred[:,:,:2]\n",
    "    # 0 not include in energy sum, 1 include in energy sum\n",
    "    # masked elements have pred_id=0 so they do not enter in the energy sum\n",
    "    pred_id = tf.cast(tf.argmax(ypred_onehot, axis=-1), tf.float32)\n",
    "    # predicted total energy\n",
    "    pred_en =  tf.reduce_sum( ens * pred_id, axis=-1)\n",
    "    # one-hot encoding for true label (signal,PU,noise)\n",
    "    return ypred_onehot, pred_en, pred_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def mse_unreduced(true, pred):\n",
    "    return tf.math.pow(true-pred,2)\n",
    "\n",
    "#@tf.function\n",
    "def msle_unreduced(true, pred):\n",
    "    return tf.math.pow(tf.math.log(tf.math.abs(true) + 1.0) - tf.math.log(tf.math.abs(pred) + 1.0), 2)\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def my_loss_full(y_true, y_pred):\n",
    "    y_true_onehot, true_en = separate_true(y_true, args.nclass_labels)\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    # since the padded y_true is -1 -> it gives [0,0] when it is onehot. The ypred for batched is [0,0] so the loss\n",
    "    # is automatically 0 for padded samples\n",
    "    l1 = tf.nn.softmax_cross_entropy_with_logits(y_true_onehot, y_pred_onehot)\n",
    "    \n",
    "    # true energy loss\n",
    "    mask_outsc = tf.cast(true_en == 0., tf.float32)\n",
    "    mask_insc = tf.cast(true_en != 0., tf.float32)\n",
    "    n_outsc = tf.reduce_sum(mask_outsc)\n",
    "    n_insc = tf.reduce_sum(mask_insc)\n",
    "    \n",
    "    l2_en = mse_unreduced(true_en, pred_en)\n",
    "    l2_en_log = msle_unreduced(true_en, pred_en)\n",
    "    \n",
    "    # separate mean resolution for windows with Caloparticle or not\n",
    "    l2_en_outsc = tf.reduce_sum(l2_en * mask_outsc) / n_outsc\n",
    "    l2_en_insc = tf.reduce_sum(l2_en * mask_insc) / n_insc\n",
    "    l2_en_outsc_log = tf.reduce_sum(l2_en_log * mask_outsc) / n_outsc\n",
    "    l2_en_insc_log = tf.reduce_sum(l2_en_log * mask_insc) / n_insc\n",
    "    \n",
    "    ltot = 2e4*tf.reduce_mean(l1) + 20* l2_en_insc +   10*l2_en_outsc + 200* l2_en_insc_log  + 100* l2_en_outsc_log\n",
    "    \n",
    "    return ltot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_resolution_outsc(y_true, y_pred):\n",
    "    y_true_onehot, true_en = separate_true(y_true, args.nclass_labels)\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_outsc = tf.cast(true_en == 0., tf.float32)\n",
    "    n_outsc = tf.reduce_sum(mask_outsc)\n",
    "    return tf.reduce_sum(mse_unreduced(true_en, pred_en)*mask_outsc) / n_outsc\n",
    "\n",
    "def energy_resolution_insc(y_true, y_pred):\n",
    "    y_true_onehot, true_en = separate_true(y_true, args.nclass_labels)\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_insc = tf.cast(true_en != 0., tf.float32)\n",
    "    n_insc = tf.reduce_sum(mask_insc)\n",
    "    return tf.reduce_sum(mse_unreduced(true_en, pred_en)*mask_insc) / n_insc\n",
    "\n",
    "def energy_resolution_outsc_log(y_true, y_pred):\n",
    "    y_true_onehot, true_en = separate_true(y_true, args.nclass_labels)\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_outsc = tf.cast(true_en == 0., tf.float32)\n",
    "    n_outsc = tf.reduce_sum(mask_outsc)\n",
    "    return tf.reduce_sum(msle_unreduced(true_en, pred_en)*mask_outsc) / n_outsc\n",
    "\n",
    "def energy_resolution_insc_log(y_true, y_pred):\n",
    "    y_true_onehot, true_en = separate_true(y_true, args.nclass_labels)\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_insc = tf.cast(true_en != 0., tf.float32)\n",
    "    n_insc = tf.reduce_sum(mask_insc)\n",
    "    return tf.reduce_sum(msle_unreduced(true_en, pred_en)*mask_insc) / n_insc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpfn_metrics(y_true, y_pred):\n",
    "    y_true_mask, n_padded = true_mask(y_true)\n",
    "    y_false_mask = (tf.cast(y_true_mask == 0., tf.float32))\n",
    "    \n",
    "    # pred_id contains the last n_padded elements to 0 that will be always True negatives\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    \n",
    "    n_pos = tf.reduce_sum(y_true_mask, axis=-1)\n",
    "    n_neg = tf.reduce_sum(y_false_mask, axis=-1) - n_padded\n",
    "    \n",
    "    n_tot = n_neg + n_pos\n",
    "    \n",
    "    true_pos = tf.reduce_sum(pred_id * y_true_mask, axis=-1)\n",
    "    false_neg = n_pos - true_pos\n",
    "    \n",
    "    false_pos = tf.reduce_sum(pred_id * y_false_mask, axis=-1)\n",
    "    true_neg = n_neg - false_pos\n",
    "    \n",
    "    return n_tot, true_pos, false_neg, false_pos, true_neg, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp,tn,fp,fn):\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def recall(tp,tn,fp,fn):\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def accuracy(tp,tn,fp,fn):\n",
    "    return (tp+tn)/(tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Precision(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='precision', **kwargs):\n",
    "        super(Precision, self).__init__(name=name, **kwargs)\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        n_tot, true_pos, false_neg, false_pos, true_neg = get_tpfn_metrics(y_true, y_pred)\n",
    "        self.tp.assign_add(tf.reduce_sum(true_pos))\n",
    "        self.fp.assign_add(tf.reduce_sum(false_pos))\n",
    "\n",
    "    def result(self):\n",
    "        return self.tp / (self.tp + self.fp)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fp.assign(0)\n",
    "        \n",
    "class Recall(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='recall', **kwargs):\n",
    "        super(Recall, self).__init__(name=name, **kwargs)\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        n_tot, true_pos, false_neg, false_pos, true_neg = get_tpfn_metrics(y_true, y_pred)\n",
    "        self.tp.assign_add(tf.reduce_sum(true_pos))\n",
    "        self.fn.assign_add(tf.reduce_sum(false_neg))\n",
    "\n",
    "    def result(self):\n",
    "        return self.tp / (self.tp + self.fn)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fn.assign(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/storage/ECAL/training_data/window_data/electrons/recordio_v4\"\n",
    "models_path = \"/storage/ECAL/deepcluster/models/gcn_models_v9/\"\n",
    "\n",
    "#rain_steps_per_epoch = \n",
    "#eval_steps_per_epoch = 3e5 // batch_size\n",
    "from collections import namedtuple\n",
    "Args = namedtuple('args', [ 'models_path', 'load','nepochs','ntrain','nval','nfeatures',\n",
    "                            'n_seed_features','batch_size','lr_decay','lr',\n",
    "                            'hidden_dim_input','hidden_dim_coord', 'hidden_dim_id',\n",
    "                            'n_layers_input', 'n_layers_id', 'n_layers_coord',\n",
    "                           'distance_dim','num_conv','dropout','convlayer',\n",
    "                           'nclass_labels', 'opt'])\n",
    "\n",
    "args = Args( \n",
    "models_path = models_path,\n",
    "load = False,\n",
    "nepochs = 100,\n",
    "ntrain = 1000000,\n",
    "nval = 200000,\n",
    "nfeatures = 13,\n",
    "n_seed_features = 12,\n",
    "lr_decay = 0.5,\n",
    "lr = 1e-4,\n",
    "batch_size = 70,\n",
    "n_layers_input = 3,\n",
    "n_layers_id = 3,\n",
    "n_layers_coord = 3,\n",
    "hidden_dim_input = 200,\n",
    "hidden_dim_coord = 200,\n",
    "hidden_dim_id = 200,\n",
    "distance_dim = 200,\n",
    "num_conv = 2,\n",
    "dropout = 0.2,\n",
    "convlayer = 'sgconv',\n",
    "nclass_labels=2,\n",
    "opt='adam'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features_clusters(X):\n",
    "    '''\n",
    "    'is_seed',\"cluster_deta\", \"cluster_dphi\", \"en_cluster\", \"et_cluster\",\n",
    "    \"cl_f5_r9\", \"cl_f5_sigmaIetaIeta\",\"cl_f5_sigmaIetaIphi\",\"cl_f5_sigmaIphiIphi\",\n",
    "    \"cl_f5_swissCross\", \"cl_nxtals\", \"cl_etaWidth\", \"cl_phiWidth\n",
    "    '''\n",
    "    x_mean = tf.constant( \n",
    "        [   0.,  -7.09402501e-04, -1.27142875e-04,  1.30375508e+00,  5.67249500e-01, \n",
    "            1.92096066e+00,  1.31476120e-02,  1.62948213e-05,  1.42948806e-02,\n",
    "            5.92920497e-01,  1.49597644e+00,  3.36213188e-03,  3.06446267e-03]\n",
    "        )\n",
    "\n",
    "    x_scale = tf.constant(\n",
    "        [  1.,  1.10279784e-01, 3.30488055e-01, 2.62605247e+00, 1.16284769e+00,\n",
    "            7.81094814e+00, 1.70392176e-02, 3.05995567e-04, 1.80176053e-02,\n",
    "            1.99316624e+00, 1.88845046e+00, 4.12315715e-03, 4.79639033e-03]       \n",
    "        )\n",
    "    return (X-x_mean)/ x_scale\n",
    "\n",
    "def scale_features_seed(X):\n",
    "    '''\n",
    "     \"seed_eta\", \"seed_iz\",\"en_seed\",\"et_seed\",\n",
    "     \"seed_f5_r9\", \"seed_f5_sigmaIetaIeta\",\"seed_f5_sigmaIetaIphi\",\"seed_f5_sigmaIphiIphi\",\n",
    "     \"seed_f5_swissCross\",\"seed_nxtals\", \"seed_etaWidth\", \"seed_phiWidth\",\n",
    "    '''\n",
    "    x_mean = tf.constant( \n",
    "        [   6.84241156e-03,  1.62242679e-03,  5.81495577e+01,  2.57215845e+01, \n",
    "            1.00772582e+00,  1.35803461e-02, -4.29317013e-06,  1.71072024e-02,\n",
    "            4.90466869e-01,  5.10511982e+00,  8.82101138e-03,  1.04095965e-02 ]\n",
    "    \n",
    "        )\n",
    "\n",
    "    x_scale = tf.constant(\n",
    "        [   1.31333380e+00, 5.06988411e-01, 9.21157365e+01, 2.98580765e+01, \n",
    "            1.17047757e-01, 1.11969442e-02, 1.86572967e-04, 1.31036359e-02,\n",
    "            4.01511744e-01, 5.67007350e+00, 6.14304203e-03, 7.24808860e-03]       \n",
    "        )\n",
    "    return (X-x_mean)/ x_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_tfr_element(element):\n",
    "    parse_dic = {\n",
    "        'X':      tf.io.FixedLenFeature([], tf.string),\n",
    "        'X_seed': tf.io.FixedLenFeature([], tf.string),\n",
    "        'y':      tf.io.FixedLenFeature([], tf.string),\n",
    "        'n_clusters': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example_message = tf.io.parse_single_example(element, parse_dic)\n",
    "\n",
    "    X = example_message['X']\n",
    "    X_seed = example_message['X_seed']\n",
    "    y = example_message['y']\n",
    "    nclusters = example_message['n_clusters']\n",
    "    \n",
    "    arr_X = tf.io.parse_tensor(X, out_type=tf.float32)\n",
    "    arr_X_seed = tf.io.parse_tensor(X_seed, out_type=tf.float32)\n",
    "    arr_y = tf.io.parse_tensor(y, out_type=tf.float32)\n",
    "    \n",
    "    #https://github.com/tensorflow/tensorflow/issues/24520#issuecomment-577325475\n",
    "    arr_X.set_shape(     tf.TensorShape((None, args.nfeatures)))\n",
    "    arr_X_seed.set_shape(tf.TensorShape((1, args.n_seed_features)))\n",
    "    arr_y.set_shape(     tf.TensorShape((None,)))\n",
    " \n",
    "    return arr_X, arr_X_seed, nclusters, arr_y\n",
    "  \n",
    "def _stack_seed_features(arr_X, arr_X_seed, nclusters, arr_y):\n",
    "    en_clusters = tf.expand_dims(arr_X[:,3], axis=-1)\n",
    "    rescaled_X = scale_features_clusters(arr_X)\n",
    "    rescaled_X_seed = scale_features_seed(arr_X_seed)\n",
    "    X = tf.concat([en_clusters, rescaled_X, tf.broadcast_to(rescaled_X_seed,[nclusters,rescaled_X_seed.shape[1]] )],\n",
    "                  axis=1)\n",
    "    return X,arr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# padding shape\n",
    "ps = ([None,args.nfeatures+args.n_seed_features+1],[None,])\n",
    "\n",
    "# Create datasets from TFRecord files.\n",
    "dataset = tf.data.TFRecordDataset(tf.io.gfile.glob('{}/training-*'.format(data_path)))\n",
    "dataset = dataset.map(_parse_tfr_element,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.map(_stack_seed_features,num_parallel_calls=tf.data.experimental.AUTOTUNE) # deterministic=False in TFv2.3\n",
    "dataset = dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
    "\n",
    "ds_train = dataset.take(args.ntrain).padded_batch(args.batch_size, padded_shapes=ps, drop_remainder=True,padding_values=(0.,-1.))\n",
    "ds_test = dataset.skip(args.ntrain).take(args.nval).padded_batch(args.batch_size, padded_shapes=ps, drop_remainder=True,padding_values=(0.,-1.))\n",
    "\n",
    "ds_train_r = ds_train.repeat(args.nepochs)\n",
    "ds_test_r = ds_test.repeat(args.nepochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = iter(ds_train)\n",
    "d = next(idata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_input = tf.expand_dims(tf.cast(d[0][:, :, 3] != 0., tf.float32), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_run():\n",
    "    previous_runs = os.listdir(args.models_path)\n",
    "    if len(previous_runs) == 0:\n",
    "        run_number = 1\n",
    "    else:\n",
    "        run_number = max([int(s.split('run_')[1]) for s in previous_runs]) + 1\n",
    "    return run_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.lr_decay > 0:\n",
    "        #lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        #    args.lr,\n",
    "        #    decay_steps=int(args.ntrain//args.batch_size),\n",
    "        #    decay_rate=args.lr_decay\n",
    "        #)\n",
    "        lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "            args.lr,\n",
    "            decay_steps=int(args.ntrain//args.batch_size),\n",
    "            decay_rate=args.lr_decay\n",
    "        )\n",
    "else:\n",
    "    lr_schedule = args.lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    model = DNNSuperCluster(hidden_dim_input=args.hidden_dim_input,hidden_dim_coord=args.hidden_dim_coord,\n",
    "                            hidden_dim_id=args.hidden_dim_id, \n",
    "                            n_layers_input=args.n_layers_input, n_layers_id=args.n_layers_id, n_layers_coord=args.n_layers_coord,\n",
    "                            nclass_labels=args.nclass_labels, \n",
    "                            distance_dim=args.distance_dim, \n",
    "                            num_conv=args.num_conv, convlayer=args.convlayer, dropout=args.dropout)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/ECAL/deepcluster/models/gcn_models_v9/run_22\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(args.models_path):\n",
    "    os.makedirs(args.models_path)\n",
    "\n",
    "name =  'run_{:02}'.format(get_unique_run())\n",
    "\n",
    "outdir = args.models_path + name\n",
    "\n",
    "if os.path.isdir(outdir):\n",
    "    print(\"Output directory exists: {}\".format(outdir), file=sys.stderr)\n",
    "\n",
    "print(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "tb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=outdir, histogram_freq=2, \n",
    "    write_graph=False, \n",
    "    write_images=True,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0,\n",
    ")\n",
    "tb.set_model(model)\n",
    "callbacks += [tb]\n",
    "\n",
    "terminate_cb = tf.keras.callbacks.TerminateOnNaN()\n",
    "callbacks += [terminate_cb]\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=outdir + \"/weights.{epoch:02d}-{val_loss:.6f}.hdf5\",\n",
    "    save_weights_only=True,\n",
    "    verbose=0\n",
    ")\n",
    "cp_callback.set_model(model)\n",
    "callbacks += [cp_callback]\n",
    "\n",
    "loss_fn = my_loss_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model.compile(optimizer=args.opt, loss=loss_fn,\n",
    "        metrics=[Precision(),Recall(), energy_resolution_insc,energy_resolution_outsc,\n",
    "                     energy_resolution_insc_log,energy_resolution_outsc_log,])\n",
    "\n",
    "    for X, y in ds_train:\n",
    "        ypred = model(X)\n",
    "        l = loss_fn(y, ypred)\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoh,true_en = separate_true(y, args.nclass_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(70, 18, 3), dtype=float32, numpy=\n",
       "array([[[-1.2423130e+00, -8.3294290e-01,  2.4955025e+00],\n",
       "        [-2.2360535e-01,  5.4161513e-01,  1.4691534e+00],\n",
       "        [-4.8525473e-01, -1.3082337e+00,  1.4982151e+00],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[-3.9056835e+00,  5.4643817e-02,  7.6484070e+01],\n",
       "        [-2.5258619e-01,  2.5784402e+00,  5.5179367e+00],\n",
       "        [-5.2430302e-01,  9.2965436e-01,  1.8009961e+00],\n",
       "        ...,\n",
       "        [ 3.3698130e-01, -2.0828109e+00,  3.1105274e-01],\n",
       "        [ 1.3320040e+00, -6.4202666e-01,  2.3348083e-01],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[-2.4870245e+00, -2.4266033e+00,  7.0290855e+01],\n",
       "        [-1.7524647e+00,  2.5555265e+00,  1.1542038e+01],\n",
       "        [ 8.5908633e-01, -1.0591314e+00,  6.2240553e-01],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-5.2153176e-01,  1.0732423e+00,  1.4401750e+01],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[-4.7483978e+00,  5.4972506e-01,  7.4594048e+01],\n",
       "        [-2.3577073e+00,  1.7990663e+00,  3.4291205e+00],\n",
       "        [-1.5349730e+00, -3.8787588e-01,  2.0304086e+00],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[-5.3643618e+00, -1.1391933e+00,  1.7532779e+02],\n",
       "        [-2.8033006e-01,  1.6778904e+00,  9.0097189e+00],\n",
       "        [-3.2091677e+00, -7.6268041e-01,  7.8461666e+00],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dnn_super_cluster\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "disctcoords_0 (Dense)        multiple                  5200      \n",
      "_________________________________________________________________\n",
      "disctcoords_1 (Dense)        multiple                  40200     \n",
      "_________________________________________________________________\n",
      "disctcoords_2 (Dense)        multiple                  40200     \n",
      "_________________________________________________________________\n",
      "distcoords_final (Dense)     multiple                  40200     \n",
      "_________________________________________________________________\n",
      "distance (Distance)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_0 (Dense)              multiple                  5200      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_1 (Dense)              multiple                  40200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_2 (Dense)              multiple                  40200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1 (SGConv)               multiple                  40200     \n",
      "_________________________________________________________________\n",
      "id_0 (Dense)                 multiple                  40200     \n",
      "_________________________________________________________________\n",
      "id_1 (Dense)                 multiple                  40200     \n",
      "_________________________________________________________________\n",
      "id_2 (Dense)                 multiple                  40200     \n",
      "_________________________________________________________________\n",
      "out_id (Dense)               multiple                  402       \n",
      "=================================================================\n",
      "Total params: 372,602\n",
      "Trainable params: 372,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 14285 steps, validate for 2857 steps\n",
      "Epoch 1/100\n",
      " 1771/14285 [==>...........................] - ETA: 6:34 - loss: 2490.5507 - precision: 0.9007 - recall: 0.8113 - energy_resolution_insc: 73.9431 - energy_resolution_outsc: 18.2733 - energy_resolution_insc_log: 0.2330 - energy_resolution_outsc_log: 0.1964"
     ]
    }
   ],
   "source": [
    "if args.load:\n",
    "    #ensure model input size is known\n",
    "    for X, y in ds_train:\n",
    "        model(X)\n",
    "        break\n",
    "\n",
    "    model.load_weights(args.load)\n",
    "if args.nepochs > 0:\n",
    "    ret = model.fit(ds_train_r,\n",
    "        validation_data=ds_test_r, epochs=args.nepochs,\n",
    "        steps_per_epoch=args.ntrain//args.batch_size, validation_steps=args.nval//args.batch_size,\n",
    "        verbose=True,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir + \"/args.txt\",'w') as config:\n",
    "    config.write(str(args))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing setGPU\n",
      "Could not import setGPU, please make sure you configure CUDA_VISIBLE_DEVICES manually\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import glob\n",
    "try:\n",
    "    if not (\"CUDA_VISIBLE_DEVICES\" in os.environ):\n",
    "        os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "        print(\"importing setGPU\")\n",
    "        import setGPU\n",
    "except:\n",
    "    print(\"Could not import setGPU, please make sure you configure CUDA_VISIBLE_DEVICES manually\")\n",
    "    pass\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import io\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import tensorflow as tf\n",
    "from numpy.lib.recfunctions import append_fields\n",
    "\n",
    "import scipy\n",
    "import scipy.special\n",
    "\n",
    "from mpnn import MessagePassing, ReadoutGraph, Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version=2.1.0, CUDA=True, GPU=True, TPU=False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json, os\n",
    "import numpy as np\n",
    "\n",
    "# Tested with TensorFlow 2.1.0\n",
    "print('version={}, CUDA={}, GPU={}, TPU={}'.format(\n",
    "    tf.__version__, tf.test.is_built_with_cuda(),\n",
    "    # GPU attached?\n",
    "    len(tf.config.list_physical_devices('GPU')) > 0,\n",
    "    # TPU accessible? (only works on Colab)\n",
    "    'COLAB_TPU_ADDR' in os.environ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpus= 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    num_gpus = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\"))\n",
    "    print(\"num_gpus=\", num_gpus)\n",
    "    if num_gpus > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(\"gpu:0\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"fallback to CPU\")\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(A,B):\n",
    "    na = tf.reduce_sum(tf.square(A), -1)\n",
    "    nb = tf.reduce_sum(tf.square(B), -1)\n",
    " \n",
    "    na = tf.reshape(na, [tf.shape(na)[0], -1, 1])\n",
    "    nb = tf.reshape(nb, [tf.shape(na)[0], 1, -1])\n",
    "    Dsq = tf.clip_by_value(na - 2*tf.linalg.matmul(A, B, transpose_a=False, transpose_b=True) + nb, 1e-12, 1e12)\n",
    "    D = tf.sqrt(Dsq)\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a list of [Nbatch, Nelem, Nfeat] input nodes, computes the dense [Nbatch, Nelem, Nelem] adjacency matrices\n",
    "class Distance(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, dist_shape, *args, **kwargs):\n",
    "        super(Distance, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs1, inputs2):\n",
    "        #compute the pairwise distance matrix between the vectors defined by the first two components of the input array\n",
    "        #inputs1, inputs2: [Nbatch, Nelem, distance_dim] embedded coordinates used for element-to-element distance calculation\n",
    "        D = dist(inputs1, inputs2)\n",
    "      \n",
    "        #adjacency between two elements should be high if the distance is small.\n",
    "        #this is equivalent to radial basis functions. \n",
    "        #self-loops adj_{i,i}=1 are included, as D_{i,i}=0 by construction\n",
    "        adj = tf.math.exp(-1.0*D)\n",
    "\n",
    "        #optionally set the adjacency matrix to 0 for low values in order to make the matrix sparse.\n",
    "        #need to test if this improves the result.\n",
    "        #adj = tf.keras.activations.relu(adj, threshold=0.01)\n",
    "\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_classes):\n",
    "        super(InputEncoding, self).__init__()\n",
    "        self.num_input_classes = num_input_classes\n",
    "        \n",
    "    def call(self, X):\n",
    "        #X: [Nbatch, Nelem, Nfeat] array of all the input detector element feature data\n",
    "\n",
    "        #X[:, :, 0] - categorical index of the element type\n",
    "        Xid = tf.one_hot(tf.cast(X[:, :, 0], tf.int32), self.num_input_classes)\n",
    "\n",
    "        #X[:, :, 1:] - all the other non-categorical features\n",
    "        Xprop = X[:, :, 1:]\n",
    "        return tf.concat([Xid, Xprop], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph Highway network\n",
    "# https://arxiv.org/pdf/2004.04635.pdf\n",
    "#https://github.com/gcucurull/jax-ghnet/blob/master/models.py \n",
    "class GHConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, k, *args, **kwargs):\n",
    "        self.activation = kwargs.pop(\"activation\")\n",
    "        self.hidden_dim = args[0]\n",
    "        self.k = k\n",
    "\n",
    "        super(GHConv, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.W_t = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"w_t\", initializer=\"random_normal\")\n",
    "        self.b_t = self.add_weight(shape=(self.hidden_dim, ), name=\"b_t\", initializer=\"zeros\")\n",
    "        self.W_h = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"w_h\", initializer=\"random_normal\")\n",
    "        self.theta = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"theta\", initializer=\"random_normal\")\n",
    " \n",
    "    def call(self, x, adj):\n",
    "        #compute the normalization of the adjacency matrix\n",
    "        in_degrees = tf.reduce_sum(adj, axis=-1)\n",
    "        #add epsilon to prevent numerical issues from 1/sqrt(x)\n",
    "        norm = tf.expand_dims(tf.pow(in_degrees + 1e-6, -0.5), -1)\n",
    "        norm_k = tf.pow(norm, self.k)\n",
    "        adj_k = tf.pow(adj, self.k)\n",
    "\n",
    "        f_hom = tf.linalg.matmul(x, self.theta)\n",
    "        f_hom = tf.linalg.matmul(adj_k, f_hom*norm_k)*norm_k\n",
    "\n",
    "        f_het = tf.linalg.matmul(x, self.W_h)\n",
    "        gate = tf.nn.sigmoid(tf.linalg.matmul(x, self.W_t) + self.b_t)\n",
    "        #tf.print(tf.reduce_mean(f_hom), tf.reduce_mean(f_het), tf.reduce_mean(gate))\n",
    "\n",
    "        out = gate*f_hom + (1-gate)*f_het\n",
    "        return out\n",
    "\n",
    "## Simple Graph Conv layer\n",
    "class SGConv(tf.keras.layers.Dense):\n",
    "    def __init__(self, k, *args, **kwargs):\n",
    "        super(SGConv, self).__init__(*args, **kwargs)\n",
    "        self.k = k\n",
    "    \n",
    "    def call(self, inputs, adj):\n",
    "        W = self.weights[0]\n",
    "        b = self.weights[1]\n",
    "\n",
    "        #compute the normalization of the adjacency matrix\n",
    "        in_degrees = tf.reduce_sum(adj, axis=-1)\n",
    "        #add epsilon to prevent numerical issues from 1/sqrt(x)\n",
    "        norm = tf.expand_dims(tf.pow(in_degrees + 1e-6, -0.5), -1)\n",
    "        norm_k = tf.pow(norm, self.k)\n",
    "\n",
    "        support = (tf.linalg.matmul(inputs, W))\n",
    "     \n",
    "        #k-th power of the normalized adjacency matrix is nearly equivalent to k consecutive GCN layers\n",
    "        adj_k = tf.pow(adj, self.k)\n",
    "        out = tf.linalg.matmul(adj_k, support*norm_k)*norm_k\n",
    "\n",
    "        return self.activation(out + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple message passing based on a matrix multiplication\n",
    "class DNNSuperCluster(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, activation=tf.nn.selu, \n",
    "                     hidden_dim_coord=256, hidden_dim_input=256, hidden_dim_id=256,     \n",
    "                     n_layers_input=2, n_layers_id=3, n_layers_coord=2,\n",
    "                     distance_dim=256, num_conv=4, convlayer=\"ghconv\", dropout=0.1):\n",
    "        super(DNNSuperCluster, self).__init__()\n",
    "        self.activation = activation\n",
    "\n",
    "        #self.enc = InputEncoding(3)\n",
    "        \n",
    "        # layers for distance coordinate extraction\n",
    "        self.layers_coord = [ ]\n",
    "        for i in range(n_layers_coord):\n",
    "            layer_coord_i = tf.keras.layers.Dense(hidden_dim_coord, activation=activation, name=\"disctcoords_\"+str(i))\n",
    "            self.layers_coord.append(layer_coord_i)\n",
    "\n",
    "        self.layer_distcoords = tf.keras.layers.Dense(distance_dim, activation=\"linear\", name=\"distcoords_final\")\n",
    "        self.layer_distance = Distance(distance_dim, name=\"distance\")\n",
    "\n",
    "        # layers for feature extraction \n",
    "        self.layers_input = [ ]\n",
    "        for i in range(n_layers_input):\n",
    "            layer_input_i = tf.keras.layers.Dense(hidden_dim_input, activation=activation, name=\"input_\"+str(i))\n",
    "            layer_input_i_do = tf.keras.layers.Dropout(dropout)\n",
    "            self.layers_input.append((layer_input_i, layer_input_i_do))\n",
    "        \n",
    "       \n",
    "\n",
    "        # Graph convolutions\n",
    "        if convlayer == \"sgconv\":\n",
    "            self.layer_conv1 = SGConv(num_conv, hidden_dim_input, activation=activation, name=\"conv1\")\n",
    "            #self.layer_conv2 = SGConv(num_conv, 2*hidden_dim+len(class_labels), activation=activation, name=\"conv2\")\n",
    "        elif convlayer == \"ghconv\":\n",
    "            self.layer_conv1 = GHConv(num_conv, hidden_dim_input, activation=activation, name=\"conv1\")\n",
    "            #self.layer_conv2 = GHConv(num_conv, 2*hidden_dim+len(class_labels), activation=activation, name=\"conv2\")\n",
    "\n",
    "        # Output layers\n",
    "        self.layers_id = [ ]\n",
    "        for i in range(n_layers_id):\n",
    "            layer_id_i = tf.keras.layers.Dense(hidden_dim_id, activation=activation, name=\"id_\"+str(i))\n",
    "            layer_id_i_do = tf.keras.layers.Dropout(dropout)\n",
    "            self.layers_id.append((layer_id_i, layer_id_i_do))\n",
    "            \n",
    "        # binary output logits\n",
    "        self.layer_id = tf.keras.layers.Dense(1, activation=\"linear\", name=\"out_id\")\n",
    "        \n",
    " \n",
    "    def predict_distancematrix(self, inputs, training=True):\n",
    "        x = inputs\n",
    "        for layer_coord in self.layers_coord:\n",
    "            x = layer_coord(x)\n",
    "\n",
    "        distcoords = self.layer_distcoords(x)\n",
    "\n",
    "        dm = self.layer_distance(distcoords, distcoords)\n",
    "        \n",
    "        # masking if the first element is -1\n",
    "        msk_elem = tf.expand_dims(tf.cast(inputs[:, :, 0] != -1, dtype=tf.float32), -1)\n",
    "        dm = dm*msk_elem\n",
    "\n",
    "        return dm\n",
    "\n",
    "    #@tf.function(input_signature=[tf.TensorSpec(shape=[None, 15], dtype=tf.float32)])\n",
    "    def call(self, inputs, training=True):\n",
    "        # separate cluster energies from rescaled inputs\n",
    "        X = inputs[:,:,1:]\n",
    "        cl_energies = inputs[:,:,0]\n",
    "        \n",
    "        msk_input = tf.expand_dims(tf.cast(X[:, :, 0] != -1, tf.float32), -1)\n",
    "\n",
    "        dm = self.predict_distancematrix(X, training=training)\n",
    "        \n",
    "        x = X\n",
    "        for layer_input, layer_input_do in self.layers_input:\n",
    "            x = layer_input(x)\n",
    "            x = layer_input_do(x, training)\n",
    "            \n",
    "        x = self.layer_conv1(x, dm)\n",
    "        \n",
    "        for layer_id, layer_id_do in self.layers_id:\n",
    "            x = layer_id(x)\n",
    "            x = layer_id_do(x, training)\n",
    "            \n",
    "        out_id_logits = self.layer_id(x)\n",
    "        \n",
    "        energies = tf.expand_dims(cl_energies, axis=-1)\n",
    "        # add the cluster energies in the output, in the future we can add here corrections\n",
    "        output = tf.concat([out_id_logits,energies], axis=-1)\n",
    "        # mask to 0 the padded output\n",
    "        output_masked = output * msk_input\n",
    "        \n",
    "        #return masked output logits and the predicted total energy\n",
    "        return output_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def separate_true(y):\n",
    "    # one-hot encoding for true label (signal,PU,noise)\n",
    "    # the padded elements have -1 so they are one_hot to (0,0)\n",
    "    #y_onehot = tf.one_hot(tf.cast(y[:,1:], tf.int32), nclass_labels)\n",
    "    ytrue = y[:, 1:]\n",
    "    true_en = y[:, 0]\n",
    "    \n",
    "    mask = tf.cast(ytrue!=-1, tf.float32)\n",
    "    ytrue_msk = ytrue * mask\n",
    "    return ytrue_msk, true_en, mask\n",
    "\n",
    "#@tf.function\n",
    "def get_true_mask(y):\n",
    "    # mask for elements that should be included in supercluster\n",
    "    in_sc = tf.cast(y[:,1:] == 1., tf.float32)\n",
    "    # number of padding elements\n",
    "    padded = tf.reduce_sum(tf.cast(y[:,1:] == -1., tf.float32), axis=-1)\n",
    "    return in_sc, padded\n",
    "    \n",
    "#@tf.function\n",
    "def separate_pred(ypred):\n",
    "    ens = ypred[:,:,1]\n",
    "    ypred_ext = ypred[:,:,0]\n",
    "    # 0 not include in energy sum, 1 include in energy sum\n",
    "    # masked elements have pred_id=0 so they do not enter in the energy sum\n",
    "    predid_mask = tf.cast(tf.math.sigmoid(ypred_ext)[:,:] > 0.5, tf.float32)\n",
    "    # predicted total energy\n",
    "    pred_en =  tf.reduce_sum( ens * predid_mask, axis=-1)\n",
    "    # one-hot encoding for true label (signal,PU,noise)\n",
    "    return ypred_ext, pred_en, predid_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def mse_unreduced(true, pred):\n",
    "    return tf.math.pow(true-pred,2)\n",
    "\n",
    "#@tf.function\n",
    "def msle_unreduced(true, pred):\n",
    "    return tf.math.pow(tf.math.log(tf.math.abs(true) + 1.0) - tf.math.log(tf.math.abs(pred) + 1.0), 2)\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def my_loss_full(y_true, y_pred):\n",
    "    y_true_msk, true_en, true_mask = separate_true(y_true)\n",
    "    y_pred_msk, pred_en, pred_id = separate_pred(y_pred)\n",
    "    # since the padded y_true is -1 -> it gives [0,0] when it is onehot. The ypred for batched is [0,0] so the loss\n",
    "    # is automatically 0 for padded samples\n",
    "    tf.print(y_true_msk, y_true_msk.shape)\n",
    "    tf.print(y_pred_msk, y_true_msk.shape)\n",
    "    \n",
    "    # apply mask on loss\n",
    "    l1 = tf.keras.backend.binary_crossentropy(y_true_msk, y_pred_msk, from_logits=True) * true_mask\n",
    "    \n",
    "    #tf.print(l1)\n",
    "    # true energy loss\n",
    "    mask_outsc = tf.cast(true_en == 0., tf.float32)\n",
    "    mask_insc = tf.cast(true_en != 0., tf.float32)\n",
    "    n_outsc = tf.reduce_sum(mask_outsc)\n",
    "    n_insc = tf.reduce_sum(mask_insc)\n",
    "    \n",
    "    l2_en = mse_unreduced(true_en, pred_en)\n",
    "    l2_en_log = msle_unreduced(true_en, pred_en)\n",
    "    \n",
    "    # separate mean resolution for windows with Caloparticle or not\n",
    "    l2_en_outsc = tf.reduce_sum(l2_en * mask_outsc) / n_outsc\n",
    "    l2_en_insc = tf.reduce_sum(l2_en * mask_insc) / n_insc\n",
    "    l2_en_outsc_log = tf.reduce_sum(l2_en_log * mask_outsc) / n_outsc\n",
    "    l2_en_insc_log = tf.reduce_sum(l2_en_log * mask_insc) / n_insc\n",
    "    \n",
    "    ltot = 1e4*tf.reduce_mean(l1) + 20* l2_en_insc +   10*l2_en_outsc + 200* l2_en_insc_log  + 100* l2_en_outsc_log\n",
    "    \n",
    "    return ltot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_resolution_outsc(y_true, y_pred):\n",
    "    y_true_msk, true_en, true_mask = separate_true(y_true)\n",
    "    y_pred_msk, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_outsc = tf.cast(true_en == 0., tf.float32)\n",
    "    n_outsc = tf.reduce_sum(mask_outsc)\n",
    "    return tf.reduce_sum(mse_unreduced(true_en, pred_en)*mask_outsc) / n_outsc\n",
    "\n",
    "def energy_resolution_insc(y_true, y_pred):\n",
    "    y_true_msk, true_en, true_mask = separate_true(y_true)\n",
    "    y_pred_msk, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_insc = tf.cast(true_en != 0., tf.float32)\n",
    "    n_insc = tf.reduce_sum(mask_insc)\n",
    "    return tf.reduce_sum(mse_unreduced(true_en, pred_en)*mask_insc) / n_insc\n",
    "\n",
    "def energy_resolution_outsc_log(y_true, y_pred):\n",
    "    y_true_msk, true_en, true_mask = separate_true(y_true)\n",
    "    y_pred_msk, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_outsc = tf.cast(true_en == 0., tf.float32)\n",
    "    n_outsc = tf.reduce_sum(mask_outsc)\n",
    "    return tf.reduce_sum(msle_unreduced(true_en, pred_en)*mask_outsc) / n_outsc\n",
    "\n",
    "def energy_resolution_insc_log(y_true, y_pred):\n",
    "    y_true_msk, true_en, true_mask = separate_true(y_true)\n",
    "    y_pred_msk, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_insc = tf.cast(true_en != 0., tf.float32)\n",
    "    n_insc = tf.reduce_sum(mask_insc)\n",
    "    return tf.reduce_sum(msle_unreduced(true_en, pred_en)*mask_insc) / n_insc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpfn_metrics(y_true, y_pred):\n",
    "    y_true_mask, n_padded = get_true_mask(y_true)\n",
    "    y_false_mask = (tf.cast(y_true_mask == 0., tf.float32))\n",
    "    \n",
    "    # pred_id contains the last n_padded elements to 0 that will be always True negatives\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    \n",
    "    n_pos = tf.reduce_sum(y_true_mask, axis=-1)\n",
    "    n_neg = tf.reduce_sum(y_false_mask, axis=-1) - n_padded\n",
    "    \n",
    "    n_tot = n_neg + n_pos\n",
    "    \n",
    "    true_pos = tf.reduce_sum(pred_id * y_true_mask, axis=-1)\n",
    "    false_neg = n_pos - true_pos\n",
    "    \n",
    "    false_pos = tf.reduce_sum(pred_id * y_false_mask, axis=-1)\n",
    "    true_neg = n_neg - false_pos\n",
    "    \n",
    "    return n_tot, true_pos, false_neg, false_pos, true_neg, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp,tn,fp,fn):\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def recall(tp,tn,fp,fn):\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def accuracy(tp,tn,fp,fn):\n",
    "    return (tp+tn)/(tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Precision(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='precision', **kwargs):\n",
    "        super(Precision, self).__init__(name=name, **kwargs)\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        n_tot, true_pos, false_neg, false_pos, true_neg = get_tpfn_metrics(y_true, y_pred)\n",
    "        self.tp.assign_add(tf.reduce_sum(true_pos))\n",
    "        self.fp.assign_add(tf.reduce_sum(false_pos))\n",
    "\n",
    "    def result(self):\n",
    "        return self.tp / (self.tp + self.fp)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fp.assign(0)\n",
    "        \n",
    "class Recall(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='recall', **kwargs):\n",
    "        super(Recall, self).__init__(name=name, **kwargs)\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        n_tot, true_pos, false_neg, false_pos, true_neg = get_tpfn_metrics(y_true, y_pred)\n",
    "        self.tp.assign_add(tf.reduce_sum(true_pos))\n",
    "        self.fn.assign_add(tf.reduce_sum(false_neg))\n",
    "\n",
    "    def result(self):\n",
    "        return self.tp / (self.tp + self.fn)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fn.assign(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/storage/ECAL/training_data/window_data/electrons/recordio_v4\"\n",
    "models_path = \"/storage/ECAL/deepcluster/models/gcn_models_v9/\"\n",
    "\n",
    "#rain_steps_per_epoch = \n",
    "#eval_steps_per_epoch = 3e5 // batch_size\n",
    "from collections import namedtuple\n",
    "Args = namedtuple('args', [ 'models_path', 'load','nepochs','ntrain','nval','nfeatures',\n",
    "                            'n_seed_features','batch_size','lr_decay','lr',\n",
    "                            'hidden_dim_input','hidden_dim_coord', 'hidden_dim_id',\n",
    "                            'n_layers_input', 'n_layers_id', 'n_layers_coord',\n",
    "                           'distance_dim','num_conv','dropout','convlayer',\n",
    "                           'opt'])\n",
    "\n",
    "args = Args( \n",
    "models_path = models_path,\n",
    "load = False,\n",
    "nepochs = 100,\n",
    "ntrain = 500000,\n",
    "nval = 100000,\n",
    "nfeatures = 13,\n",
    "n_seed_features = 12,\n",
    "lr_decay = 0,\n",
    "lr = 0.00001,\n",
    "batch_size = 150,\n",
    "n_layers_input = 3,\n",
    "n_layers_id = 2,\n",
    "n_layers_coord = 2,\n",
    "hidden_dim_input = 100,\n",
    "hidden_dim_coord = 50,\n",
    "hidden_dim_id = 100,\n",
    "distance_dim = 30,\n",
    "num_conv = 2,\n",
    "dropout = 0.01,\n",
    "convlayer = 'ghconv',\n",
    "opt='adam'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features_clusters(X):\n",
    "    '''\n",
    "    'is_seed',\"cluster_deta\", \"cluster_dphi\", \"en_cluster\", \"et_cluster\",\n",
    "    \"cl_f5_r9\", \"cl_f5_sigmaIetaIeta\",\"cl_f5_sigmaIetaIphi\",\"cl_f5_sigmaIphiIphi\",\n",
    "    \"cl_f5_swissCross\", \"cl_nxtals\", \"cl_etaWidth\", \"cl_phiWidth\n",
    "    '''\n",
    "    x_mean = tf.constant( \n",
    "        [   0.,  -7.09402501e-04, -1.27142875e-04,  1.30375508e+00,  5.67249500e-01, \n",
    "            1.92096066e+00,  1.31476120e-02,  1.62948213e-05,  1.42948806e-02,\n",
    "            5.92920497e-01,  1.49597644e+00,  3.36213188e-03,  3.06446267e-03]\n",
    "        )\n",
    "\n",
    "    x_scale = tf.constant(\n",
    "        [  1.,  1.10279784e-01, 3.30488055e-01, 2.62605247e+00, 1.16284769e+00,\n",
    "            7.81094814e+00, 1.70392176e-02, 3.05995567e-04, 1.80176053e-02,\n",
    "            1.99316624e+00, 1.88845046e+00, 4.12315715e-03, 4.79639033e-03]       \n",
    "        )\n",
    "    return (X-x_mean)/ x_scale\n",
    "\n",
    "def scale_features_seed(X):\n",
    "    '''\n",
    "     \"seed_eta\", \"seed_iz\",\"en_seed\",\"et_seed\",\n",
    "     \"seed_f5_r9\", \"seed_f5_sigmaIetaIeta\",\"seed_f5_sigmaIetaIphi\",\"seed_f5_sigmaIphiIphi\",\n",
    "     \"seed_f5_swissCross\",\"seed_nxtals\", \"seed_etaWidth\", \"seed_phiWidth\",\n",
    "    '''\n",
    "    x_mean = tf.constant( \n",
    "        [   6.84241156e-03,  1.62242679e-03,  5.81495577e+01,  2.57215845e+01, \n",
    "            1.00772582e+00,  1.35803461e-02, -4.29317013e-06,  1.71072024e-02,\n",
    "            4.90466869e-01,  5.10511982e+00,  8.82101138e-03,  1.04095965e-02 ]\n",
    "    \n",
    "        )\n",
    "\n",
    "    x_scale = tf.constant(\n",
    "        [   1.31333380e+00, 5.06988411e-01, 9.21157365e+01, 2.98580765e+01, \n",
    "            1.17047757e-01, 1.11969442e-02, 1.86572967e-04, 1.31036359e-02,\n",
    "            4.01511744e-01, 5.67007350e+00, 6.14304203e-03, 7.24808860e-03]       \n",
    "        )\n",
    "    return (X-x_mean)/ x_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_tfr_element(element):\n",
    "    parse_dic = {\n",
    "        'X':      tf.io.FixedLenFeature([], tf.string),\n",
    "        'X_seed': tf.io.FixedLenFeature([], tf.string),\n",
    "        'y':      tf.io.FixedLenFeature([], tf.string),\n",
    "        'n_clusters': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example_message = tf.io.parse_single_example(element, parse_dic)\n",
    "\n",
    "    X = example_message['X']\n",
    "    X_seed = example_message['X_seed']\n",
    "    y = example_message['y']\n",
    "    nclusters = example_message['n_clusters']\n",
    "    \n",
    "    arr_X = tf.io.parse_tensor(X, out_type=tf.float32)\n",
    "    arr_X_seed = tf.io.parse_tensor(X_seed, out_type=tf.float32)\n",
    "    arr_y = tf.io.parse_tensor(y, out_type=tf.float32)\n",
    "    \n",
    "    #https://github.com/tensorflow/tensorflow/issues/24520#issuecomment-577325475\n",
    "    arr_X.set_shape(     tf.TensorShape((None, args.nfeatures)))\n",
    "    arr_X_seed.set_shape(tf.TensorShape((1, args.n_seed_features)))\n",
    "    arr_y.set_shape(     tf.TensorShape((None,)))\n",
    " \n",
    "    return arr_X, arr_X_seed, nclusters, arr_y\n",
    "  \n",
    "def _stack_seed_features(arr_X, arr_X_seed, nclusters, arr_y):\n",
    "    en_clusters = tf.expand_dims(arr_X[:,3], axis=-1)\n",
    "    rescaled_X = scale_features_clusters(arr_X)\n",
    "    rescaled_X_seed = scale_features_seed(arr_X_seed)\n",
    "    X = tf.concat([en_clusters, rescaled_X, tf.broadcast_to(rescaled_X_seed,[nclusters,rescaled_X_seed.shape[1]] )],\n",
    "                  axis=1)\n",
    "    return X,arr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# padding shape\n",
    "ps = ([None,args.nfeatures+args.n_seed_features+1],[None,])\n",
    "\n",
    "# Create datasets from TFRecord files.\n",
    "dataset = tf.data.TFRecordDataset(tf.io.gfile.glob('{}/training-*'.format(data_path)))\n",
    "dataset = dataset.map(_parse_tfr_element,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.map(_stack_seed_features,num_parallel_calls=tf.data.experimental.AUTOTUNE) # deterministic=False in TFv2.3\n",
    "dataset = dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
    "\n",
    "ds_train = dataset.take(args.ntrain).padded_batch(args.batch_size, padded_shapes=ps, drop_remainder=True,padding_values=(-1.,-1.))\n",
    "ds_test = dataset.skip(args.ntrain).take(args.nval).padded_batch(args.batch_size, padded_shapes=ps, drop_remainder=True,padding_values=(-1.,-1.))\n",
    "\n",
    "ds_train_r = ds_train.repeat(args.nepochs)\n",
    "ds_test_r = ds_test.repeat(args.nepochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8, 26), dtype=float32, numpy=\n",
       " array([[ 6.21106052e+00,  1.00000000e+00,  6.43275212e-03,\n",
       "          3.84712475e-04,  1.86870062e+00,  4.22896862e-01,\n",
       "         -9.00832936e-02,  2.02615070e+00,  2.36034527e-01,\n",
       "          9.92546976e-01, -2.75320299e-02, -2.62636721e-01,\n",
       "          7.02001715e+00,  3.60062718e+00,  1.86392045e+00,\n",
       "          1.96923149e+00, -5.63839614e-01, -8.25993299e-01,\n",
       "          1.79071259e+00,  3.04469562e+00,  4.97464955e-01,\n",
       "          1.15013874e+00,  1.18496425e-01, -7.23997593e-01,\n",
       "          3.82314777e+00,  1.36931002e+00],\n",
       "        [ 5.17345333e+00,  0.00000000e+00, -1.41852963e+00,\n",
       "          1.26807344e+00,  1.47358000e+00,  3.97443622e-01,\n",
       "         -1.17790654e-01,  1.01207280e+00, -9.00833189e-01,\n",
       "          1.02417421e+00,  2.04237625e-01,  2.66897947e-01,\n",
       "         -7.86338374e-02,  5.68524885e+00,  1.86392045e+00,\n",
       "          1.96923149e+00, -5.63839614e-01, -8.25993299e-01,\n",
       "          1.79071259e+00,  3.04469562e+00,  4.97464955e-01,\n",
       "          1.15013874e+00,  1.18496425e-01, -7.23997593e-01,\n",
       "          3.82314777e+00,  1.36931002e+00],\n",
       "        [ 6.67877102e+00,  0.00000000e+00,  1.65376174e+00,\n",
       "          1.01214290e+00,  2.04680467e+00,  3.30621034e-01,\n",
       "         -1.17906384e-01, -7.71608949e-01, -5.32518215e-02,\n",
       "         -7.93384075e-01,  2.04237625e-01,  7.96432614e-01,\n",
       "          1.86837792e-01, -5.99330246e-01,  1.86392045e+00,\n",
       "          1.96923149e+00, -5.63839614e-01, -8.25993299e-01,\n",
       "          1.79071259e+00,  3.04469562e+00,  4.97464955e-01,\n",
       "          1.15013874e+00,  1.18496425e-01, -7.23997593e-01,\n",
       "          3.82314777e+00,  1.36931002e+00],\n",
       "        [ 3.98259401e+00,  0.00000000e+00, -9.34673905e-01,\n",
       "         -7.81856716e-01,  1.02010107e+00,  1.58914194e-01,\n",
       "         -1.17906384e-01, -5.52926779e-01, -1.64762661e-01,\n",
       "          4.46850449e-01, -1.00631490e-01,  2.66897947e-01,\n",
       "          2.35854030e-01,  4.17063332e+00,  1.86392045e+00,\n",
       "          1.96923149e+00, -5.63839614e-01, -8.25993299e-01,\n",
       "          1.79071259e+00,  3.04469562e+00,  4.97464955e-01,\n",
       "          1.15013874e+00,  1.18496425e-01, -7.23997593e-01,\n",
       "          3.82314777e+00,  1.36931002e+00],\n",
       "        [ 3.47712135e+00,  0.00000000e+00, -6.77480578e-01,\n",
       "          1.75906360e-01,  8.27617288e-01,  6.13138936e-02,\n",
       "         -9.78201181e-02,  1.24109375e+00,  1.06670582e+00,\n",
       "          6.94672108e-01,  2.04237625e-01,  2.66897947e-01,\n",
       "          7.09340906e+00,  1.63959336e+00,  1.86392045e+00,\n",
       "          1.96923149e+00, -5.63839614e-01, -8.25993299e-01,\n",
       "          1.79071259e+00,  3.04469562e+00,  4.97464955e-01,\n",
       "          1.15013874e+00,  1.18496425e-01, -7.23997593e-01,\n",
       "          3.82314777e+00,  1.36931002e+00],\n",
       "        [ 1.59282565e+00,  0.00000000e+00, -8.24458063e-01,\n",
       "          1.15051496e+00,  1.10078007e-01, -2.32224748e-01,\n",
       "         -2.87744682e-02,  2.84978414e+00,  1.14864543e-01,\n",
       "          8.58506739e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "          1.17382035e-01, -6.21177375e-01,  1.86392045e+00,\n",
       "          1.96923149e+00, -5.63839614e-01, -8.25993299e-01,\n",
       "          1.79071259e+00,  3.04469562e+00,  4.97464955e-01,\n",
       "          1.15013874e+00,  1.18496425e-01, -7.23997593e-01,\n",
       "          3.82314777e+00,  1.36931002e+00],\n",
       "        [ 1.20396602e+00,  0.00000000e+00, -1.78031349e+00,\n",
       "         -1.68710124e+00, -3.79996300e-02, -2.73586512e-01,\n",
       "         -1.17906384e-01, -7.71608949e-01, -5.32518215e-02,\n",
       "         -7.65104115e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "         -5.52268803e-01, -6.36257052e-01,  1.86392045e+00,\n",
       "          1.96923149e+00, -5.63839614e-01, -8.25993299e-01,\n",
       "          1.79071259e+00,  3.04469562e+00,  4.97464955e-01,\n",
       "          1.15013874e+00,  1.18496425e-01, -7.23997593e-01,\n",
       "          3.82314777e+00,  1.36931002e+00],\n",
       "        [ 1.41401088e+00,  0.00000000e+00, -2.87868172e-01,\n",
       "          7.00460672e-01,  4.19853926e-02, -2.73744553e-01,\n",
       "         -1.17906384e-01, -7.71608949e-01, -5.32518215e-02,\n",
       "         -7.93384075e-01,  2.04237625e-01,  2.66897947e-01,\n",
       "         -5.12782037e-01, -6.27054870e-01,  1.86392045e+00,\n",
       "          1.96923149e+00, -5.63839614e-01, -8.25993299e-01,\n",
       "          1.79071259e+00,  3.04469562e+00,  4.97464955e-01,\n",
       "          1.15013874e+00,  1.18496425e-01, -7.23997593e-01,\n",
       "          3.82314777e+00,  1.36931002e+00]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(9,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = next(idata)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_run():\n",
    "    previous_runs = os.listdir(args.models_path)\n",
    "    if len(previous_runs) == 0:\n",
    "        run_number = 1\n",
    "    else:\n",
    "        run_number = max([int(s.split('run_')[1]) for s in previous_runs]) + 1\n",
    "    return run_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.lr_decay > 0:\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            args.lr,\n",
    "            decay_steps=2*int(args.ntrain//args.batch_size),\n",
    "            decay_rate=args.lr_decay\n",
    "        )\n",
    "else:\n",
    "    lr_schedule = args.lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    model = DNNSuperCluster(hidden_dim_input=args.hidden_dim_input,hidden_dim_coord=args.hidden_dim_coord,\n",
    "                            hidden_dim_id=args.hidden_dim_id, \n",
    "                            n_layers_input=args.n_layers_input, n_layers_id=args.n_layers_id, n_layers_coord=args.n_layers_coord,\n",
    "                            distance_dim=args.distance_dim, \n",
    "                            num_conv=args.num_conv, convlayer=args.convlayer, dropout=args.dropout)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/ECAL/deepcluster/models/gcn_models_v9/run_04\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(args.models_path):\n",
    "    os.makedirs(args.models_path)\n",
    "\n",
    "name =  'run_{:02}'.format(get_unique_run())\n",
    "\n",
    "outdir = args.models_path + name\n",
    "\n",
    "if os.path.isdir(outdir):\n",
    "    print(\"Output directory exists: {}\".format(outdir), file=sys.stderr)\n",
    "\n",
    "print(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "tb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=outdir, histogram_freq=2, \n",
    "    write_graph=False, \n",
    "    write_images=True,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0,\n",
    ")\n",
    "tb.set_model(model)\n",
    "callbacks += [tb]\n",
    "\n",
    "terminate_cb = tf.keras.callbacks.TerminateOnNaN()\n",
    "callbacks += [terminate_cb]\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=outdir + \"/weights.{epoch:02d}-{val_loss:.6f}.hdf5\",\n",
    "    save_weights_only=True,\n",
    "    verbose=0\n",
    ")\n",
    "cp_callback.set_model(model)\n",
    "callbacks += [cp_callback]\n",
    "\n",
    "loss_fn = my_loss_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... -0 -0 -0]\n",
      " [1 1 1 ... -0 -0 -0]\n",
      " [1 1 0 ... -0 -0 -0]\n",
      " ...\n",
      " [0 0 0 ... -0 -0 -0]\n",
      " [1 1 0 ... -0 -0 -0]\n",
      " [1 1 0 ... -0 -0 -0]] TensorShape([150, 21])\n",
      "[[185.544373 134.538452 2144.61035 ... -0 -0 -0]\n",
      " [21.5093803 29.8237114 86.2532272 ... -0 -0 -0]\n",
      " [-1.65062845 0.339733124 0.0180167407 ... -0 -0 -0]\n",
      " ...\n",
      " [22.4429779 32.8211517 6.51217937 ... -0 -0 -0]\n",
      " [-2.78467894 -0.112497598 0.397031486 ... -0 -0 -0]\n",
      " [-1.1252085 -0.76353085 3.70884728 ... -0 -0 -0]] TensorShape([150, 21])\n"
     ]
    }
   ],
   "source": [
    "for X, y in ds_train:\n",
    "    ypred = model(X)\n",
    "    l = loss_fn(y, ypred)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model.compile(optimizer=args.opt, loss=loss_fn,\n",
    "        metrics=[Precision(),Recall(), energy_resolution_insc,energy_resolution_outsc,\n",
    "                     energy_resolution_insc_log,energy_resolution_outsc_log,])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoh,true_en,true_mask = separate_true(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(150,), dtype=float32, numpy=\n",
       "array([  0.       ,   6.1480455, 102.64274  ,   0.       ,   0.       ,\n",
       "         0.       ,   0.       , 117.57765  ,  34.755424 ,  76.20421  ,\n",
       "         0.       ,   0.       ,   0.       , 496.408    ,   0.       ,\n",
       "        72.68577  ,  93.457436 ,  44.630966 ,   0.       ,  58.61595  ,\n",
       "       110.17344  ,  25.631573 , 114.91464  ,  55.073254 , 122.57496  ,\n",
       "        68.83105  ,  51.022644 ,  47.621117 , 658.0822   ,   0.       ,\n",
       "         0.       ,  57.11605  , 600.94476  ,  69.80868  , 676.72064  ,\n",
       "         0.       ,  26.032593 ,   0.       , 238.51596  ,  91.3131   ,\n",
       "         0.       ,   0.       ,   0.       , 116.750694 ,   0.       ,\n",
       "        47.84798  ,   0.       ,   0.       , 132.83043  , 135.0918   ,\n",
       "        62.00114  ,   0.       , 691.1379   , 213.19713  ,  15.771311 ,\n",
       "         0.       ,  27.844202 , 117.78779  ,   0.       , 167.27051  ,\n",
       "        11.196314 ,  38.535778 , 515.0749   ,  80.20902  ,   0.       ,\n",
       "        56.639275 ,   0.       ,  86.28748  ,   0.       , 111.04272  ,\n",
       "       535.1645   , 147.32025  ,  32.039158 , 473.6685   ,  68.9274   ,\n",
       "        56.659294 , 266.20642  ,  71.91534  ,  80.55842  ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "         8.064288 ,   0.       ,   0.       ,  11.646223 ,   0.       ,\n",
       "         0.       ,   0.       , 151.50726  , 104.55189  ,   0.       ,\n",
       "       335.45758  , 268.7886   , 221.72894  , 168.27525  , 487.23364  ,\n",
       "         0.       ,  32.199497 ,   0.       ,  46.018784 , 489.22922  ,\n",
       "        57.26116  ,  81.07906  ,   0.       ,   0.       ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,  18.4205   ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       ,   0.       , 408.775    ,\n",
       "        23.849068 ,   0.       ,   0.       ,   0.       ,   0.       ,\n",
       "         3.7205107,   0.       ,  64.86896  , 205.51225  ,   0.       ,\n",
       "        25.720268 ,   0.       ,   0.       , 220.53598  , 388.46173  ,\n",
       "         0.       ,   0.       ,   0.       ,  77.084274 ,   0.       ,\n",
       "         0.       ,  25.312475 , 222.00488  ,  41.10948  ,   0.       ,\n",
       "         0.       ,   0.       ,   0.       , 134.73381  ,  37.855896 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dnn_super_cluster\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "disctcoords_0 (Dense)        multiple                  1300      \n",
      "_________________________________________________________________\n",
      "disctcoords_1 (Dense)        multiple                  2550      \n",
      "_________________________________________________________________\n",
      "distcoords_final (Dense)     multiple                  1530      \n",
      "_________________________________________________________________\n",
      "distance (Distance)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_0 (Dense)              multiple                  2600      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_1 (Dense)              multiple                  10100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_2 (Dense)              multiple                  10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1 (GHConv)               multiple                  30100     \n",
      "_________________________________________________________________\n",
      "id_0 (Dense)                 multiple                  10100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "id_1 (Dense)                 multiple                  10100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "out_id (Dense)               multiple                  101       \n",
      "=================================================================\n",
      "Total params: 78,581\n",
      "Trainable params: 78,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "logits and labels must have the same shape ((None, 21) vs (None, None, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0mnew_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_same_rank\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    977\u001b[0m         raise ValueError(\"Shapes %s and %s must have the same rank\" %\n\u001b[0;32m--> 978\u001b[0;31m                          (self, other))\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (None, None, None) and (None, 21) must have the same rank",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py\u001b[0m in \u001b[0;36msigmoid_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are not compatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (None, None, None) and (None, 21) are not compatible",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-e2e1e3b2f2b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntrain\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnval\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, standardize_function, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstandardize_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;31m# Note that the dataset instance is immutable, its fine to reusing the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mstandardize_function\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weight_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sample_weight_mode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0;31m# Then we map using only the tensor standardization portion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2358\u001b[0m     \u001b[0mis_compile_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2360\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2361\u001b[0m       \u001b[0mis_compile_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_compile_from_inputs\u001b[0;34m(self, all_inputs, target, orig_inputs, orig_target)\u001b[0m\n\u001b[1;32m   2616\u001b[0m         \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2618\u001b[0;31m         experimental_run_tf_function=self._experimental_run_tf_function)\n\u001b[0m\u001b[1;32m   2619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m   \u001b[0;31m# TODO(omalleyt): Consider changing to a more descriptive function name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m       \u001b[0;31m# Creates the model loss and weighted metrics sub-graphs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_weights_loss_and_weighted_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m       \u001b[0;31m# Functions for train, test and predict will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_compile_weights_loss_and_weighted_metrics\u001b[0;34m(self, sample_weights)\u001b[0m\n\u001b[1;32m   1590\u001b[0m       \u001b[0;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m       \u001b[0;31m#                   layer losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_skip_target_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[0;34m(self, masks)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reduction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             \u001b[0mper_sample_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m             weighted_losses = losses_utils.compute_weighted_loss(\n\u001b[1;32m   1654\u001b[0m                 \u001b[0mper_sample_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    219\u001b[0m       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n\u001b[1;32m    220\u001b[0m           y_pred, y_true)\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2d4fc402a7c6>\u001b[0m in \u001b[0;36mmy_loss_full\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# apply mask on loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_msk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_msk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrue_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#tf.print(l1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbinary_crossentropy\u001b[0;34m(target, output, from_logits)\u001b[0m\n\u001b[1;32m   4613\u001b[0m       \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4614\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4615\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py\u001b[0m in \u001b[0;36msigmoid_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m       raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\u001b[0;32m--> 170\u001b[0;31m                        (logits.get_shape(), labels.get_shape()))\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# The logistic loss formula from above is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: logits and labels must have the same shape ((None, 21) vs (None, None, None))"
     ]
    }
   ],
   "source": [
    "if args.load:\n",
    "    #ensure model input size is known\n",
    "    for X, y in ds_train:\n",
    "        model(X)\n",
    "        break\n",
    "\n",
    "    model.load_weights(args.load)\n",
    "if args.nepochs > 0:\n",
    "    ret = model.fit(ds_train_r,\n",
    "        validation_data=ds_test_r, epochs=args.nepochs,\n",
    "        steps_per_epoch=args.ntrain//args.batch_size, validation_steps=args.nval//args.batch_size,\n",
    "        verbose=True,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"args(models_path='/storage/ECAL/deepcluster/models/gcn_models_v9/', load=False, nepochs=100, ntrain=500000, nval=100000, nfeatures=13, n_seed_features=12, batch_size=150, lr_decay=0, lr=1e-05, hidden_dim_input=100, hidden_dim_coord=50, hidden_dim_id=100, n_layers_input=3, n_layers_id=2, n_layers_coord=2, distance_dim=30, num_conv=2, dropout=0.01, convlayer='ghconv', opt='adam')\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/storage/ECAL/deepcluster/models/gcn_models_v9/run_03/args.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-3527301366bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/args.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/storage/ECAL/deepcluster/models/gcn_models_v9/run_03/args.txt'"
     ]
    }
   ],
   "source": [
    "with open(outdir + \"/args.txt\",'w') as config:\n",
    "    config.write(str(args))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

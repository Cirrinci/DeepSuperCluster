{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218ae4b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T11:47:23.103112Z",
     "start_time": "2022-07-28T11:47:22.437429Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 10:46:12.140694: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-07 10:46:12.140733: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib as mpl \n",
    "mpl.rcParams[\"image.origin\"] = 'lower'\n",
    "mpl.rcParams[\"image.cmap\"] = \"Blues\"\n",
    "mpl.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.CMS)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb48a04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T11:47:25.821496Z",
     "start_time": "2022-07-28T11:47:25.818148Z"
    }
   },
   "outputs": [],
   "source": [
    "inputdir_ele = \"/work/dvalsecc/Clustering/dataset/electrons_awkward_2022v8/\"\n",
    "inputdir_gamma = \"/work/dvalsecc/Clustering/dataset/gammas_awkward_2022v8/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173c7a51-65a3-4612-9dec-1d1960050c1f",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933c83ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T11:47:35.723665Z",
     "start_time": "2022-07-28T11:47:35.719822Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest, islice\n",
    "from collections import deque\n",
    "\n",
    "def load_dataset_chunks(df, config, chunk_size, offset=0, maxevents=None):\n",
    "    # Filtering the columns to keey only the requested ones\n",
    "    cols = { key: df[key][v] for key, v in config[\"columns\"].items() }\n",
    "    # Adding the clusters hits \n",
    "    if config[\"read_hits\"]:\n",
    "        cols['cl_h'] = df.cl_h\n",
    "    filtered_df = ak.zip(cols, depth_limit=1)\n",
    "    # Now load in large chunks batching\n",
    "    if maxevents:\n",
    "        nchunks = maxevents // chunk_size\n",
    "    else:\n",
    "        nchunks = ak.num(filtered_df.cl_features, axis=0)//chunk_size \n",
    "    for i in range(nchunks):\n",
    "        # Then materialize it\n",
    "        yield chunk_size, ak.materialized(filtered_df[offset + i*chunk_size: offset + (i+1)*chunk_size])\n",
    "        #yield batch_size, df[i*batch_size: (i+1)*batch_size]\n",
    "        \n",
    "def split_batches(gen, batch_size):\n",
    "    for size, df in gen:\n",
    "        if size % batch_size == 0:\n",
    "            for i in range(size//batch_size):\n",
    "                if isinstance(df, tuple):\n",
    "                    yield batch_size, tuple(d[i*batch_size : (i+1)*batch_size] for d in df)\n",
    "                else:\n",
    "                    yield batch_size, df[i*batch_size : (i+1)*batch_size]\n",
    "        else:\n",
    "            raise Exception(\"Please specifie a batchsize compatible with the loaded chunks size\")\n",
    "        \n",
    "def buffer(gen,size):\n",
    "    ''' This generator buffer a number `size` of elements from an iterator and yields them. \n",
    "    When the buffer is empty the quee is filled again'''\n",
    "    q = deque()\n",
    "    while True:\n",
    "        # Caching in the the queue some number of elements\n",
    "        in_q = 0\n",
    "        try:\n",
    "            for _ in range(size):\n",
    "                q.append(next(gen))\n",
    "                in_q +=1\n",
    "        except StopIteration:\n",
    "            for _ in range(in_q):\n",
    "                yield q.popleft()\n",
    "            break\n",
    "        # Now serve them\n",
    "        for _ in range(in_q):\n",
    "            yield q.popleft()\n",
    "        \n",
    "def shuffle_fn(size, df):\n",
    "    try:\n",
    "        perm_i = np.random.permutation(size)\n",
    "        return size, df[perm_i]\n",
    "    except:\n",
    "        return 0, ak.Array([])\n",
    "    \n",
    "    \n",
    "def shuffle_dataset(gen, n_batches=None):\n",
    "    if n_batches==None: \n",
    "        # permute the single batch\n",
    "        for i, (size, df) in enumerate(gen):\n",
    "            yield shuffle_fn(size, df)\n",
    "    else:\n",
    "        for dflist in cache_generator(gen, n_batches):\n",
    "            size = dflist[0][0] \n",
    "            perm_i = np.random.permutation(size*len(dflist))\n",
    "            totdf = ak.concatenate([df[1] for df in dflist])[perm_i]\n",
    "            for i in range(n_batches):\n",
    "                yield size, totdf[i*size: (i+1)*size]\n",
    "                \n",
    "def zip_datasets(*iterables):\n",
    "    yield from zip_longest(*iterables, fillvalue=(0, ak.Array([])))\n",
    "    \n",
    "def concat_fn(dfs):\n",
    "    return sum([d[0] for d in dfs]), ak.concatenate([d[1] for d in dfs])\n",
    "\n",
    "def concat_datasets(*iterables):\n",
    "    for dfs in zip_datasets(*iterables):\n",
    "        yield concat_fn(dfs)\n",
    "        \n",
    "def to_flat_numpy(X, axis=2, allow_missing=True):\n",
    "    return np.stack([ak.to_numpy(X[f], allow_missing=allow_missing) for f in X.fields], axis=axis)\n",
    "\n",
    "def convert_to_tf(df):\n",
    "    return [ tf.convert_to_tensor(d) for d in df ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7552cf80-e601-44cf-9f6d-e08d557aa820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batches_from_files_generator(config, preprocessing_fn):\n",
    "    '''\n",
    "    The function provides a generator producing batches of data from a list of files (usually ele+gamma). \n",
    "    It is intended to use with the multiprocessing generator. \n",
    "    The config contains all the options. \n",
    "    \n",
    "    config = { \n",
    "            \"file_input_columns\":  [\"cl_features\", \"cl_labels\", \"window_features\", \"window_metadata\", \"cl_h\"]\n",
    "            \"columns\": {\"cl_features\": cl_x_f, \"cl_labels\":cl_y_f,\n",
    "                          \"window_metadata\":[\"flavour\", \"nVtx\", \"rho\", \"obsPU\", \"truePU\",\"ncls\",\"ncls\", \"nclusters_insc\"]}, \n",
    "              \"padding\" : True,\n",
    "              \"read_hits\":True, \n",
    "              \"ncls_padding\" : 45, \n",
    "              \"nhits_padding\": 45,\n",
    "              \"chunk_size\": 5120\n",
    "              \"batch_size\": 512,\n",
    "              \"maxevents\": 2500000 # this is the global threshold and it is applied later to stop the multiprocessing generator\n",
    "              \"offset\" : 0\n",
    "             }\n",
    "        '''\n",
    "    def _fn(files): \n",
    "        # Parquet files\n",
    "        dfs_raw = [ ak.from_parquet(file, lazy=True, use_threads=True, columns=config[\"file_input_columns\"]) for file in files if file!=None]\n",
    "        # Loading chunks from the files\n",
    "        initial_dfs = [ load_dataset_chunks(df, config, chunk_size=config[\"chunk_size\"], offset=config[\"offset\"]) for df in dfs_raw] \n",
    "        # Contatenate the chunks from the list of files\n",
    "        concat_df = concat_datasets(*initial_dfs)\n",
    "        # Shuffle the axis=0\n",
    "        shuffled = shuffle_dataset(concat_df)\n",
    "        # Processing the data to extract X,Y, etc\n",
    "        _preprocess_fn = preprocessing_fn(config)\n",
    "        processed  = (_preprocess_fn(d) for d in shuffled)\n",
    "        # Split in batches\n",
    "        #yield from processed\n",
    "        yield from split_batches(processed, config[\"batch_size\"])\n",
    "    \n",
    "    return _fn\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7888b5fa-e1df-444b-9c66-6b3628cf1490",
   "metadata": {},
   "source": [
    "### Multiprocessing generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5f1bcc-e12c-4a1a-8155-b9558d833b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def multiprocessor_generator(iterable, heavy_processing, input_queue_size=10, output_queue_size=20, nworkers=2):\n",
    "    def gen_to_queue(input_q, iterable):\n",
    "        # This function simply consume our generator and write it to the input queue\n",
    "        for it in iterable:\n",
    "            input_q.put(it)\n",
    "        for _ in range(nworkers):    # Once generator is consumed, send end-signal\n",
    "            input_q.put(None)\n",
    "\n",
    "    def process(input_q, output_q):\n",
    "        # Change the random seed for each processor\n",
    "        pid = mp.current_process().pid\n",
    "        np.random.seed()\n",
    "        while True:\n",
    "            it = input_q.get()\n",
    "            if it is None:\n",
    "                output_q.put(None)\n",
    "                break\n",
    "            output_q.put(heavy_processing(it))\n",
    "    \n",
    "    input_q = mp.Queue(maxsize=input_queue_size)\n",
    "    output_q = mp.Queue(maxsize=output_queue_size)\n",
    "\n",
    "    # Here we need 3 groups of worker :\n",
    "    # * One that will consume the input generator and put it into a queue. It will be `gen_pool`. It's ok to have only 1 process doing this, since this is a very light task\n",
    "    # * One that do the main processing. It will be `pool`.\n",
    "    # * One that read the results and yield it back, to keep it as a generator. The main thread will do it.\n",
    "    gen_pool = mp.Pool(1, initializer=gen_to_queue, initargs=(input_q, iterable))\n",
    "    pool = mp.Pool(nworkers, initializer=process, initargs=(input_q, output_q))\n",
    "    \n",
    "    \n",
    "    try : \n",
    "        finished_workers = 0\n",
    "        while True:\n",
    "            it = output_q.get()\n",
    "            if it is None:\n",
    "                finished_workers += 1\n",
    "                if finished_workers == nworkers:\n",
    "                    break\n",
    "            else:\n",
    "                yield it\n",
    "    finally: \n",
    "        # This is called at GeneratorExit\n",
    "        gen_pool.close()\n",
    "        pool.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f726746b-87af-4a79-99a8-29fcd31678b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiprocessor_generator_from_files(files, internal_generator, output_queue_size=40, nworkers=4, maxevents=None):\n",
    "    '''\n",
    "    Generator with multiprocessing working on a list of input files.\n",
    "    All the input files are put in a Queue that is consumed by a Pool of workers. \n",
    "    Each worker passes the file to the `internal_generator` and consumes it. \n",
    "    The output is put in an output Queue which is consumed by the main thread.\n",
    "    Doing so the processing is in parallel. \n",
    "    '''\n",
    "    def process(input_q, output_q):\n",
    "        # Change the random seed for each processor\n",
    "        pid = mp.current_process().pid\n",
    "        np.random.seed()\n",
    "        while True:\n",
    "            file = input_q.get()\n",
    "            if file is None:\n",
    "                output_q.put(None)\n",
    "                break\n",
    "            # We give the file to the generator and then yield from it\n",
    "            for out in internal_generator(file):\n",
    "                output_q.put(out)\n",
    "    \n",
    "    input_q = mp.Queue()\n",
    "    # Load all the files in the input file\n",
    "    for file in files: \n",
    "        input_q.put(file)\n",
    "    # Once generator is consumed, send end-signal\n",
    "    for i in range(nworkers):\n",
    "        input_q.put(None)\n",
    "    \n",
    "    #output_q = mp.Queue(maxsize=output_queue_size)\n",
    "    output_q = mp.SimpleQueue()\n",
    "    # Here we need 2 groups of worker :\n",
    "    # * One that do the main processing. It will be `pool`.\n",
    "    # * One that read the results and yield it back, to keep it as a generator. The main thread will do it.\n",
    "    pool = mp.Pool(nworkers, initializer=process, initargs=(input_q, output_q))\n",
    "    \n",
    "    try : \n",
    "        finished_workers = 0\n",
    "        tot_events = 0\n",
    "        while True:\n",
    "            it = output_q.get()\n",
    "            if it is None:\n",
    "                finished_workers += 1\n",
    "                if finished_workers == nworkers:\n",
    "                    break\n",
    "            else:\n",
    "                size, df = it\n",
    "                tot_events += size\n",
    "                if maxevents and tot_events > maxevents:\n",
    "                    break\n",
    "                else:\n",
    "                    yield it\n",
    "    finally: \n",
    "        # This is called at GeneratorExit\n",
    "        pool.close()\n",
    "        pool.terminate()\n",
    "        print(\"Multiprocessing generator closed\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f92b5e",
   "metadata": {},
   "source": [
    "## Getting features and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91055f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T11:45:09.910996Z",
     "start_time": "2022-07-28T11:45:09.893560Z"
    }
   },
   "outputs": [],
   "source": [
    "default_features_dict = {\n",
    "        \"cl_features\" : [ \"en_cluster\",\"et_cluster\",\n",
    "                        \"cluster_eta\", \"cluster_phi\", \n",
    "                        \"cluster_ieta\",\"cluster_iphi\",\"cluster_iz\",\n",
    "                        \"cluster_deta\", \"cluster_dphi\",\n",
    "                        \"cluster_den_seed\",\"cluster_det_seed\",\n",
    "                        \"en_cluster_calib\", \"et_cluster_calib\",\n",
    "                        \"cl_f5_r9\", \"cl_f5_sigmaIetaIeta\", \"cl_f5_sigmaIetaIphi\",\n",
    "                        \"cl_f5_sigmaIphiIphi\",\"cl_f5_swissCross\",\n",
    "                        \"cl_r9\", \"cl_sigmaIetaIeta\", \"cl_sigmaIetaIphi\",\n",
    "                        \"cl_sigmaIphiIphi\",\"cl_swissCross\",\n",
    "                        \"cl_nxtals\", \"cl_etaWidth\",\"cl_phiWidth\"],\n",
    "\n",
    "\n",
    "    \"cl_metadata\": [ \"calo_score\", \"calo_simen_sig\", \"calo_simen_PU\",\n",
    "                     \"cluster_PUfrac\",\"calo_nxtals_PU\",\n",
    "                     \"noise_en\",\"noise_en_uncal\",\"noise_en_nofrac\",\"noise_en_uncal_nofrac\" ],\n",
    "\n",
    "    \"cl_labels\" : [\"is_seed\",\"is_calo_matched\",\"is_calo_seed\", \"in_scluster\",\"in_geom_mustache\",\"in_mustache\"],\n",
    "\n",
    "    \n",
    "    \"seed_features\" : [\"seed_eta\",\"seed_phi\", \"seed_ieta\",\"seed_iphi\", \"seed_iz\", \n",
    "                     \"en_seed\", \"et_seed\",\"en_seed_calib\",\"et_seed_calib\",\n",
    "                    \"seed_f5_r9\",\"seed_f5_sigmaIetaIeta\", \"seed_f5_sigmaIetaIphi\",\n",
    "                    \"seed_f5_sigmaIphiIphi\",\"seed_f5_swissCross\",\n",
    "                    \"seed_r9\",\"seed_sigmaIetaIeta\", \"seed_sigmaIetaIphi\",\n",
    "                    \"seed_sigmaIphiIphi\",\"seed_swissCross\",\n",
    "                    \"seed_nxtals\",\"seed_etaWidth\",\"seed_phiWidth\"\n",
    "                    ],\n",
    "\n",
    "    \"seed_metadata\": [ \"seed_score\", \"seed_simen_sig\", \"seed_simen_PU\", \"seed_PUfrac\"],\n",
    "    \"seed_labels\" : [ \"is_seed_calo_matched\", \"is_seed_calo_seed\", \"is_seed_mustache_matched\"],\n",
    "\n",
    "     \"window_features\" : [ \"max_en_cluster\",\"max_et_cluster\",\"max_deta_cluster\",\"max_dphi_cluster\",\"max_den_cluster\",\"max_det_cluster\",\n",
    "                         \"min_en_cluster\",\"min_et_cluster\",\"min_deta_cluster\",\"min_dphi_cluster\",\"min_den_cluster\",\"min_det_cluster\",\n",
    "                         \"mean_en_cluster\",\"mean_et_cluster\",\"mean_deta_cluster\",\"mean_dphi_cluster\",\"mean_den_cluster\",\"mean_det_cluster\" ],\n",
    "\n",
    "    \"window_metadata\": [\"flavour\", \"ncls\", \"nclusters_insc\",\n",
    "                        \"nVtx\", \"rho\", \"obsPU\", \"truePU\",\n",
    "                        \"sim_true_eta\", \"sim_true_phi\",  \n",
    "                        \"gen_true_eta\",\"gen_true_phi\",\n",
    "                        \"en_true_sim\",\"et_true_sim\", \"en_true_gen\", \"et_true_gen\",\n",
    "                        \"en_true_sim_good\", \"et_true_sim_good\",\n",
    "                        \"en_mustache_raw\", \"et_mustache_raw\",\"en_mustache_calib\", \"et_mustache_calib\",\n",
    "                        \"max_en_cluster_insc\",\"max_deta_cluster_insc\",\"max_dphi_cluster_insc\",\n",
    "                        \"event_tot_simen_PU\",\"wtot_simen_PU\",\"wtot_simen_sig\" ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813d4de-8633-4a53-833c-623c6defe3f3",
   "metadata": {},
   "source": [
    "### Example of branches selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcdbe587-69da-474d-8419-1e6730b1f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_x_f = [\"en_cluster\",\"et_cluster\",\n",
    "            \"cluster_eta\", \"cluster_phi\", \n",
    "            \"cluster_ieta\",\"cluster_iphi\",\"cluster_iz\",\n",
    "            \"cluster_deta\", \"cluster_dphi\",\n",
    "            \"cluster_den_seed\",\"cluster_det_seed\",\n",
    "            \"en_cluster_calib\", \"et_cluster_calib\",\n",
    "            \"cl_f5_r9\", \"cl_f5_sigmaIetaIeta\", \"cl_f5_sigmaIetaIphi\",\n",
    "            \"cl_f5_sigmaIphiIphi\",\"cl_f5_swissCross\",\n",
    "            \"cl_r9\", \"cl_sigmaIetaIeta\", \"cl_sigmaIetaIphi\",\n",
    "            \"cl_sigmaIphiIphi\",\"cl_swissCross\",\n",
    "            \"cl_nxtals\", \"cl_etaWidth\",\"cl_phiWidth\"]\n",
    "\n",
    "cl_y_f = [\"in_scluster\", \"is_seed\"] \n",
    "\n",
    "wind_m = [\"flavour\", \"nVtx\", \"rho\", \"obsPU\", \"truePU\",\"ncls\",\"ncls\", \"nclusters_insc\"]\n",
    "\n",
    "wind_x = [ \"max_en_cluster\",\"max_et_cluster\",\"max_deta_cluster\",\"max_dphi_cluster\",\"max_den_cluster\",\"max_det_cluster\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed10c0b-475b-4649-bd00-de7061914350",
   "metadata": {},
   "source": [
    "### Normalization factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "051c7c4b-f447-4ffa-8bfa-4d66984a68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ele_all = ak.from_parquet(inputdir_ele, lazy=True)\n",
    "df_gamma_all = ak.from_parquet(inputdir_gamma, lazy=True)\n",
    "\n",
    "df_tot = ak.concatenate([df_ele_all, df_gamma_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "24136d0d-ee22-4d50-999e-7508bb259d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_factor = { \"cluster\" : { \"mean\" : {}, \"max\": {}, \"min\": {}, \"std\": {}},\n",
    "               \"window\":  { \"mean\" : {}, \"max\": {}, \"min\": {}, \"std\": {}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed6a2d65-9ddd-4b85-b6ee-0a28203fed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in default_features_dict[\"cl_features\"]:\n",
    "    norm_factor[\"cluster\"][\"mean\"][cl] = ak.mean(df_tot.cl_features[cl])\n",
    "    norm_factor[\"cluster\"][\"std\"][cl] = ak.std(df_tot.cl_features[cl])\n",
    "    norm_factor[\"cluster\"][\"min\"][cl] = ak.min(df_tot.cl_features[cl])\n",
    "    norm_factor[\"cluster\"][\"max\"][cl] = ak.max(df_tot.cl_features[cl])\n",
    "    \n",
    "for wi in default_features_dict[\"window_features\"]:\n",
    "    norm_factor[\"window\"][\"mean\"][wi] = ak.mean(df_tot.window_features[wi])\n",
    "    norm_factor[\"window\"][\"std\"][wi] = ak.std(df_tot.window_features[wi])\n",
    "    norm_factor[\"window\"][\"min\"][wi] = ak.min(df_tot.window_features[wi])\n",
    "    norm_factor[\"window\"][\"max\"][wi] = ak.max(df_tot.window_features[wi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03ec6cb4-f4ae-47d1-a212-be83b8fb476c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm_factor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m norm_fact_awk \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mRecord(\u001b[43mnorm_factor\u001b[49m)\n\u001b[1;32m      2\u001b[0m ak\u001b[38;5;241m.\u001b[39mto_json(norm_fact_awk, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalization_factor_v1.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, pretty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'norm_factor' is not defined"
     ]
    }
   ],
   "source": [
    "norm_fact_awk = ak.Record(norm_factor)\n",
    "ak.to_json(norm_fact_awk, \"normalization_factor_v1.json\", pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e5b469-47fc-4b85-b654-62de2d63e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_factors(cl_features, wind_features, numpy=True):\n",
    "    # Loading the factors from file\n",
    "    norm_factors = ak.from_json(\"normalization_factors_v1.json\")\n",
    "    if numpy:\n",
    "        return {\n",
    "            \"cluster\" : {\n",
    "                \"mean\": to_flat_numpy(norm_factors[\"cluster\"][\"mean\"][cl_features], axis=0),\n",
    "                \"std\": to_flat_numpy(norm_factors[\"cluster\"][\"std\"][cl_features], axis=0),\n",
    "                \"min\": to_flat_numpy(norm_factors[\"cluster\"][\"min\"][cl_features], axis=0),\n",
    "                \"max\": to_flat_numpy(norm_factors[\"cluster\"][\"max\"][cl_features], axis=0)\n",
    "            },\n",
    "            \"window\":{\n",
    "                \"mean\": to_flat_numpy(norm_factors[\"window\"][\"mean\"][wind_features], axis=0),\n",
    "                \"std\": to_flat_numpy(norm_factors[\"window\"][\"std\"][wind_features], axis=0),\n",
    "                \"min\": to_flat_numpy(norm_factors[\"window\"][\"min\"][wind_features], axis=0),\n",
    "                \"max\": to_flat_numpy(norm_factors[\"window\"][\"max\"][wind_features], axis=0)\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"cluster\" : {\n",
    "                \"mean\": norm_factors[\"cluster\"][\"mean\"][cl_features],\n",
    "                \"std\": norm_factors[\"cluster\"][\"std\"][cl_features],\n",
    "                \"min\": norm_factors[\"cluster\"][\"min\"][cl_features],\n",
    "                \"max\": norm_factors[\"cluster\"][\"max\"][cl_features],\n",
    "            },\n",
    "            \"window\":{\n",
    "                \"mean\": norm_factors[\"window\"][\"mean\"][wind_features],\n",
    "                \"std\": norm_factors[\"window\"][\"std\"][wind_features], \n",
    "                \"min\": norm_factors[\"window\"][\"min\"][wind_features], \n",
    "                \"max\": norm_factors[\"window\"][\"max\"][wind_features], \n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df848f-853b-4b01-9602-dd5c6e960c91",
   "metadata": {},
   "source": [
    "## Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "213503b4-6729-45a0-a412-38b1dc66cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    " def preprocessing(config):\n",
    "    '''\n",
    "    config must contains\n",
    "    - ncls_padding\n",
    "    - nhits_padding\n",
    "    '''\n",
    "    def process_fn(data): \n",
    "        size, df = data\n",
    "        # Extraction of the ntuples and zero padding\n",
    "\n",
    "        #padding\n",
    "        if config[\"padding\"]:\n",
    "            if config[\"ncls_padding\"] == -1:\n",
    "                # dynamic padding\n",
    "                max_ncls = ak.max(ak.num(df.cl_features, axis=1))\n",
    "            else:\n",
    "                max_ncls = config[\"ncls_padding\"]\n",
    "            if config[\"nhits_padding\"] == -1:\n",
    "                max_nhits = ak.max(ak.num(df.cl_h, axis=2))\n",
    "            else:\n",
    "                max_nhits = config[\"nhits_padding\"]\n",
    "\n",
    "            cls_X_pad = ak.pad_none(df.cl_features, max_ncls, clip=True)\n",
    "            cls_Y_pad = ak.pad_none(df.cl_labels, max_ncls, clip=True)\n",
    "            wind_X = df.window_features\n",
    "            wind_meta = df.window_metadata\n",
    "            is_seed_pad = ak.pad_none(df.cl_labels[\"is_seed\"], max_ncls, clip=True)\n",
    "\n",
    "            # cls_X_pad = ak.fill_none(cls_X_pad, {k:0 for k in config[\"columns\"][\"cl_features\"]})\n",
    "            # cls_Y_pad = ak.fill_none(cls_Y_pad, 0.)\n",
    "            # is_seed_pad = ak.fill_none(is_seed_pad, False)\n",
    "            # hits padding\n",
    "            cl_hits_padrec = ak.pad_none(df.cl_h, max_nhits, axis=2, clip=True) # --> pad rechits dim\n",
    "            cl_hits_padded = ak.pad_none(cl_hits_padrec, max_ncls, axis=1, clip=True) # --> pad ncls dimension\n",
    "            # h_padh_padcl_fillnoneCL = ak.fill_none(h_padh_padcl, [None]*max_nhits, axis=1) #-- > fill the out dimension with None\n",
    "            # cl_hits_pad = np.asarray(ak.fill_none(h_padh_padcl_fillnoneCL, [0.,0.,0.,0.] , axis=2)) # --> fill the padded rechit dim with 0.\n",
    "           \n",
    "            cls_X_pad_n = to_flat_numpy(cls_X_pad, axis=2, allow_missing=True)\n",
    "            cls_Y_pad_n = to_flat_numpy(cls_Y_pad, axis=2, allow_missing=True)\n",
    "            is_seed_pad_n = ak.to_numpy(is_seed_pad, allow_missing=True)\n",
    "            cl_hits_pad_n = ak.to_numpy(cl_hits_padded, allow_missing=True)\n",
    "            wind_X_n = to_flat_numpy(wind_X, axis=1)\n",
    "            wind_meta_n = to_flat_numpy(wind_meta, axis=1)\n",
    "            \n",
    "            # Masks for padding\n",
    "            hits_mask = np.array(np.any(~cl_hits_pad_n.mask, axis=-1), dtype=int)\n",
    "            cls_mask = np.array(np.any(hits_mask, axis=-1), dtype=int)\n",
    "            #adding the last dim for broadcasting the 0s\n",
    "            hits_mask = hits_mask[:,:,:,None]\n",
    "            cls_mask = cls_mask[:,:,None]\n",
    "            \n",
    "            \n",
    "            # Normalization\n",
    "            norm_fact = config[\"norm_factors\"]\n",
    "            if config[\"norm_type\"] == \"stdscale\":\n",
    "                # With remasking\n",
    "                cls_X_pad_n = ((cls_X_pad_n - norm_fact[\"cluster\"][\"mean\"])/ norm_fact[\"cluster\"][\"std\"] ) * cls_mask\n",
    "                wind_X_n =  ((wind_X_n - norm_fact[\"window\"][\"mean\"])/ norm_fact[\"window\"][\"std\"] )  \n",
    "            elif config[\"norm_type\"] == \"minmax\":\n",
    "                cls_X_pad_n = ((cls_X_pad_n - norm_fact[\"cluster\"][\"min\"])/ (norm_fact[\"cluster\"][\"max\"]-norm_fact[\"cluster\"][\"min\"])) * cls_mask\n",
    "                wind_X_n =  ((wind_X_n - norm_fact[\"window\"][\"min\"])/ (norm_fact[\"window\"][\"max\"]-norm_fact[\"window\"][\"min\"]) )  \n",
    "            \n",
    "            flavour = np.asarray(df.window_metadata.flavour)\n",
    "            \n",
    "            return size, ( cls_X_pad_n, cls_Y_pad_n, is_seed_pad_n, cl_hits_pad_n,  wind_X_n, wind_meta_n, flavour, hits_mask, cls_mask)\n",
    "        else:\n",
    "            cls_X = df.cl_features, max_ncls\n",
    "            cls_Y = df.cl_labels[\"in_scluster\"], max_ncls\n",
    "            is_seed = df.cl_labels[\"is_seed\"], max_ncls\n",
    "            cl_hits = df.cl_h\n",
    "            return size, (cls_X, cls_Y, is_seed, cl_hits, flavour)\n",
    "            \n",
    "    return process_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc86ec98-c349-447c-8ac5-47524d24f7ec",
   "metadata": {},
   "source": [
    "###  Running configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2be756b-07b1-4a9c-82d3-7bb380feebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { \n",
    "                \"file_input_columns\":  [\"cl_features\", \"cl_labels\", \"window_features\", \"window_metadata\", \"cl_h\"],\n",
    "                \"columns\": {\"cl_features\": cl_x_f, \"cl_labels\":cl_y_f,\n",
    "                              \"window_metadata\":wind_m, \"window_features\":wind_x},\n",
    "                \"norm_factors\": get_norm_factors(cl_x_f, wind_x), # record with all the factors\n",
    "                \"norm_type\" : \"stdscale\", #minmax or stdscale\n",
    "                  \"padding\" : True,\n",
    "                  \"read_hits\":True, \n",
    "                  \"ncls_padding\" : -1, \n",
    "                  \"nhits_padding\": -1,\n",
    "                  \"chunk_size\": 256*20,\n",
    "                  \"batch_size\": 256,\n",
    "                  \"maxevents\": 500000,\n",
    "                  \"offset\" : 0,\n",
    "                 }\n",
    "\n",
    "input_files = zip_longest(glob(inputdir_ele+\"/*.parquet\"),\n",
    "                          glob(inputdir_gamma+\"/*.parquet\"))\n",
    "\n",
    "file_loader_generator = load_batches_from_files_generator(config, preprocessing)\n",
    "\n",
    "multidataset = multiprocessor_generator_from_files(input_files, file_loader_generator, output_queue_size=300, nworkers=2, maxevents=config[\"maxevents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e5d8db5-dc6d-422e-a424-c3dbe0bf09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "multidataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc3cbe-c777-401d-a02a-dc54eb8bedb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, (size, df) in enumerate(multidataset):\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95bb2717-625b-4199-a0a3-68a128dd1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = next(multidataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9fc756-cec1-4435-8c59-2324b50e1ddd",
   "metadata": {},
   "source": [
    "### Alternative example:\n",
    "Single thread input and multiprocessing of chunks: limited by IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df1efe5-34bf-45ba-90cc-1db87bbabe2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ele' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: cl_x_f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m:cl_y_f,\n\u001b[1;32m      5\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflavour\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnVtx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrho\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobsPU\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruePU\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mncls\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mncls\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnclusters_insc\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, \n\u001b[1;32m      6\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnhits_padding\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m45\u001b[39m,\n\u001b[1;32m     10\u001b[0m          }\n\u001b[0;32m---> 12\u001b[0m initial_ele \u001b[38;5;241m=\u001b[39m load_dataset_chunks(\u001b[43mdf_ele\u001b[49m, chunk_size,\u001b[38;5;241m2500000\u001b[39m, config)\n\u001b[1;32m     13\u001b[0m initial_gamma \u001b[38;5;241m=\u001b[39m load_dataset_chunks(df_gamma, chunk_size, \u001b[38;5;241m2500000\u001b[39m, config)\n\u001b[1;32m     14\u001b[0m ele_gamma \u001b[38;5;241m=\u001b[39m concat_datasets(initial_ele, initial_gamma)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_ele' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "chunk_size = batch_size *10\n",
    "\n",
    "config = { \"columns\": {\"cl_features\": cl_x_f, \"cl_labels\":cl_y_f,\n",
    "                      \"window_metadata\":[\"flavour\", \"nVtx\", \"rho\", \"obsPU\", \"truePU\",\"ncls\",\"ncls\", \"nclusters_insc\"]}, \n",
    "          \"padding\" : True,\n",
    "          \"read_hits\":True, \n",
    "          \"ncls_padding\" : 45, \n",
    "          \"nhits_padding\": 45,\n",
    "         }\n",
    "\n",
    "initial_ele = load_dataset_chunks(df_ele, chunk_size,2500000, config)\n",
    "initial_gamma = load_dataset_chunks(df_gamma, chunk_size, 2500000, config)\n",
    "ele_gamma = concat_datasets(initial_ele, initial_gamma)\n",
    "shuffled = shuffle_dataset(ele_gamma)\n",
    "preprocess_fn = preprocessing(config)\n",
    "\n",
    "m = multiprocessor_generator(shuffled, preprocess_fn, 8, 6, nworkers=4)\n",
    "final = split_batches(m, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c5dcf39-7a36-48e4-b7e9-affec225a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9994bf39-937a-4483-8e8c-ae8819f1299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.5 ms ± 5.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "next(ele_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bc437ba-8f4a-4fa2-9406-b2ac519f863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54 s ± 88.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "next(shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faac0a45-1f05-42b6-931f-e9b9ebcab23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.17 s ± 464 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "next(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac3d48-cd5e-4721-9efd-0bd81cdf86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, (size, df) in enumerate(shuffled):\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f4867-ad40-49d5-ad2e-ac641da81fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = \"/work/dvalsecc/Clustering/dataset/numpy_ntuples/all\"\n",
    "os.makedirs(output_dataset, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf79f3-fc3e-4005-8668-20efe8ce2fee",
   "metadata": {},
   "source": [
    "###  Test numpy export from awkward record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4877aa2-a7f1-4521-a095-38acd1ef3eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.1 s ± 250 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "X_numpy = np.transpose(np.asarray(ak.concatenate(ak.unzip(X[:, np.newaxis]), axis=1)),axes=(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "85cdd8a5-0b1e-4492-9fa3-33cb5c495e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509 ms ± 9.37 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "nps = []\n",
    "for fil in X.fields:\n",
    "    nps.append(ak.to_numpy(X[fil]))\n",
    "    \n",
    "out = np.stack(nps, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0234381-3f57-4a2a-afa0-3952850056e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.stack([ak.to_numpy(X[f]) for f in X.fields], axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd8e86-fba1-4b8b-98d7-74516ff99f70",
   "metadata": {},
   "source": [
    "# Tensorflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a57d12a-65b0-41b0-b272-542a0b9b57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3569375-651c-46e8-92f2-15c34c9c0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_generator(config):\n",
    "    def _gen():\n",
    "        print(\"Creating generator\")\n",
    "        file_loader_generator = load_batches_from_files_generator(config, preprocessing)\n",
    "        multidataset = multiprocessor_generator_from_files(config[\"input_files\"], \n",
    "                                                           file_loader_generator, \n",
    "                                                           output_queue_size=config[\"output_queue\"], \n",
    "                                                           nworkers=config[\"nworkers\"], \n",
    "                                                           maxevents=config.get(\"maxevents\", None))\n",
    "       \n",
    "        for size, df in multidataset:\n",
    "            tfs = convert_to_tf(df)\n",
    "            yield tuple(tfs)\n",
    "    return _gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b1aac69-3c7c-4b36-bb05-11ed49c740ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { \n",
    "    \"input_files\": list(zip_longest(glob(inputdir_ele+\"/*.parquet\"),\n",
    "                             glob(inputdir_gamma+\"/*.parquet\"))),\n",
    "    \"file_input_columns\":  [\"cl_features\", \"cl_labels\", \"window_features\", \"window_metadata\", \"cl_h\"],\n",
    "    \"columns\": {\"cl_features\": cl_x_f, \"cl_labels\":cl_y_f,\n",
    "                  \"window_metadata\":[\"flavour\", \"nVtx\", \"rho\", \"obsPU\", \"truePU\",\"ncls\",\"ncls\", \"nclusters_insc\"], \n",
    "                  \"window_features\":wind_x}, \n",
    "      \"norm_factors\": get_norm_factors(cl_x_f, wind_x), # record with all the factors\n",
    "      \"norm_type\" : \"stdscale\", #minmax or stdscale\n",
    "      \"padding\" : True,\n",
    "      \"read_hits\":True, \n",
    "      \"ncls_padding\" : -1, \n",
    "      \"nhits_padding\": -1,\n",
    "      \"chunk_size\": 256*20,\n",
    "      \"batch_size\": 256,\n",
    "      \"maxevents\": 1000000,\n",
    "      \"nworkers\": 3,\n",
    "      \"output_queue\":100,\n",
    "      \"offset\" : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20ef1350-478d-4291-8709-e711447d0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cls_X_pad_n, cls_Y_pad_n, is_seed_pad_n, cl_hits_pad_n,  wind_X_n, wind_meta_n, flavour, hits_mask, cls_mask\n",
    "df = tf.data.Dataset.from_generator(tf_generator(config), \n",
    "                                    output_signature= (\n",
    "                           tf.TensorSpec(shape=(None,None,len(config[\"columns\"][\"cl_features\"])), dtype=tf.float64), # cl_x (batch, ncls, #cl_x_features)\n",
    "                           tf.TensorSpec(shape=(None,None, len(config[\"columns\"][\"cl_labels\"])), dtype=tf.bool),  #cl_y (batch, ncls, #cl_labels)\n",
    "                           tf.TensorSpec(shape=(None,None), dtype=tf.bool),  # is seed (batch, ncls,)\n",
    "                           tf.TensorSpec(shape=(None,None, None, 4), dtype=tf.float64), #hits  (batch, ncls, nhits, 4)\n",
    "                           tf.TensorSpec(shape=(None,len(config[\"columns\"][\"window_features\"])), dtype=tf.float64),  #windox_X (batch, #wind_x)\n",
    "                           tf.TensorSpec(shape=(None,len(config[\"columns\"][\"window_metadata\"])), dtype=tf.float64),  #windox_metadata (batch, #wind_meta)\n",
    "                           tf.TensorSpec(shape=(None,), dtype=tf.int32),  # flavour (batch,)\n",
    "                           tf.TensorSpec(shape=(None,None,None,1), dtype=tf.int32), #hits mask\n",
    "                           tf.TensorSpec(shape=(None,None,1), dtype=tf.int32),   #clusters mask\n",
    "                                        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9fdf50f-af67-4eda-a3e6-5a1404ce6700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating generator\n",
      "0, 20, 40, 60, 80, 100, 120, 140, 160, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/dvalsecc/miniconda3/envs/clustering/lib/python3.10/site-packages/numpy/ma/core.py:1156: RuntimeWarning: overflow encountered in true_divide\n",
      "  result = self.f(da, db, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180, 200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520, 540, 560, 580, 600, 620, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/dvalsecc/miniconda3/envs/clustering/lib/python3.10/site-packages/numpy/ma/core.py:1156: RuntimeWarning: overflow encountered in true_divide\n",
      "  result = self.f(da, db, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640, 660, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/dvalsecc/miniconda3/envs/clustering/lib/python3.10/site-packages/numpy/ma/core.py:1156: RuntimeWarning: overflow encountered in true_divide\n",
      "  result = self.f(da, db, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680, 700, 720, 740, 760, 780, 800, 820, 840, 860, 880, 900, 920, 940, 960, 980, 1000, 1020, 1040, 1060, 1080, 1100, 1120, 1140, 1160, 1180, 1200, 1220, 1240, 1260, 1280, 1300, 1320, 1340, 1360, 1380, 1400, 1420, 1440, 1460, 1480, 1500, 1520, 1540, 1560, 1580, 1600, 1620, 1640, 1660, 1680, 1700, 1720, 1740, 1760, 1780, 1800, 1820, 1840, 1860, 1880, 1900, 1920, 1940, 1960, 1980, 2000, 2020, 2040, 2060, 2080, 2100, 2120, 2140, 2160, 2180, 2200, 2220, 2240, 2260, 2280, 2300, 2320, 2340, 2360, 2380, 2400, 2420, 2440, 2460, 2480, 2500, 2520, 2540, 2560, 2580, 2600, 2620, 2640, 2660, 2680, 2700, 2720, 2740, 2760, 2780, 2800, 2820, 2840, 2860, 2880, 2900, 2920, 2940, 2960, 2980, 3000, 3020, 3040, 3060, 3080, 3100, 3120, 3140, 3160, 3180, 3200, 3220, 3240, 3260, 3280, 3300, 3320, 3340, 3360, 3380, 3400, 3420, 3440, 3460, 3480, 3500, 3520, 3540, 3560, 3580, 3600, 3620, 3640, 3660, 3680, 3700, 3720, 3740, 3760, 3780, 3800, 3820, 3840, 3860, 3880, 3900, Multiprocessing generator closed\n",
      "CPU times: user 33.8 s, sys: 22.3 s, total: 56.1 s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, d in enumerate(df):\n",
    "    if i % 20 == 0:\n",
    "        print(i, end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba76dec-853f-488d-9fe2-3c6fae946d5c",
   "metadata": {},
   "source": [
    "# Test of the python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011abc05-c3fd-4261-89c5-bee291a80436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 10:51:02.383337: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-07 10:51:02.383369: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from awk_data import LoaderConfig, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836b42ed-9919-4891-bc71-e898081c902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_x_f = [\"en_cluster\",\"et_cluster\",\n",
    "            \"cluster_eta\", \"cluster_phi\", \n",
    "            \"cluster_ieta\",\"cluster_iphi\",\"cluster_iz\",\n",
    "            \"cluster_deta\", \"cluster_dphi\",\n",
    "            \"cluster_den_seed\",\"cluster_det_seed\",\n",
    "            \"en_cluster_calib\", \"et_cluster_calib\",\n",
    "            \"cl_f5_r9\", \"cl_f5_sigmaIetaIeta\", \"cl_f5_sigmaIetaIphi\",\n",
    "            \"cl_f5_sigmaIphiIphi\",\"cl_f5_swissCross\",\n",
    "            \"cl_r9\", \"cl_sigmaIetaIeta\", \"cl_sigmaIetaIphi\",\n",
    "            \"cl_sigmaIphiIphi\",\"cl_swissCross\",\n",
    "            \"cl_nxtals\", \"cl_etaWidth\",\"cl_phiWidth\"]\n",
    "\n",
    "cl_y_f = [\"in_scluster\", \"is_seed\"] \n",
    "\n",
    "wind_m = [\"flavour\", \"nVtx\", \"rho\", \"obsPU\", \"truePU\",\"ncls\",\"ncls\", \"nclusters_insc\"]\n",
    "\n",
    "wind_x = [ \"max_en_cluster\",\"max_et_cluster\",\"max_deta_cluster\",\"max_dphi_cluster\",\"max_den_cluster\",\"max_det_cluster\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839d67bf-2669-4786-9098-925ff3bcb979",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdir_ele = \"/work/dvalsecc/Clustering/dataset/electrons_awkward_2022v8/\"\n",
    "inputdir_gamma = \"/work/dvalsecc/Clustering/dataset/gammas_awkward_2022v8/\"\n",
    "\n",
    "config = { \n",
    "    \"input_folders\": [inputdir_ele, inputdir_gamma],\n",
    "    \"file_input_columns\":  [\"cl_features\", \"cl_labels\", \"window_features\", \"window_metadata\", \"cl_h\"],\n",
    "    \"columns\": {\"cl_features\": cl_x_f, \"cl_labels\":cl_y_f,\n",
    "                  \"window_metadata\":[\"flavour\", \"nVtx\", \"rho\", \"obsPU\", \"truePU\",\"ncls\",\"ncls\", \"nclusters_insc\"], \n",
    "                  \"window_features\":wind_x}, \n",
    "      \"norm_factors_file\": \"normalization_factors_v1.json\", # record with all the factors\n",
    "      \"norm_type\" : \"stdscale\", #minmax or stdscale\n",
    "      \"padding\" : True, \n",
    "      \"ncls_padding\" : -1, \n",
    "      \"nhits_padding\": -1,\n",
    "      \"chunk_size\": 256*20,\n",
    "      \"batch_size\": 256,\n",
    "      \"maxevents\": 1000000,\n",
    "      \"nworkers\": 3,\n",
    "      \"max_batches_in_memory\":100,\n",
    "      \"offset\" : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7a5afa-dfe6-469b-805a-8bd0a87ec06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = LoaderConfig(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d035b6-cf86-4a6f-98b4-87e6ea3929d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoaderConfig(input_files=[], input_folders=['/work/dvalsecc/Clustering/dataset/electrons_awkward_2022v8/', '/work/dvalsecc/Clustering/dataset/gammas_awkward_2022v8/'], file_input_columns=['cl_features', 'cl_labels', 'window_features', 'window_metadata', 'cl_h'], columns={'cl_features': ['en_cluster', 'et_cluster', 'cluster_eta', 'cluster_phi', 'cluster_ieta', 'cluster_iphi', 'cluster_iz', 'cluster_deta', 'cluster_dphi', 'cluster_den_seed', 'cluster_det_seed', 'en_cluster_calib', 'et_cluster_calib', 'cl_f5_r9', 'cl_f5_sigmaIetaIeta', 'cl_f5_sigmaIetaIphi', 'cl_f5_sigmaIphiIphi', 'cl_f5_swissCross', 'cl_r9', 'cl_sigmaIetaIeta', 'cl_sigmaIetaIphi', 'cl_sigmaIphiIphi', 'cl_swissCross', 'cl_nxtals', 'cl_etaWidth', 'cl_phiWidth'], 'cl_labels': ['in_scluster', 'is_seed'], 'window_metadata': ['flavour', 'nVtx', 'rho', 'obsPU', 'truePU', 'ncls', 'ncls', 'nclusters_insc'], 'window_features': ['max_en_cluster', 'max_et_cluster', 'max_deta_cluster', 'max_dphi_cluster', 'max_den_cluster', 'max_det_cluster']}, padding=True, ncls_padding=-1, nhits_padding=-1, chunk_size=5120, batch_size=256, maxevents=1000000, offset=0, norm_type='stdscale', norm_factors_file='normalization_factors_v1.json', norm_factors=None, nworkers=3, max_batches_in_memory=100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b25aa0-4ca7-4f73-b4e9-1b2628f7ba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-07 10:51:04.974126: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-07 10:51:04.974173: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-07 10:51:04.974207: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (t3ui01.psi.ch): /proc/driver/nvidia/version does not exist\n",
      "2022-09-07 10:51:04.974718: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "df = load_dataset(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee869dc3-8dcd-4b04-ac23-245bc9a71c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 20, 40, 60, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/dvalsecc/miniconda3/envs/clustering/lib/python3.10/site-packages/numpy/ma/core.py:1156: RuntimeWarning: overflow encountered in true_divide\n",
      "  result = self.f(da, db, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/dvalsecc/miniconda3/envs/clustering/lib/python3.10/site-packages/numpy/ma/core.py:1156: RuntimeWarning: overflow encountered in true_divide\n",
      "  result = self.f(da, db, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300, 320, 340, 360, 380, 400, 420, 440, 460, 480, 500, 520, 540, 560, 580, 600, 620, 640, 660, 680, 700, 720, 740, 760, 780, 800, 820, 840, 860, 880, 900, 920, 940, 960, 980, 1000, 1020, 1040, 1060, 1080, 1100, 1120, 1140, 1160, 1180, 1200, 1220, 1240, 1260, 1280, 1300, 1320, 1340, 1360, 1380, 1400, 1420, 1440, 1460, 1480, 1500, 1520, 1540, 1560, 1580, 1600, 1620, 1640, 1660, 1680, 1700, 1720, 1740, 1760, 1780, 1800, 1820, 1840, 1860, 1880, "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, d in enumerate(df):\n",
    "    if i % 20 == 0:\n",
    "        print(i, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c413c4ff-808f-4353-9ffa-947324f42849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.norm_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a194a0f-bdd8-4f59-bc07-e2a857803cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

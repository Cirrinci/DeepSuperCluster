{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing setGPU\n",
      "Could not import setGPU, please make sure you configure CUDA_VISIBLE_DEVICES manually\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import glob\n",
    "try:\n",
    "    if not (\"CUDA_VISIBLE_DEVICES\" in os.environ):\n",
    "        os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "        print(\"importing setGPU\")\n",
    "        import setGPU\n",
    "except:\n",
    "    print(\"Could not import setGPU, please make sure you configure CUDA_VISIBLE_DEVICES manually\")\n",
    "    pass\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import io\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import tensorflow as tf\n",
    "from numpy.lib.recfunctions import append_fields\n",
    "\n",
    "import scipy\n",
    "import scipy.special\n",
    "\n",
    "from mpnn import MessagePassing, ReadoutGraph, Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version=2.1.0, CUDA=True, GPU=True, TPU=False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json, os\n",
    "import numpy as np\n",
    "\n",
    "# Tested with TensorFlow 2.1.0\n",
    "print('version={}, CUDA={}, GPU={}, TPU={}'.format(\n",
    "    tf.__version__, tf.test.is_built_with_cuda(),\n",
    "    # GPU attached?\n",
    "    len(tf.config.list_physical_devices('GPU')) > 0,\n",
    "    # TPU accessible? (only works on Colab)\n",
    "    'COLAB_TPU_ADDR' in os.environ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpus= 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    num_gpus = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\"))\n",
    "    print(\"num_gpus=\", num_gpus)\n",
    "    if num_gpus > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(\"gpu:0\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"fallback to CPU\")\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(A,B):\n",
    "    na = tf.reduce_sum(tf.square(A), -1)\n",
    "    nb = tf.reduce_sum(tf.square(B), -1)\n",
    " \n",
    "    na = tf.reshape(na, [tf.shape(na)[0], -1, 1])\n",
    "    nb = tf.reshape(nb, [tf.shape(na)[0], 1, -1])\n",
    "    Dsq = tf.clip_by_value(na - 2*tf.linalg.matmul(A, B, transpose_a=False, transpose_b=True) + nb, 1e-12, 1e12)\n",
    "    D = tf.sqrt(Dsq)\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a list of [Nbatch, Nelem, Nfeat] input nodes, computes the dense [Nbatch, Nelem, Nelem] adjacency matrices\n",
    "class Distance(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, dist_shape, *args, **kwargs):\n",
    "        super(Distance, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs1, inputs2):\n",
    "        #compute the pairwise distance matrix between the vectors defined by the first two components of the input array\n",
    "        #inputs1, inputs2: [Nbatch, Nelem, distance_dim] embedded coordinates used for element-to-element distance calculation\n",
    "        D = dist(inputs1, inputs2)\n",
    "      \n",
    "        #adjacency between two elements should be high if the distance is small.\n",
    "        #this is equivalent to radial basis functions. \n",
    "        #self-loops adj_{i,i}=1 are included, as D_{i,i}=0 by construction\n",
    "        adj = tf.math.exp(-1.0*D)\n",
    "\n",
    "        #optionally set the adjacency matrix to 0 for low values in order to make the matrix sparse.\n",
    "        #need to test if this improves the result.\n",
    "        #adj = tf.keras.activations.relu(adj, threshold=0.01)\n",
    "\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_classes):\n",
    "        super(InputEncoding, self).__init__()\n",
    "        self.num_input_classes = num_input_classes\n",
    "        \n",
    "    def call(self, X):\n",
    "        #X: [Nbatch, Nelem, Nfeat] array of all the input detector element feature data\n",
    "\n",
    "        #X[:, :, 0] - categorical index of the element type\n",
    "        Xid = tf.one_hot(tf.cast(X[:, :, 0], tf.int32), self.num_input_classes)\n",
    "\n",
    "        #X[:, :, 1:] - all the other non-categorical features\n",
    "        Xprop = X[:, :, 1:]\n",
    "        return tf.concat([Xid, Xprop], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph Highway network\n",
    "# https://arxiv.org/pdf/2004.04635.pdf\n",
    "#https://github.com/gcucurull/jax-ghnet/blob/master/models.py \n",
    "class GHConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, k, *args, **kwargs):\n",
    "        self.activation = kwargs.pop(\"activation\")\n",
    "        self.hidden_dim = args[0]\n",
    "        self.k = k\n",
    "\n",
    "        super(GHConv, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.W_t = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"w_t\", initializer=\"random_normal\")\n",
    "        self.b_t = self.add_weight(shape=(self.hidden_dim, ), name=\"b_t\", initializer=\"zeros\")\n",
    "        self.W_h = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"w_h\", initializer=\"random_normal\")\n",
    "        self.theta = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"theta\", initializer=\"random_normal\")\n",
    " \n",
    "    def call(self, x, adj):\n",
    "        #compute the normalization of the adjacency matrix\n",
    "        in_degrees = tf.reduce_sum(adj, axis=-1)\n",
    "        #add epsilon to prevent numerical issues from 1/sqrt(x)\n",
    "        norm = tf.expand_dims(tf.pow(in_degrees + 1e-6, -0.5), -1)\n",
    "        norm_k = tf.pow(norm, self.k)\n",
    "        adj_k = tf.pow(adj, self.k)\n",
    "\n",
    "        f_hom = tf.linalg.matmul(x, self.theta)\n",
    "        f_hom = tf.linalg.matmul(adj_k, f_hom*norm_k)*norm_k\n",
    "\n",
    "        f_het = tf.linalg.matmul(x, self.W_h)\n",
    "        gate = tf.nn.sigmoid(tf.linalg.matmul(x, self.W_t) + self.b_t)\n",
    "        #tf.print(tf.reduce_mean(f_hom), tf.reduce_mean(f_het), tf.reduce_mean(gate))\n",
    "\n",
    "        out = gate*f_hom + (1-gate)*f_het\n",
    "        return out\n",
    "\n",
    "## Simple Graph Conv layer\n",
    "class SGConv(tf.keras.layers.Dense):\n",
    "    def __init__(self, k, *args, **kwargs):\n",
    "        super(SGConv, self).__init__(*args, **kwargs)\n",
    "        self.k = k\n",
    "    \n",
    "    def call(self, inputs, adj):\n",
    "        W = self.weights[0]\n",
    "        b = self.weights[1]\n",
    "\n",
    "        #compute the normalization of the adjacency matrix\n",
    "        in_degrees = tf.reduce_sum(adj, axis=-1)\n",
    "        #add epsilon to prevent numerical issues from 1/sqrt(x)\n",
    "        norm = tf.expand_dims(tf.pow(in_degrees + 1e-6, -0.5), -1)\n",
    "        norm_k = tf.pow(norm, self.k)\n",
    "\n",
    "        support = (tf.linalg.matmul(inputs, W))\n",
    "     \n",
    "        #k-th power of the normalized adjacency matrix is nearly equivalent to k consecutive GCN layers\n",
    "        adj_k = tf.pow(adj, self.k)\n",
    "        out = tf.linalg.matmul(adj_k, support*norm_k)*norm_k\n",
    "\n",
    "        return self.activation(out + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple message passing based on a matrix multiplication\n",
    "class DNNSuperCluster(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, activation=tf.nn.selu, \n",
    "                     hidden_dim_coord=256, hidden_dim_input=256, hidden_dim_id=256,     \n",
    "                     n_layers_input=2, n_layers_id=3, n_layers_coord=2,\n",
    "                     distance_dim=256, num_conv=4, convlayer=\"ghconv\", dropout=0.1):\n",
    "        super(DNNSuperCluster, self).__init__()\n",
    "        self.activation = activation\n",
    "\n",
    "        #self.enc = InputEncoding(3)\n",
    "        \n",
    "        # layers for distance coordinate extraction\n",
    "        self.layers_coord = [ ]\n",
    "        for i in range(n_layers_coord):\n",
    "            layer_coord_i = tf.keras.layers.Dense(hidden_dim_coord, activation=activation, name=\"disctcoords_\"+str(i))\n",
    "            self.layers_coord.append(layer_coord_i)\n",
    "\n",
    "        self.layer_distcoords = tf.keras.layers.Dense(distance_dim, activation=\"linear\", name=\"distcoords_final\")\n",
    "        self.layer_distance = Distance(distance_dim, name=\"distance\")\n",
    "\n",
    "        # layers for feature extraction \n",
    "        self.layers_input = [ ]\n",
    "        for i in range(n_layers_input):\n",
    "            layer_input_i = tf.keras.layers.Dense(hidden_dim_input, activation=activation, name=\"input_\"+str(i))\n",
    "            layer_input_i_do = tf.keras.layers.Dropout(dropout)\n",
    "            self.layers_input.append((layer_input_i, layer_input_i_do))\n",
    "        \n",
    "       \n",
    "\n",
    "        # Graph convolutions\n",
    "        if convlayer == \"sgconv\":\n",
    "            self.layer_conv1 = SGConv(num_conv, hidden_dim_input, activation=activation, name=\"conv1\")\n",
    "            #self.layer_conv2 = SGConv(num_conv, 2*hidden_dim+len(class_labels), activation=activation, name=\"conv2\")\n",
    "        elif convlayer == \"ghconv\":\n",
    "            self.layer_conv1 = GHConv(num_conv, hidden_dim_input, activation=activation, name=\"conv1\")\n",
    "            #self.layer_conv2 = GHConv(num_conv, 2*hidden_dim+len(class_labels), activation=activation, name=\"conv2\")\n",
    "\n",
    "        # Output layers\n",
    "        self.layers_id = [ ]\n",
    "        for i in range(n_layers_id):\n",
    "            layer_id_i = tf.keras.layers.Dense(hidden_dim_id, activation=activation, name=\"id_\"+str(i))\n",
    "            layer_id_i_do = tf.keras.layers.Dropout(dropout)\n",
    "            self.layers_id.append((layer_id_i, layer_id_i_do))\n",
    "            \n",
    "        # binary output logits\n",
    "        self.layer_id = tf.keras.layers.Dense(1, activation=\"linear\", name=\"out_id\")\n",
    "        \n",
    " \n",
    "    def predict_distancematrix(self, inputs, training=True):\n",
    "        x = inputs\n",
    "        for layer_coord in self.layers_coord:\n",
    "            x = layer_coord(x)\n",
    "\n",
    "        distcoords = self.layer_distcoords(x)\n",
    "\n",
    "        dm = self.layer_distance(distcoords, distcoords)\n",
    "        \n",
    "        # masking if the first element is -1\n",
    "        msk_elem = tf.expand_dims(tf.cast(inputs[:, :, 0] != -1, dtype=tf.float32), -1)\n",
    "        dm = dm*msk_elem\n",
    "\n",
    "        return dm\n",
    "\n",
    "    #@tf.function(input_signature=[tf.TensorSpec(shape=[None, 15], dtype=tf.float32)])\n",
    "    def call(self, inputs, training=True):\n",
    "        # separate cluster energies from rescaled inputs\n",
    "        X = inputs[:,:,1:]\n",
    "        cl_energies = inputs[:,:,0]\n",
    "        \n",
    "        msk_input = tf.expand_dims(tf.cast(X[:, :, 0] != -1, tf.float32), -1)\n",
    "\n",
    "        dm = self.predict_distancematrix(X, training=training)\n",
    "        \n",
    "        x = X\n",
    "        for layer_input, layer_input_do in self.layers_input:\n",
    "            x = layer_input(x)\n",
    "            x = layer_input_do(x, training)\n",
    "            \n",
    "        x = self.layer_conv1(x, dm)\n",
    "        \n",
    "        for layer_id, layer_id_do in self.layers_id:\n",
    "            x = layer_id(x)\n",
    "            x = layer_id_do(x, training)\n",
    "            \n",
    "        out_id_logits = self.layer_id(x)\n",
    "        \n",
    "        energies = tf.expand_dims(cl_energies, axis=-1)\n",
    "        # add the cluster energies in the output, in the future we can add here corrections\n",
    "        output = tf.concat([out_id_logits,energies], axis=-1)\n",
    "        # mask to 0 the padded output\n",
    "        output_masked = output * msk_input\n",
    "        \n",
    "        #return masked output logits and the predicted total energy\n",
    "        return output_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def separate_true(y):\n",
    "    # one-hot encoding for true label (signal,PU,noise)\n",
    "    # the padded elements have -1 so they are one_hot to (0,0)\n",
    "    #y_onehot = tf.one_hot(tf.cast(y[:,1:], tf.int32), nclass_labels)\n",
    "    ytrue = y[:, 1:]\n",
    "    true_en = y[:, 0]\n",
    "    mask = tf.cast(ytrue!=-1, tf.float32)\n",
    "    ytrue_msk = ytrue * mask\n",
    "    return ytrue_msk, true_en, mask\n",
    "\n",
    "#@tf.function\n",
    "def get_true_mask(y):\n",
    "    # mask for elements that should be included in supercluster\n",
    "    in_sc = tf.cast(y[:,1:] == 1., tf.float32)\n",
    "    # number of padding elements\n",
    "    padded = tf.reduce_sum(tf.cast(y[:,1:] == -1., tf.float32), axis=-1)\n",
    "    return in_sc, padded\n",
    "    \n",
    "#@tf.function\n",
    "def separate_pred(ypred):\n",
    "    ens = ypred[:,:,1]\n",
    "    ypred_ext = ypred[:,:,0]\n",
    "    # 0 not include in energy sum, 1 include in energy sum\n",
    "    # masked elements have pred_id=0 so they do not enter in the energy sum\n",
    "    predid_mask = tf.cast(tf.math.sigmoid(ypred_ext)[:,:] > 0.5, tf.float32)\n",
    "    # predicted total energy\n",
    "    pred_en =  tf.reduce_sum( ens * predid_mask, axis=-1)\n",
    "    # one-hot encoding for true label (signal,PU,noise)\n",
    "    return ypred_ext, pred_en, predid_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def mse_unreduced(true, pred):\n",
    "    return tf.math.pow(true-pred,2)\n",
    "\n",
    "#@tf.function\n",
    "def msle_unreduced(true, pred):\n",
    "    return tf.math.pow(tf.math.log(tf.math.abs(true) + 1.0) - tf.math.log(tf.math.abs(pred) + 1.0), 2)\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def my_loss_full(y_true, y_pred):\n",
    "    y_true_msk, true_en, true_mask = separate_true(y_true)\n",
    "    y_pred_msk, pred_en, pred_id = separate_pred(y_pred)\n",
    "    # since the padded y_true is -1 -> it gives [0,0] when it is onehot. The ypred for batched is [0,0] so the loss\n",
    "    # is automatically 0 for padded samples\n",
    "    #tf.print(y_true_msk, y_true_msk.shape)\n",
    "    #tf.print(y_pred_msk, y_true_msk.shape)\n",
    "    \n",
    "    # apply mask on loss\n",
    "    l1 = tf.nn.softmax_cross_entropy_with_logits(y_true_msk, y_pred_msk) * true_mask\n",
    "    #l1 = tf.keras.backend.binary_crossentropy(y_true_msk, y_pred_msk, from_logits=True) * true_mask\n",
    "    \n",
    "    #tf.print(l1)\n",
    "    # true energy loss\n",
    "    mask_outsc = tf.cast(true_en == 0., tf.float32)\n",
    "    mask_insc = tf.cast(true_en != 0., tf.float32)\n",
    "    n_outsc = tf.reduce_sum(mask_outsc)\n",
    "    n_insc = tf.reduce_sum(mask_insc)\n",
    "    \n",
    "    l2_en = mse_unreduced(true_en, pred_en)\n",
    "    l2_en_log = msle_unreduced(true_en, pred_en)\n",
    "    \n",
    "    # separate mean resolution for windows with Caloparticle or not\n",
    "    l2_en_outsc = tf.reduce_sum(l2_en * mask_outsc) / n_outsc\n",
    "    l2_en_insc = tf.reduce_sum(l2_en * mask_insc) / n_insc\n",
    "    l2_en_outsc_log = tf.reduce_sum(l2_en_log * mask_outsc) / n_outsc\n",
    "    l2_en_insc_log = tf.reduce_sum(l2_en_log * mask_insc) / n_insc\n",
    "    \n",
    "    ltot = 1e4*tf.reduce_mean(l1) + 20* l2_en_insc +   10*l2_en_outsc + 200* l2_en_insc_log  + 100* l2_en_outsc_log\n",
    "    \n",
    "    return ltot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_resolution_outsc(y_true, y_pred):\n",
    "    y_true_msk, true_en, true_mask = separate_true(y_true)\n",
    "    y_pred_msk, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_outsc = tf.cast(true_en == 0., tf.float32)\n",
    "    n_outsc = tf.reduce_sum(mask_outsc)\n",
    "    return tf.reduce_sum(mse_unreduced(true_en, pred_en)*mask_outsc) / n_outsc\n",
    "\n",
    "def energy_resolution_insc(y_true, y_pred):\n",
    "    y_true_msk, true_en, true_mask = separate_true(y_true)\n",
    "    y_pred_msk, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_insc = tf.cast(true_en != 0., tf.float32)\n",
    "    n_insc = tf.reduce_sum(mask_insc)\n",
    "    return tf.reduce_sum(mse_unreduced(true_en, pred_en)*mask_insc) / n_insc\n",
    "\n",
    "def energy_resolution_outsc_log(y_true, y_pred):\n",
    "    y_true_msk, true_en, true_mask = separate_true(y_true)\n",
    "    y_pred_msk, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_outsc = tf.cast(true_en == 0., tf.float32)\n",
    "    n_outsc = tf.reduce_sum(mask_outsc)\n",
    "    return tf.reduce_sum(msle_unreduced(true_en, pred_en)*mask_outsc) / n_outsc\n",
    "\n",
    "def energy_resolution_insc_log(y_true, y_pred):\n",
    "    y_true_msk, true_en, true_mask = separate_true(y_true)\n",
    "    y_pred_msk, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_insc = tf.cast(true_en != 0., tf.float32)\n",
    "    n_insc = tf.reduce_sum(mask_insc)\n",
    "    return tf.reduce_sum(msle_unreduced(true_en, pred_en)*mask_insc) / n_insc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpfn_metrics(y_true, y_pred):\n",
    "    y_true_mask, n_padded = get_true_mask(y_true)\n",
    "    y_false_mask = (tf.cast(y_true_mask == 0., tf.float32))\n",
    "    \n",
    "    # pred_id contains the last n_padded elements to 0 that will be always True negatives\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    \n",
    "    n_pos = tf.reduce_sum(y_true_mask, axis=-1)\n",
    "    n_neg = tf.reduce_sum(y_false_mask, axis=-1) - n_padded\n",
    "    \n",
    "    n_tot = n_neg + n_pos\n",
    "    \n",
    "    true_pos = tf.reduce_sum(pred_id * y_true_mask, axis=-1)\n",
    "    false_neg = n_pos - true_pos\n",
    "    \n",
    "    false_pos = tf.reduce_sum(pred_id * y_false_mask, axis=-1)\n",
    "    true_neg = n_neg - false_pos\n",
    "    \n",
    "    return n_tot, true_pos, false_neg, false_pos, true_neg, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp,tn,fp,fn):\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def recall(tp,tn,fp,fn):\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def accuracy(tp,tn,fp,fn):\n",
    "    return (tp+tn)/(tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Precision(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='precision', **kwargs):\n",
    "        super(Precision, self).__init__(name=name, **kwargs)\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        n_tot, true_pos, false_neg, false_pos, true_neg = get_tpfn_metrics(y_true, y_pred)\n",
    "        self.tp.assign_add(tf.reduce_sum(true_pos))\n",
    "        self.fp.assign_add(tf.reduce_sum(false_pos))\n",
    "\n",
    "    def result(self):\n",
    "        return self.tp / (self.tp + self.fp)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fp.assign(0)\n",
    "        \n",
    "class Recall(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='recall', **kwargs):\n",
    "        super(Recall, self).__init__(name=name, **kwargs)\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        n_tot, true_pos, false_neg, false_pos, true_neg = get_tpfn_metrics(y_true, y_pred)\n",
    "        self.tp.assign_add(tf.reduce_sum(true_pos))\n",
    "        self.fn.assign_add(tf.reduce_sum(false_neg))\n",
    "\n",
    "    def result(self):\n",
    "        return self.tp / (self.tp + self.fn)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fn.assign(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/storage/ECAL/training_data/window_data/electrons/recordio_v4\"\n",
    "models_path = \"/storage/ECAL/deepcluster/models/gcn_models_v9/\"\n",
    "\n",
    "#rain_steps_per_epoch = \n",
    "#eval_steps_per_epoch = 3e5 // batch_size\n",
    "from collections import namedtuple\n",
    "Args = namedtuple('args', [ 'models_path', 'load','nepochs','ntrain','nval','nfeatures',\n",
    "                            'n_seed_features','batch_size','lr_decay','lr',\n",
    "                            'hidden_dim_input','hidden_dim_coord', 'hidden_dim_id',\n",
    "                            'n_layers_input', 'n_layers_id', 'n_layers_coord',\n",
    "                           'distance_dim','num_conv','dropout','convlayer',\n",
    "                           'opt'])\n",
    "\n",
    "args = Args( \n",
    "models_path = models_path,\n",
    "load = False,\n",
    "nepochs = 100,\n",
    "ntrain = 500000,\n",
    "nval = 100000,\n",
    "nfeatures = 13,\n",
    "n_seed_features = 12,\n",
    "lr_decay = 0,\n",
    "lr = 0.00001,\n",
    "batch_size = 150,\n",
    "n_layers_input = 3,\n",
    "n_layers_id = 2,\n",
    "n_layers_coord = 2,\n",
    "hidden_dim_input = 100,\n",
    "hidden_dim_coord = 50,\n",
    "hidden_dim_id = 100,\n",
    "distance_dim = 30,\n",
    "num_conv = 2,\n",
    "dropout = 0.01,\n",
    "convlayer = 'ghconv',\n",
    "opt='adam'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features_clusters(X):\n",
    "    '''\n",
    "    'is_seed',\"cluster_deta\", \"cluster_dphi\", \"en_cluster\", \"et_cluster\",\n",
    "    \"cl_f5_r9\", \"cl_f5_sigmaIetaIeta\",\"cl_f5_sigmaIetaIphi\",\"cl_f5_sigmaIphiIphi\",\n",
    "    \"cl_f5_swissCross\", \"cl_nxtals\", \"cl_etaWidth\", \"cl_phiWidth\n",
    "    '''\n",
    "    x_mean = tf.constant( \n",
    "        [   0.,  -7.09402501e-04, -1.27142875e-04,  1.30375508e+00,  5.67249500e-01, \n",
    "            1.92096066e+00,  1.31476120e-02,  1.62948213e-05,  1.42948806e-02,\n",
    "            5.92920497e-01,  1.49597644e+00,  3.36213188e-03,  3.06446267e-03]\n",
    "        )\n",
    "\n",
    "    x_scale = tf.constant(\n",
    "        [  1.,  1.10279784e-01, 3.30488055e-01, 2.62605247e+00, 1.16284769e+00,\n",
    "            7.81094814e+00, 1.70392176e-02, 3.05995567e-04, 1.80176053e-02,\n",
    "            1.99316624e+00, 1.88845046e+00, 4.12315715e-03, 4.79639033e-03]       \n",
    "        )\n",
    "    return (X-x_mean)/ x_scale\n",
    "\n",
    "def scale_features_seed(X):\n",
    "    '''\n",
    "     \"seed_eta\", \"seed_iz\",\"en_seed\",\"et_seed\",\n",
    "     \"seed_f5_r9\", \"seed_f5_sigmaIetaIeta\",\"seed_f5_sigmaIetaIphi\",\"seed_f5_sigmaIphiIphi\",\n",
    "     \"seed_f5_swissCross\",\"seed_nxtals\", \"seed_etaWidth\", \"seed_phiWidth\",\n",
    "    '''\n",
    "    x_mean = tf.constant( \n",
    "        [   6.84241156e-03,  1.62242679e-03,  5.81495577e+01,  2.57215845e+01, \n",
    "            1.00772582e+00,  1.35803461e-02, -4.29317013e-06,  1.71072024e-02,\n",
    "            4.90466869e-01,  5.10511982e+00,  8.82101138e-03,  1.04095965e-02 ]\n",
    "    \n",
    "        )\n",
    "\n",
    "    x_scale = tf.constant(\n",
    "        [   1.31333380e+00, 5.06988411e-01, 9.21157365e+01, 2.98580765e+01, \n",
    "            1.17047757e-01, 1.11969442e-02, 1.86572967e-04, 1.31036359e-02,\n",
    "            4.01511744e-01, 5.67007350e+00, 6.14304203e-03, 7.24808860e-03]       \n",
    "        )\n",
    "    return (X-x_mean)/ x_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_tfr_element(element):\n",
    "    parse_dic = {\n",
    "        'X':      tf.io.FixedLenFeature([], tf.string),\n",
    "        'X_seed': tf.io.FixedLenFeature([], tf.string),\n",
    "        'y':      tf.io.FixedLenFeature([], tf.string),\n",
    "        'n_clusters': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example_message = tf.io.parse_single_example(element, parse_dic)\n",
    "\n",
    "    X = example_message['X']\n",
    "    X_seed = example_message['X_seed']\n",
    "    y = example_message['y']\n",
    "    nclusters = example_message['n_clusters']\n",
    "    \n",
    "    arr_X = tf.io.parse_tensor(X, out_type=tf.float32)\n",
    "    arr_X_seed = tf.io.parse_tensor(X_seed, out_type=tf.float32)\n",
    "    arr_y = tf.io.parse_tensor(y, out_type=tf.float32)\n",
    "    \n",
    "    #https://github.com/tensorflow/tensorflow/issues/24520#issuecomment-577325475\n",
    "    arr_X.set_shape(     tf.TensorShape((None, args.nfeatures)))\n",
    "    arr_X_seed.set_shape(tf.TensorShape((1, args.n_seed_features)))\n",
    "    arr_y.set_shape(     tf.TensorShape((None,)))\n",
    " \n",
    "    return arr_X, arr_X_seed, nclusters, arr_y\n",
    "  \n",
    "def _stack_seed_features(arr_X, arr_X_seed, nclusters, arr_y):\n",
    "    en_clusters = tf.expand_dims(arr_X[:,3], axis=-1)\n",
    "    rescaled_X = scale_features_clusters(arr_X)\n",
    "    rescaled_X_seed = scale_features_seed(arr_X_seed)\n",
    "    X = tf.concat([en_clusters, rescaled_X, tf.broadcast_to(rescaled_X_seed,[nclusters,rescaled_X_seed.shape[1]] )],\n",
    "                  axis=1)\n",
    "    return X,arr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# padding shape\n",
    "ps = ([None,args.nfeatures+args.n_seed_features+1],[None,])\n",
    "\n",
    "# Create datasets from TFRecord files.\n",
    "dataset = tf.data.TFRecordDataset(tf.io.gfile.glob('{}/training-*'.format(data_path)))\n",
    "dataset = dataset.map(_parse_tfr_element,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.map(_stack_seed_features,num_parallel_calls=tf.data.experimental.AUTOTUNE) # deterministic=False in TFv2.3\n",
    "dataset = dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
    "\n",
    "ds_train = dataset.take(args.ntrain).padded_batch(args.batch_size, padded_shapes=ps, drop_remainder=True,padding_values=(-1.,-1.))\n",
    "ds_test = dataset.skip(args.ntrain).take(args.nval).padded_batch(args.batch_size, padded_shapes=ps, drop_remainder=True,padding_values=(-1.,-1.))\n",
    "\n",
    "ds_train_r = ds_train.repeat(args.nepochs)\n",
    "ds_test_r = ds_test.repeat(args.nepochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(15, 26), dtype=float32, numpy=\n",
       " array([[ 2.60765910e+00,  1.00000000e+00,  6.43275212e-03,\n",
       "          3.84712475e-04,  4.96526301e-01,  7.15929449e-01,\n",
       "         -1.17906384e-01, -7.71608949e-01, -5.32518215e-02,\n",
       "         -7.93384075e-01,  2.04237625e-01,  1.32596731e+00,\n",
       "         -1.09502457e-01, -2.92296886e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 2.60415387e+00,  0.00000000e+00, -1.46129057e-01,\n",
       "          1.31661534e+00,  4.95191514e-01,  7.31447101e-01,\n",
       "         -1.55343652e-01, -7.71608949e-01, -5.32518215e-02,\n",
       "         -3.27732670e-03, -2.70530909e-01, -2.62636721e-01,\n",
       "         -1.17125720e-01,  2.26948810e+00,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 1.30552912e+00,  0.00000000e+00,  8.74986768e-01,\n",
       "         -1.03457999e+00,  6.75566378e-04,  6.73607886e-02,\n",
       "         -1.17906384e-01, -2.28345916e-01,  2.26778060e-01,\n",
       "         -2.79621184e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "          1.43512928e+00,  8.99041653e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 1.17204928e+00,  0.00000000e+00,  7.09293306e-01,\n",
       "         -7.40598679e-01, -5.01535162e-02,  1.85571052e-02,\n",
       "         -8.30051824e-02,  2.89435387e-01, -9.97851938e-02,\n",
       "         -2.30185837e-01, -2.48299032e-01,  2.66897947e-01,\n",
       "          1.23014879e+00, -4.27211434e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 6.69278085e-01,  0.00000000e+00, -9.61289585e-01,\n",
       "          3.66828203e-01, -2.41608649e-01, -1.50339916e-01,\n",
       "         -1.17906384e-01, -2.36625612e-01,  4.89866644e-01,\n",
       "          2.18481496e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "         -2.40284532e-01, -3.34946275e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 5.06324172e-01,  0.00000000e+00, -8.56220245e-01,\n",
       "         -1.39368236e+00, -3.03661436e-01, -2.34897271e-01,\n",
       "         -7.08004981e-02,  5.67548051e-02, -5.92480637e-02,\n",
       "         -3.30582321e-01,  2.04237625e-01,  2.66897947e-01,\n",
       "          1.81906009e+00,  1.15479314e+00,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 4.44437206e-01,  0.00000000e+00, -7.64677107e-01,\n",
       "          1.05742228e+00, -3.27227980e-01, -2.67630994e-01,\n",
       "         -1.17906384e-01, -7.71608949e-01, -5.32518215e-02,\n",
       "         -7.93384075e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "         -2.47537524e-01, -3.45782608e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 4.34556514e-01,  0.00000000e+00, -9.65373516e-01,\n",
       "          1.68427515e+00, -3.30990553e-01, -2.68613875e-01,\n",
       "         -1.17906384e-01, -7.71608949e-01, -5.32518215e-02,\n",
       "         -7.93384075e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "         -2.78736651e-01, -3.59800220e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 4.74399537e-01,  0.00000000e+00,  1.68315783e-01,\n",
       "          1.68447936e+00, -3.15818340e-01, -2.72103310e-01,\n",
       "         -1.17906384e-01, -2.35306889e-01,  4.92547542e-01,\n",
       "          2.20975727e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "         -2.47498393e-01, -3.78341287e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 3.53689492e-01,  0.00000000e+00, -1.11399305e+00,\n",
       "         -4.80076760e-01, -3.61784697e-01, -3.07028860e-01,\n",
       "         -1.17906384e-01, -7.71608949e-01, -5.32518215e-02,\n",
       "         -7.93384075e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "         -2.87288666e-01, -3.63677442e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 3.34478617e-01,  0.00000000e+00,  9.53226388e-01,\n",
       "          1.74887121e+00, -3.69100183e-01, -3.46638829e-01,\n",
       "         -1.17906384e-01, -2.53386259e-01,  4.56368685e-01,\n",
       "          1.86780483e-01,  2.04237625e-01,  2.66897947e-01,\n",
       "         -2.79081851e-01, -3.91513884e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 3.35082084e-01,  0.00000000e+00,  1.10114861e+00,\n",
       "         -1.26327097e+00, -3.68870407e-01, -3.48384440e-01,\n",
       "         -1.17906384e-01, -7.71608949e-01, -5.32518215e-02,\n",
       "         -7.93384075e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "         -2.83938825e-01, -3.95440817e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 3.24162126e-01,  0.00000000e+00,  1.72280335e+00,\n",
       "          1.58057702e+00, -3.73028696e-01, -3.60849857e-01,\n",
       "         -1.17906384e-01, -7.71608949e-01, -5.32518215e-02,\n",
       "         -7.93384075e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "         -2.90376514e-01, -4.21537459e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 2.41937235e-01,  0.00000000e+00, -4.51380253e-01,\n",
       "         -1.26346934e+00, -4.04339910e-01, -3.71312201e-01,\n",
       "         -1.17906384e-01,  8.97755027e-01, -2.69740272e+00,\n",
       "          7.85330534e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "         -2.98573762e-01, -3.75607342e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00],\n",
       "        [ 2.83761114e-01,  0.00000000e+00,  9.45855200e-01,\n",
       "          1.37153006e+00, -3.88413399e-01, -3.67960095e-01,\n",
       "         -7.63415620e-02, -2.31836841e-01,  2.23190799e-01,\n",
       "         -2.82922536e-01,  2.04237625e-01, -2.62636721e-01,\n",
       "         -2.97132164e-01, -4.07619238e-01,  9.34330761e-01,\n",
       "         -3.20012611e-03, -6.02957785e-01, -8.14580858e-01,\n",
       "         -6.60058334e-02, -1.21286190e+00,  2.30106749e-02,\n",
       "         -1.30553091e+00,  1.26903677e+00, -1.94903940e-01,\n",
       "         -9.62125242e-01, -1.20681524e+00]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = next(idata)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_run():\n",
    "    previous_runs = os.listdir(args.models_path)\n",
    "    if len(previous_runs) == 0:\n",
    "        run_number = 1\n",
    "    else:\n",
    "        run_number = max([int(s.split('run_')[1]) for s in previous_runs]) + 1\n",
    "    return run_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.lr_decay > 0:\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            args.lr,\n",
    "            decay_steps=2*int(args.ntrain//args.batch_size),\n",
    "            decay_rate=args.lr_decay\n",
    "        )\n",
    "else:\n",
    "    lr_schedule = args.lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    model = DNNSuperCluster(hidden_dim_input=args.hidden_dim_input,hidden_dim_coord=args.hidden_dim_coord,\n",
    "                            hidden_dim_id=args.hidden_dim_id, \n",
    "                            n_layers_input=args.n_layers_input, n_layers_id=args.n_layers_id, n_layers_coord=args.n_layers_coord,\n",
    "                            distance_dim=args.distance_dim, \n",
    "                            num_conv=args.num_conv, convlayer=args.convlayer, dropout=args.dropout)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/ECAL/deepcluster/models/gcn_models_v9/run_08\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(args.models_path):\n",
    "    os.makedirs(args.models_path)\n",
    "\n",
    "name =  'run_{:02}'.format(get_unique_run())\n",
    "\n",
    "outdir = args.models_path + name\n",
    "\n",
    "if os.path.isdir(outdir):\n",
    "    print(\"Output directory exists: {}\".format(outdir), file=sys.stderr)\n",
    "\n",
    "print(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "tb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=outdir, histogram_freq=2, \n",
    "    write_graph=False, \n",
    "    write_images=True,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0,\n",
    ")\n",
    "tb.set_model(model)\n",
    "callbacks += [tb]\n",
    "\n",
    "terminate_cb = tf.keras.callbacks.TerminateOnNaN()\n",
    "callbacks += [terminate_cb]\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=outdir + \"/weights.{epoch:02d}-{val_loss:.6f}.hdf5\",\n",
    "    save_weights_only=True,\n",
    "    verbose=0\n",
    ")\n",
    "cp_callback.set_model(model)\n",
    "callbacks += [cp_callback]\n",
    "\n",
    "loss_fn = my_loss_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [150] vs. [150,20] [Op:Mul] name: mul/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-488c02e4040e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-a2aab601aa33>\u001b[0m in \u001b[0;36mmy_loss_full\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# apply mask on loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_msk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_msk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrue_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#l1 = tf.keras.backend.binary_crossentropy(y_true_msk, y_pred_msk, from_logits=True) * true_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1199\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6120\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6122\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6123\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6124\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [150] vs. [150,20] [Op:Mul] name: mul/"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model.compile(optimizer=args.opt, loss=loss_fn,\n",
    "        metrics=[Precision(),Recall(), energy_resolution_insc,energy_resolution_outsc,\n",
    "                     energy_resolution_insc_log,energy_resolution_outsc_log,])\n",
    "   \n",
    "    for X, y in ds_train:\n",
    "        ypred = model(X)\n",
    "        l = loss_fn(y, ypred)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoh,true_en,true_mask = separate_true(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dnn_super_cluster_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "disctcoords_0 (Dense)        multiple                  1300      \n",
      "_________________________________________________________________\n",
      "disctcoords_1 (Dense)        multiple                  2550      \n",
      "_________________________________________________________________\n",
      "distcoords_final (Dense)     multiple                  1530      \n",
      "_________________________________________________________________\n",
      "distance (Distance)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_0 (Dense)              multiple                  2600      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_1 (Dense)              multiple                  10100     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "input_2 (Dense)              multiple                  10100     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1 (GHConv)               multiple                  30100     \n",
      "_________________________________________________________________\n",
      "id_0 (Dense)                 multiple                  10100     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "id_1 (Dense)                 multiple                  10100     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "out_id (Dense)               multiple                  101       \n",
      "=================================================================\n",
      "Total params: 78,581\n",
      "Trainable params: 78,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 3333 steps, validate for 666 steps\n",
      "Epoch 1/100\n",
      "   1/3333 [..............................] - ETA: 3:36:03"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [150,16] vs. [150,150]\n\t [[node Mul_22 (defined at <ipython-input-54-e2e1e3b2f2b4>:13) ]] [Op:__inference_distributed_function_8735]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node Mul_22:\n loss/output_1_loss/Cast (defined at <ipython-input-10-dab5755b9c1f>:8)\n\nFunction call stack:\ndistributed_function\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e2e1e3b2f2b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntrain\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnval\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                       total_epochs=1)\n\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 397\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    769\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m       \u001b[0;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[1;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[1;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[0;32m-> 1055\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m       \u001b[0;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "if args.load:\n",
    "    #ensure model input size is known\n",
    "    for X, y in ds_train:\n",
    "        model(X)\n",
    "        break\n",
    "\n",
    "    model.load_weights(args.load)\n",
    "if args.nepochs > 0:\n",
    "    ret = model.fit(ds_train_r,\n",
    "        validation_data=ds_test_r, epochs=args.nepochs,\n",
    "        steps_per_epoch=args.ntrain//args.batch_size, validation_steps=args.nval//args.batch_size,\n",
    "        verbose=True,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outdir + \"/args.txt\",'w') as config:\n",
    "    config.write(str(args))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version=2.1.0, CUDA=True, GPU=True, TPU=False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json, os\n",
    "\n",
    "# Tested with TensorFlow 2.1.0\n",
    "print('version={}, CUDA={}, GPU={}, TPU={}'.format(\n",
    "    tf.__version__, tf.test.is_built_with_cuda(),\n",
    "    # GPU attached?\n",
    "    len(tf.config.list_physical_devices('GPU')) > 0,\n",
    "    # TPU accessible? (only works on Colab)\n",
    "    'COLAB_TPU_ADDR' in os.environ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/storage/ECAL/training_data/window_data/electrons/recordio_v1\"\n",
    "models_path = \"/storage/ECAL/deepcluster/models/rnn_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary specifies what \"features\" we want to extract from the\n",
    "# tf.train.Example protos (i.e. what they look like on disk). We only\n",
    "# need the image data \"img_64\" and the \"label\". Both features are tensors\n",
    "# with a fixed length.\n",
    "# You need to specify the correct \"shape\" and \"dtype\" parameters for\n",
    "# these features.\n",
    "feature_spec = {\n",
    "}\n",
    "\n",
    "seed_features = {\n",
    "                    tf.float32: [\"seed_eta\", \"seed_phi\", \"seed_iz\", \"en_seed\", \"et_seed\",  # \"en_seed_calib\", \"et_seed_calib\", \n",
    "                    \"seed_f5_r9\",\"seed_f5_sigmaIetaIeta\", \"seed_f5_sigmaIetaIphi\",\n",
    "                    \"seed_f5_sigmaIphiIphi\",\"seed_f5_swissCross\",\"seed_etaWidth\",\n",
    "                    \"seed_phiWidth\",\"seed_nxtals\"]\n",
    "}\n",
    "\n",
    "# For the moment include the seed as cluster and not the seed variables\n",
    "#for typ,flist in seed_features.items():\n",
    "#    for feat in flist:\n",
    "#        feature_spec[feat] = tf.io.FixedLenFeature(shape=[1], dtype=typ)\n",
    "\n",
    "clusters_features = {   tf.int64 :[\"is_seed\", \"in_scluster\",\"cl_nxtals\"], \n",
    "                     tf.float32: [\"cluster_dphi\",\"en_cluster\",\"et_cluster\",  #\"en_cluster_calib\", \"et_cluster_calib\",\n",
    "                               \"cl_f5_r9\", \"cl_f5_sigmaIetaIeta\", \"cl_f5_sigmaIetaIphi\",\n",
    "                                \"cl_f5_sigmaIphiIphi\",\"cl_f5_swissCross\",\"cl_etaWidth\",\n",
    "                                \"cl_phiWidth\"] }\n",
    "\n",
    "for typ,flist in clusters_features.items():\n",
    "    for feat in flist:\n",
    "        feature_spec[feat] = tf.io.VarLenFeature(dtype=typ)\n",
    "        \n",
    "\n",
    "def parse_example(serialized_example):\n",
    "  # Convert string to tf.train.Example and then extract features/label.\n",
    "    features = tf.io.parse_single_example(serialized_example, feature_spec)\n",
    "    \n",
    "    to_be_stacked = []\n",
    "    for typ, feats in clusters_features.items():\n",
    "        for f in feats:\n",
    "            if f ==\"in_scluster\":continue\n",
    "            if typ == tf.int64:\n",
    "                to_be_stacked.append(tf.sparse.to_dense(tf.cast(features[f], tf.float32)))\n",
    "            else:\n",
    "                to_be_stacked.append(features[f].values)\n",
    "                \n",
    "    a = tf.transpose(tf.stack(to_be_stacked))\n",
    "    label = tf.expand_dims(tf.sparse.to_dense(tf.cast(features[\"in_scluster\"],tf.float32)),1)\n",
    "    \n",
    "    return a, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "n_features = 12\n",
    "steps_per_epoch = 1e6 // batch_size\n",
    "eval_steps_per_epoch = 3e5 // batch_size\n",
    "\n",
    "# Create datasets from TFRecord files.\n",
    "train_ds = tf.data.TFRecordDataset(tf.io.gfile.glob(\n",
    "    '{}/training-*'.format(data_path)))\n",
    "train_ds = train_ds.map(parse_example)\n",
    "train_ds = train_ds.padded_batch(batch_size, padded_shapes=([None,12],[None,1]),\n",
    "                                  padding_values=(0.,0.),drop_remainder=True).repeat()\n",
    "\n",
    "eval_ds = tf.data.TFRecordDataset(tf.io.gfile.glob(\n",
    "    '{}/validation-*'.format(data_path)))\n",
    "eval_ds = eval_ds.map(parse_example)\n",
    "eval_ds = eval_ds.padded_batch(batch_size, padded_shapes=([None,12],[None,1]),\n",
    "                                   padding_values=(0.,0.),drop_remainder=True).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(shape=[None,n_features], batch_size=batch_size)\n",
    "mask = layers.Masking(mask_value=0)(input)\n",
    "\n",
    "\n",
    "#norm = layers.experimental.preprocessing.Normalization()(mask)\n",
    "\n",
    "\n",
    "rnn = layers.LSTM(units=50,activation=\"relu\", return_sequences=True)(mask)\n",
    "\n",
    "dense1 = layers.Dense(50,  activation=\"relu\")\n",
    "dense2 = layers.Dense(1,  activation=\"sigmoid\")\n",
    "\n",
    "output_dense1 = layers.TimeDistributed(dense1)(rnn)\n",
    "output = layers.TimeDistributed(dense2)(output_dense1)\n",
    "\n",
    "\n",
    "model = keras.Model(input, output, name=\"rnn_model1\")\n",
    "\n",
    "model.compile(loss=keras.losses.binary_crossentropy, \n",
    "              optimizer=keras.optimizers.Adam(), metrics=[\"accuracy\", \"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn_model1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_55 (InputLayer)        [(200, None, 12)]         0         \n",
      "_________________________________________________________________\n",
      "masking_26 (Masking)         (200, None, 12)           0         \n",
      "_________________________________________________________________\n",
      "normalization_2 (Normalizati (200, None, 12)           25        \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (200, None, 50)           12600     \n",
      "_________________________________________________________________\n",
      "time_distributed_48 (TimeDis (None, None, 50)          2550      \n",
      "_________________________________________________________________\n",
      "time_distributed_49 (TimeDis (None, None, 1)           51        \n",
      "=================================================================\n",
      "Total params: 15,226\n",
      "Trainable params: 15,201\n",
      "Non-trainable params: 25\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 5000.0 steps, validate for 1500.0 steps\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 552s 110ms/step - loss: 0.0426 - accuracy: 0.9540 - AUC: 0.9616 - val_loss: 0.0391 - val_accuracy: 0.9570 - val_AUC: 0.9685\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 550s 110ms/step - loss: 0.0385 - accuracy: 0.9573 - AUC: 0.9691 - val_loss: 0.0384 - val_accuracy: 0.9576 - val_AUC: 0.9695\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 549s 110ms/step - loss: 0.0378 - accuracy: 0.9580 - AUC: 0.9702 - val_loss: 0.0372 - val_accuracy: 0.9588 - val_AUC: 0.9715\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 551s 110ms/step - loss: 0.0367 - accuracy: 0.9591 - AUC: 0.9719 - val_loss: 0.0362 - val_accuracy: 0.9598 - val_AUC: 0.9731\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 552s 110ms/step - loss: 0.0359 - accuracy: 0.9599 - AUC: 0.9733 - val_loss: 0.0357 - val_accuracy: 0.9602 - val_AUC: 0.9743\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(  train_ds,\n",
    "                    validation_data = eval_ds,\n",
    "                   steps_per_epoch = steps_per_epoch,\n",
    "                   validation_steps = eval_steps_per_epoch,\n",
    "                   epochs=5,\n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0d5331ac18>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(models_path+\"/model_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

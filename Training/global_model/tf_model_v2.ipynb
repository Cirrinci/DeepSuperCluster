{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing setGPU\n",
      "Could not import setGPU, please make sure you configure CUDA_VISIBLE_DEVICES manually\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import glob\n",
    "try:\n",
    "    if not (\"CUDA_VISIBLE_DEVICES\" in os.environ):\n",
    "        os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "        print(\"importing setGPU\")\n",
    "        import setGPU\n",
    "except:\n",
    "    print(\"Could not import setGPU, please make sure you configure CUDA_VISIBLE_DEVICES manually\")\n",
    "    pass\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import pandas\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import io\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import tensorflow as tf\n",
    "from numpy.lib.recfunctions import append_fields\n",
    "\n",
    "import scipy\n",
    "import scipy.special\n",
    "\n",
    "from mpnn import MessagePassing, ReadoutGraph, Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version=2.1.0, CUDA=True, GPU=True, TPU=False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json, os\n",
    "import numpy as np\n",
    "\n",
    "# Tested with TensorFlow 2.1.0\n",
    "print('version={}, CUDA={}, GPU={}, TPU={}'.format(\n",
    "    tf.__version__, tf.test.is_built_with_cuda(),\n",
    "    # GPU attached?\n",
    "    len(tf.config.list_physical_devices('GPU')) > 0,\n",
    "    # TPU accessible? (only works on Colab)\n",
    "    'COLAB_TPU_ADDR' in os.environ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpus= 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    num_gpus = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\"))\n",
    "    print(\"num_gpus=\", num_gpus)\n",
    "    if num_gpus > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(\"gpu:0\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"fallback to CPU\")\n",
    "    strategy = tf.distribute.OneDeviceStrategy(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.experimental_run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(A,B):\n",
    "    na = tf.reduce_sum(tf.square(A), -1)\n",
    "    nb = tf.reduce_sum(tf.square(B), -1)\n",
    " \n",
    "    na = tf.reshape(na, [tf.shape(na)[0], -1, 1])\n",
    "    nb = tf.reshape(nb, [tf.shape(na)[0], 1, -1])\n",
    "    Dsq = tf.clip_by_value(na - 2*tf.linalg.matmul(A, B, transpose_a=False, transpose_b=True) + nb, 1e-12, 1e12)\n",
    "    D = tf.sqrt(Dsq)\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a list of [Nbatch, Nelem, Nfeat] input nodes, computes the dense [Nbatch, Nelem, Nelem] adjacency matrices\n",
    "class Distance(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, dist_shape, *args, **kwargs):\n",
    "        super(Distance, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs1, inputs2):\n",
    "        #compute the pairwise distance matrix between the vectors defined by the first two components of the input array\n",
    "        #inputs1, inputs2: [Nbatch, Nelem, distance_dim] embedded coordinates used for element-to-element distance calculation\n",
    "        D = dist(inputs1, inputs2)\n",
    "      \n",
    "        #adjacency between two elements should be high if the distance is small.\n",
    "        #this is equivalent to radial basis functions. \n",
    "        #self-loops adj_{i,i}=1 are included, as D_{i,i}=0 by construction\n",
    "        adj = tf.math.exp(-1.0*D)\n",
    "\n",
    "        #optionally set the adjacency matrix to 0 for low values in order to make the matrix sparse.\n",
    "        #need to test if this improves the result.\n",
    "        #adj = tf.keras.activations.relu(adj, threshold=0.01)\n",
    "\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_classes):\n",
    "        super(InputEncoding, self).__init__()\n",
    "        self.num_input_classes = num_input_classes\n",
    "        \n",
    "    def call(self, X):\n",
    "        #X: [Nbatch, Nelem, Nfeat] array of all the input detector element feature data\n",
    "\n",
    "        #X[:, :, 0] - categorical index of the element type\n",
    "        Xid = tf.one_hot(tf.cast(X[:, :, 0], tf.int32), self.num_input_classes)\n",
    "\n",
    "        #X[:, :, 1:] - all the other non-categorical features\n",
    "        Xprop = X[:, :, 1:]\n",
    "        return tf.concat([Xid, Xprop], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph Highway network\n",
    "# https://arxiv.org/pdf/2004.04635.pdf\n",
    "#https://github.com/gcucurull/jax-ghnet/blob/master/models.py \n",
    "class GHConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, k, *args, **kwargs):\n",
    "        self.activation = kwargs.pop(\"activation\")\n",
    "        self.hidden_dim = args[0]\n",
    "        self.k = k\n",
    "\n",
    "        super(GHConv, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.W_t = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"w_t\", initializer=\"random_normal\")\n",
    "        self.b_t = self.add_weight(shape=(self.hidden_dim, ), name=\"b_t\", initializer=\"zeros\")\n",
    "        self.W_h = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"w_h\", initializer=\"random_normal\")\n",
    "        self.theta = self.add_weight(shape=(self.hidden_dim, self.hidden_dim), name=\"theta\", initializer=\"random_normal\")\n",
    " \n",
    "    def call(self, x, adj):\n",
    "        #compute the normalization of the adjacency matrix\n",
    "        in_degrees = tf.reduce_sum(adj, axis=-1)\n",
    "        #add epsilon to prevent numerical issues from 1/sqrt(x)\n",
    "        norm = tf.expand_dims(tf.pow(in_degrees + 1e-6, -0.5), -1)\n",
    "        norm_k = tf.pow(norm, self.k)\n",
    "        adj_k = tf.pow(adj, self.k)\n",
    "\n",
    "        f_hom = tf.linalg.matmul(x, self.theta)\n",
    "        f_hom = tf.linalg.matmul(adj_k, f_hom*norm_k)*norm_k\n",
    "\n",
    "        f_het = tf.linalg.matmul(x, self.W_h)\n",
    "        gate = tf.nn.sigmoid(tf.linalg.matmul(x, self.W_t) + self.b_t)\n",
    "        #tf.print(tf.reduce_mean(f_hom), tf.reduce_mean(f_het), tf.reduce_mean(gate))\n",
    "\n",
    "        out = gate*f_hom + (1-gate)*f_het\n",
    "        return out\n",
    "\n",
    "## Simple Graph Conv layer\n",
    "class SGConv(tf.keras.layers.Dense):\n",
    "    def __init__(self, k, *args, **kwargs):\n",
    "        super(SGConv, self).__init__(*args, **kwargs)\n",
    "        self.k = k\n",
    "    \n",
    "    def call(self, inputs, adj):\n",
    "        W = self.weights[0]\n",
    "        b = self.weights[1]\n",
    "\n",
    "        #compute the normalization of the adjacency matrix\n",
    "        in_degrees = tf.reduce_sum(adj, axis=-1)\n",
    "        #add epsilon to prevent numerical issues from 1/sqrt(x)\n",
    "        norm = tf.expand_dims(tf.pow(in_degrees + 1e-6, -0.5), -1)\n",
    "        norm_k = tf.pow(norm, self.k)\n",
    "\n",
    "        support = (tf.linalg.matmul(inputs, W))\n",
    "     \n",
    "        #k-th power of the normalized adjacency matrix is nearly equivalent to k consecutive GCN layers\n",
    "        adj_k = tf.pow(adj, self.k)\n",
    "        out = tf.linalg.matmul(adj_k, support*norm_k)*norm_k\n",
    "\n",
    "        return self.activation(out + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple message passing based on a matrix multiplication\n",
    "class DNNSuperCluster(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, activation=tf.nn.selu, nclass_labels=2, hidden_dim=256, distance_dim=256, num_conv=4, convlayer=\"ghconv\", dropout=0.1):\n",
    "        super(DNNSuperCluster, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.nclass_labels = nclass_labels\n",
    "\n",
    "        #self.enc = InputEncoding(3)\n",
    "        \n",
    "        # layers for distance coordinate extraction\n",
    "        self.layer_distcoords1 = tf.keras.layers.Dense(hidden_dim, activation=activation, name=\"distcoords1\")\n",
    "        self.layer_distcoords2 = tf.keras.layers.Dense(hidden_dim, activation=activation, name=\"distcoords2\")\n",
    "        #self.layer_distcoords3 = tf.keras.layers.Dense(hidden_dim, activation=activation, name=\"distcoords3\")\n",
    "        self.layer_distcoords = tf.keras.layers.Dense(distance_dim, activation=\"linear\", name=\"distcoords\")\n",
    "\n",
    "        # layers for feature extraction \n",
    "        self.layer_input1 = tf.keras.layers.Dense(hidden_dim, activation=activation, name=\"input1\")\n",
    "        self.layer_input1_do = tf.keras.layers.Dropout(dropout)\n",
    "        self.layer_input2 = tf.keras.layers.Dense(hidden_dim, activation=activation, name=\"input2\")\n",
    "        self.layer_input2_do = tf.keras.layers.Dropout(dropout)\n",
    "        self.layer_input3 = tf.keras.layers.Dense(2*hidden_dim, activation=activation, name=\"input3\")\n",
    "        self.layer_input3_do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        self.layer_dist = Distance(distance_dim, name=\"distance\")\n",
    "\n",
    "        # Graph convolutions\n",
    "        if convlayer == \"sgconv\":\n",
    "            self.layer_conv1 = SGConv(num_conv, 2*hidden_dim, activation=activation, name=\"conv1\")\n",
    "            #self.layer_conv2 = SGConv(num_conv, 2*hidden_dim+len(class_labels), activation=activation, name=\"conv2\")\n",
    "        elif convlayer == \"ghconv\":\n",
    "            self.layer_conv1 = GHConv(num_conv, 2*hidden_dim, activation=activation, name=\"conv1\")\n",
    "            #self.layer_conv2 = GHConv(num_conv, 2*hidden_dim+len(class_labels), activation=activation, name=\"conv2\")\n",
    "\n",
    "        # Output layers\n",
    "        self.layer_id1 = tf.keras.layers.Dense(2*hidden_dim, activation=activation, name=\"id1\")\n",
    "        self.layer_id2 = tf.keras.layers.Dense(hidden_dim, activation=activation, name=\"id2\")\n",
    "        #self.layer_id3 = tf.keras.layers.Dense(hidden_dim, activation=activation, name=\"id3\")\n",
    "        self.layer_id = tf.keras.layers.Dense(nclass_labels, activation=\"linear\", name=\"out_id\")\n",
    "        #self.layer_charge = tf.keras.layers.Dense(1, activation=\"linear\", name=\"out_charge\")\n",
    "        \n",
    "        #self.layer_momentum1 = tf.keras.layers.Dense(2*hidden_dim, activation=activation, name=\"momentum1\")\n",
    "        #self.layer_momentum2 = tf.keras.layers.Dense(hidden_dim, activation=activation, name=\"momentum2\")\n",
    "        #self.layer_momentum3 = tf.keras.layers.Dense(hidden_dim, activation=activation, name=\"momentum3\")\n",
    "        #self.layer_momentum = tf.keras.layers.Dense(3, activation=\"linear\", name=\"out_momentum\")\n",
    " \n",
    "    def predict_distancematrix(self, inputs, training=True):\n",
    "\n",
    "        x = self.layer_distcoords1(inputs)\n",
    "        x = self.layer_distcoords2(x)\n",
    "        #x = self.layer_distcoords3(x)\n",
    "        distcoords = self.layer_distcoords(x)\n",
    "\n",
    "        dm = self.layer_dist(distcoords, distcoords)\n",
    "        \n",
    "        # masking if the first element is -1\n",
    "        msk_elem = tf.expand_dims(tf.cast(inputs[:, :, 0] != -1, dtype=tf.float32), -1)\n",
    "        dm = dm*msk_elem\n",
    "\n",
    "        return dm\n",
    "\n",
    "    #@tf.function(input_signature=[tf.TensorSpec(shape=[None, 15], dtype=tf.float32)])\n",
    "    def call(self, inputs, training=True):\n",
    "        X = inputs\n",
    "        msk_input = tf.expand_dims(tf.cast(X[:, :, 0] != -1, tf.float32), -1)\n",
    "\n",
    "        dm = self.predict_distancematrix(X, training=training)\n",
    "\n",
    "        x = self.layer_input1(X)\n",
    "        x = self.layer_input1_do(x, training)\n",
    "        x = self.layer_input2(x)\n",
    "        x = self.layer_input2_do(x, training)\n",
    "        x = self.layer_input3(x)\n",
    "        x = self.layer_input3_do(x, training)\n",
    "        x = self.layer_conv1(x, dm)\n",
    "        x = self.layer_id1(x)\n",
    "        x = self.layer_id2(x)\n",
    "        #x = self.layer_id3(x)\n",
    "        out_id_logits = self.layer_id(x)\n",
    "        \n",
    "        energies = tf.expand_dims(X[:,:,3], axis=-1)\n",
    "        # add the cluster energies in the output, in the future we can add here corrections\n",
    "        output = tf.concat([out_id_logits,energies], axis=-1)\n",
    "        # mask to 0 the padded output\n",
    "        output_masked = output * msk_input\n",
    "        \n",
    "        #return masked output logits and the predicted total energy\n",
    "        return output_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_true(y, nclass_labels):\n",
    "    # one-hot encoding for true label (signal,PU,noise)\n",
    "    # the padded elements have -1 so they are one_hot to (0,0)\n",
    "    y_onehot = tf.one_hot(tf.cast(y[:,1:], tf.int32), nclass_labels)\n",
    "    true_en = y[:,0]\n",
    "    return y_onehot, true_en\n",
    "\n",
    "def true_mask(y):\n",
    "    # mask for elements that should be included in supercluster\n",
    "    in_sc = tf.cast(y[:,1:] == 1., tf.float32)\n",
    "    # number of padding elements\n",
    "    padded = tf.reduce_sum(tf.cast(y[:,1:] == -1., tf.float32), axis=-1)\n",
    "    return in_sc, padded\n",
    "    \n",
    "    \n",
    "def separate_pred(ypred):\n",
    "    ens = ypred[:,:,2]\n",
    "    ypred_onehot = ypred[:,:,:2]\n",
    "    # 0 not include in energy sum, 1 include in energy sum\n",
    "    # masked elements have pred_id=0 so they do not enter in the energy sum\n",
    "    pred_id = tf.cast(tf.argmax(ypred_onehot, axis=-1), tf.float32)\n",
    "    # predicted total energy\n",
    "    pred_en =  tf.reduce_sum( ens * pred_id, axis=-1)\n",
    "    # one-hot encoding for true label (signal,PU,noise)\n",
    "    return ypred_onehot, pred_en, pred_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_unreduced(true, pred):\n",
    "    return tf.math.pow(true-pred,2)\n",
    "\n",
    "def msle_unreduced(true, pred):\n",
    "    return tf.math.pow(tf.math.log(tf.math.abs(true) + 1.0) - tf.math.log(tf.math.abs(pred) + 1.0), 2)\n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def my_loss_full(y_true, y_pred):\n",
    "    y_true_onehot, true_en = separate_true(y_true, args.nclass_labels)\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    # since the padded y_true is -1 -> it gives [0,0] when it is onehot. The ypred for batched is [0,0] so the loss\n",
    "    # is automatically 0 for padded samples\n",
    "    l1 = tf.nn.softmax_cross_entropy_with_logits(y_true_onehot, y_pred_onehot)\n",
    "    \n",
    "    # true energy loss\n",
    "    mask_outsc = tf.cast(true_en == 0., tf.float32)\n",
    "    mask_insc = tf.cast(true_en != 0., tf.float32)\n",
    "    n_outsc = tf.reduce_sum(mask_outsc)\n",
    "    n_insc = tf.reduce_sum(mask_insc)\n",
    "    \n",
    "    l2_en = mse_unreduced(true_en, pred_en)\n",
    "    \n",
    "    # separate mean resolution for windows with Caloparticle or not\n",
    "    l2_en_outsc = tf.reduce_sum(l2_en * mask_outsc) / n_outsc\n",
    "    l2_en_insc = tf.reduce_sum(l2_en * mask_insc) / n_insc\n",
    "    \n",
    "    ltot = 1e4*tf.reduce_mean(l1) + 20* l2_en_insc + 10* l2_en_outsc\n",
    "    \n",
    "    return ltot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_resolution_outsc(y_true, y_pred):\n",
    "    y_true_onehot, true_en = separate_true(y_true, args.nclass_labels)\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_outsc = tf.cast(true_en == 0., tf.float32)\n",
    "    n_outsc = tf.reduce_sum(mask_outsc)\n",
    "    \n",
    "    return tf.reduce_sum(mse_unreduced(true_en, pred_en)*mask_outsc) / n_outsc\n",
    "\n",
    "def energy_resolution_insc(y_true, y_pred):\n",
    "    y_true_onehot, true_en = separate_true(y_true, args.nclass_labels)\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    mask_insc = tf.cast(true_en != 0., tf.float32)\n",
    "    n_insc = tf.reduce_sum(mask_insc)\n",
    "    \n",
    "    return tf.reduce_sum(mse_unreduced(true_en, pred_en)*mask_insc) / n_insc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpfn_metrics(y_true, y_pred):\n",
    "    y_true_mask, n_padded = true_mask(y_true)\n",
    "    y_false_mask = (tf.cast(y_true_mask == 0., tf.float32))\n",
    "    \n",
    "    # pred_id contains the last n_padded elements to 0 that will be always True negatives\n",
    "    y_pred_onehot, pred_en, pred_id = separate_pred(y_pred)\n",
    "    \n",
    "    n_pos = tf.reduce_sum(y_true_mask, axis=-1)\n",
    "    n_neg = tf.reduce_sum(y_false_mask, axis=-1) - n_padded\n",
    "    \n",
    "    n_tot = n_neg + n_pos\n",
    "    \n",
    "    true_pos = tf.reduce_sum(pred_id * y_true_mask, axis=-1)\n",
    "    false_neg = n_pos - true_pos\n",
    "    \n",
    "    false_pos = tf.reduce_sum(pred_id * y_false_mask, axis=-1)\n",
    "    true_neg = n_neg - false_pos\n",
    "    \n",
    "    return n_tot, true_pos, false_neg, false_pos, true_neg, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp,tn,fp,fn):\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def recall(tp,tn,fp,fn):\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def accuracy(tp,tn,fp,fn):\n",
    "    return (tp+tn)/(tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Precision(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='precision', **kwargs):\n",
    "        super(Precision, self).__init__(name=name, **kwargs)\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        n_tot, true_pos, false_neg, false_pos, true_neg = get_tpfn_metrics(y_true, y_pred)\n",
    "        self.tp.assign_add(tf.reduce_sum(true_pos))\n",
    "        self.fp.assign_add(tf.reduce_sum(false_pos))\n",
    "\n",
    "    def result(self):\n",
    "        return self.tp / (self.tp + self.fp)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fp.assign(0)\n",
    "        \n",
    "class Recall(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='recall', **kwargs):\n",
    "        super(Recall, self).__init__(name=name, **kwargs)\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        n_tot, true_pos, false_neg, false_pos, true_neg = get_tpfn_metrics(y_true, y_pred)\n",
    "        self.tp.assign_add(tf.reduce_sum(true_pos))\n",
    "        self.fn.assign_add(tf.reduce_sum(false_neg))\n",
    "\n",
    "    def result(self):\n",
    "        return self.tp / (self.tp + self.fn)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fn.assign(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/storage/ECAL/training_data/window_data/electrons/recordio_v3\"\n",
    "models_path = \"/storage/ECAL/deepcluster/models/gcn_models_v5/\"\n",
    "\n",
    "#rain_steps_per_epoch = \n",
    "#eval_steps_per_epoch = 3e5 // batch_size\n",
    "from collections import namedtuple\n",
    "Args = namedtuple('args', [ 'models_path', 'load','nepochs','ntrain','nval','nfeatures',\n",
    "                            'n_seed_features','batch_size','lr_decay','lr',\n",
    "                            'nhidden','distance_dim','num_conv','dropout','convlayer',\n",
    "                           'nclass_labels', 'opt'])\n",
    "\n",
    "args = Args( \n",
    "        models_path = models_path,\n",
    "        load = False,\n",
    "        nepochs = 300,\n",
    "        ntrain = 700000,\n",
    "        nval = 100000,\n",
    "        batch_size = 65,\n",
    "        nfeatures = 13,\n",
    "        n_seed_features = 13,\n",
    "        lr_decay = 0.96,\n",
    "        lr = 0.001,\n",
    "        nhidden = 128,\n",
    "        distance_dim = 128,\n",
    "        num_conv = 2,\n",
    "        dropout = 0.1,\n",
    "        convlayer = 'ghconv',\n",
    "        nclass_labels=2,\n",
    "        opt='adam'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_tfr_element(element):\n",
    "    parse_dic = {\n",
    "        'X':      tf.io.FixedLenFeature([], tf.string),\n",
    "        'X_seed': tf.io.FixedLenFeature([], tf.string),\n",
    "        'y':      tf.io.FixedLenFeature([], tf.string),\n",
    "        'n_clusters': tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example_message = tf.io.parse_single_example(element, parse_dic)\n",
    "\n",
    "    X = example_message['X']\n",
    "    X_seed = example_message['X_seed']\n",
    "    y = example_message['y']\n",
    "    nclusters = example_message['n_clusters']\n",
    "    \n",
    "    arr_X = tf.io.parse_tensor(X, out_type=tf.float32)\n",
    "    arr_X_seed = tf.io.parse_tensor(X_seed, out_type=tf.float32)\n",
    "    arr_y = tf.io.parse_tensor(y, out_type=tf.float32)\n",
    "    \n",
    "    #https://github.com/tensorflow/tensorflow/issues/24520#issuecomment-577325475\n",
    "    arr_X.set_shape(     tf.TensorShape((None, args.nfeatures)))\n",
    "    arr_X_seed.set_shape(tf.TensorShape((1, args.n_seed_features)))\n",
    "    arr_y.set_shape(     tf.TensorShape((None,)))\n",
    " \n",
    "    return arr_X, arr_X_seed, nclusters, arr_y\n",
    "  \n",
    "def _stack_seed_features(arr_X, arr_X_seed, nclusters, arr_y):\n",
    "    X = tf.concat([arr_X,tf.broadcast_to(arr_X_seed,[nclusters,arr_X_seed.shape[1]] )],\n",
    "                  axis=1)\n",
    "    return X,arr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding shape\n",
    "ps = ([None,args.nfeatures+args.n_seed_features],[None,])\n",
    "\n",
    "# Create datasets from TFRecord files.\n",
    "dataset = tf.data.TFRecordDataset(tf.io.gfile.glob('{}/training-*'.format(data_path)))\n",
    "dataset = dataset.map(_parse_tfr_element,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.map(_stack_seed_features,num_parallel_calls=tf.data.experimental.AUTOTUNE) # deterministic=False in TFv2.3\n",
    "dataset = dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
    "\n",
    "ds_train = dataset.take(args.ntrain).padded_batch(args.batch_size, padded_shapes=ps, drop_remainder=True,padding_values=(-1.,-1.))\n",
    "ds_test = dataset.skip(args.ntrain).take(args.nval).padded_batch(args.batch_size, padded_shapes=ps, drop_remainder=True,padding_values=(-1.,-1.))\n",
    "\n",
    "ds_train_r = ds_train.repeat(args.nepochs)\n",
    "ds_test_r = ds_test.repeat(args.nepochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = iter(ds_train_r)\n",
    "d = next(idata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(65, 20), dtype=float32, numpy=\n",
       "array([[ 86.267654,   1.      ,   1.      , ...,  -1.      ,  -1.      ,\n",
       "         -1.      ],\n",
       "       [  0.      ,   0.      ,  -1.      , ...,  -1.      ,  -1.      ,\n",
       "         -1.      ],\n",
       "       [  0.      ,   0.      ,   0.      , ...,  -1.      ,  -1.      ,\n",
       "         -1.      ],\n",
       "       ...,\n",
       "       [  0.      ,   0.      ,   0.      , ...,  -1.      ,  -1.      ,\n",
       "         -1.      ],\n",
       "       [105.83477 ,   1.      ,   1.      , ...,  -1.      ,  -1.      ,\n",
       "         -1.      ],\n",
       "       [ 77.395004,   1.      ,   0.      , ...,  -1.      ,  -1.      ,\n",
       "         -1.      ]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_run():\n",
    "    previous_runs = os.listdir(args.models_path)\n",
    "    if len(previous_runs) == 0:\n",
    "        run_number = 1\n",
    "    else:\n",
    "        run_number = max([int(s.split('run_')[1]) for s in previous_runs]) + 1\n",
    "    return run_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.lr_decay > 0:\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            args.lr,\n",
    "            decay_steps=3*int(args.ntrain//args.batch_size),\n",
    "            decay_rate=args.lr_decay\n",
    "        )\n",
    "else:\n",
    "    lr_schedule = args.lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    model = DNNSuperCluster(hidden_dim=args.nhidden, nclass_labels=args.nclass_labels, distance_dim=args.distance_dim, \n",
    "                            num_conv=args.num_conv, convlayer=args.convlayer, dropout=args.dropout)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/ECAL/deepcluster/models/gcn_models_v5/run_21\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(args.models_path):\n",
    "    os.makedirs(args.models_path)\n",
    "\n",
    "name =  'run_{:02}'.format(get_unique_run())\n",
    "\n",
    "outdir = args.models_path + name\n",
    "\n",
    "if os.path.isdir(outdir):\n",
    "    print(\"Output directory exists: {}\".format(outdir), file=sys.stderr)\n",
    "\n",
    "print(outdir)\n",
    "\n",
    "with open(outdir + \"/args.txt\",'w') as config:\n",
    "    config.write(str(args))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "tb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=outdir, histogram_freq=5, \n",
    "    write_graph=True, \n",
    "    write_images=True,\n",
    "    update_freq='epoch',\n",
    "    embeddings_freq = 3 ,\n",
    "    #profile_batch=(10,90),\n",
    "    profile_batch=0,\n",
    ")\n",
    "tb.set_model(model)\n",
    "callbacks += [tb]\n",
    "\n",
    "terminate_cb = tf.keras.callbacks.TerminateOnNaN()\n",
    "callbacks += [terminate_cb]\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=outdir + \"/weights.{epoch:02d}-{val_loss:.6f}.hdf5\",\n",
    "    save_weights_only=True,\n",
    "    verbose=0\n",
    ")\n",
    "cp_callback.set_model(model)\n",
    "callbacks += [cp_callback]\n",
    "\n",
    "loss_fn = my_loss_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model.compile(optimizer=args.opt, loss=loss_fn,\n",
    "        metrics=[Precision(),Recall(), energy_resolution_insc,energy_resolution_outsc])\n",
    "\n",
    "    for X, y in ds_train:\n",
    "        ypred = model(X)\n",
    "        l = loss_fn(y, ypred)\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoh,true_en = separate_true(y, args.nclass_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dnn_super_cluster\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distcoords1 (Dense)          multiple                  3456      \n",
      "_________________________________________________________________\n",
      "distcoords2 (Dense)          multiple                  16512     \n",
      "_________________________________________________________________\n",
      "distcoords (Dense)           multiple                  16512     \n",
      "_________________________________________________________________\n",
      "input1 (Dense)               multiple                  3456      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "input2 (Dense)               multiple                  16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "input3 (Dense)               multiple                  33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "distance (Distance)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1 (GHConv)               multiple                  196864    \n",
      "_________________________________________________________________\n",
      "id1 (Dense)                  multiple                  65792     \n",
      "_________________________________________________________________\n",
      "id2 (Dense)                  multiple                  32896     \n",
      "_________________________________________________________________\n",
      "out_id (Dense)               multiple                  258       \n",
      "=================================================================\n",
      "Total params: 385,282\n",
      "Trainable params: 385,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 10769 steps, validate for 1538 steps\n",
      "Epoch 1/300\n",
      "10769/10769 [==============================] - 389s 36ms/step - loss: 1113.1295 - precision: 0.9148 - recall: 0.7541 - energy_resolution_insc: 24.9060 - energy_resolution_outsc: 17.4900 - val_loss: 714.4939 - val_precision: 0.9364 - val_recall: 0.8195 - val_energy_resolution_insc: 12.1999 - val_energy_resolution_outsc: 15.0496\n",
      "Epoch 2/300\n",
      "10769/10769 [==============================] - 359s 33ms/step - loss: 770.9626 - precision: 0.9209 - recall: 0.8312 - energy_resolution_insc: 14.5087 - energy_resolution_outsc: 16.7083 - val_loss: 749.2466 - val_precision: 0.9346 - val_recall: 0.8090 - val_energy_resolution_insc: 14.0684 - val_energy_resolution_outsc: 13.4225\n",
      "Epoch 3/300\n",
      "10769/10769 [==============================] - 364s 34ms/step - loss: 708.2441 - precision: 0.9261 - recall: 0.8522 - energy_resolution_insc: 13.0578 - energy_resolution_outsc: 16.3651 - val_loss: 635.4963 - val_precision: 0.9592 - val_recall: 0.8327 - val_energy_resolution_insc: 11.0073 - val_energy_resolution_outsc: 13.7282\n",
      "Epoch 4/300\n",
      "10769/10769 [==============================] - 363s 34ms/step - loss: 731.3053 - precision: 0.9291 - recall: 0.8640 - energy_resolution_insc: 15.2532 - energy_resolution_outsc: 16.1372 - val_loss: 582.9932 - val_precision: 0.9536 - val_recall: 0.8512 - val_energy_resolution_insc: 8.6918 - val_energy_resolution_outsc: 15.6052\n",
      "Epoch 5/300\n",
      "10769/10769 [==============================] - 362s 34ms/step - loss: 672.1702 - precision: 0.9297 - recall: 0.8668 - energy_resolution_insc: 12.5544 - energy_resolution_outsc: 15.8842 - val_loss: 563.6096 - val_precision: 0.9199 - val_recall: 0.8898 - val_energy_resolution_insc: 7.2070 - val_energy_resolution_outsc: 17.3543\n",
      "Epoch 6/300\n",
      "10769/10769 [==============================] - 366s 34ms/step - loss: 908.9034 - precision: 0.9295 - recall: 0.8614 - energy_resolution_insc: 23.3203 - energy_resolution_outsc: 15.8219 - val_loss: 570.6702 - val_precision: 0.9441 - val_recall: 0.8604 - val_energy_resolution_insc: 7.5821 - val_energy_resolution_outsc: 16.5032\n",
      "Epoch 7/300\n",
      "10769/10769 [==============================] - 360s 33ms/step - loss: 720.8288 - precision: 0.9299 - recall: 0.8707 - energy_resolution_insc: 15.3825 - energy_resolution_outsc: 15.7076 - val_loss: 568.7187 - val_precision: 0.9440 - val_recall: 0.8642 - val_energy_resolution_insc: 7.9466 - val_energy_resolution_outsc: 15.9358\n",
      "Epoch 8/300\n",
      "10769/10769 [==============================] - 366s 34ms/step - loss: 735.2751 - precision: 0.9300 - recall: 0.8709 - energy_resolution_insc: 16.0901 - energy_resolution_outsc: 15.7018 - val_loss: 579.8888 - val_precision: 0.9470 - val_recall: 0.8612 - val_energy_resolution_insc: 8.7510 - val_energy_resolution_outsc: 15.4640\n",
      "Epoch 9/300\n",
      "10769/10769 [==============================] - 365s 34ms/step - loss: 652.9414 - precision: 0.9305 - recall: 0.8710 - energy_resolution_insc: 11.8880 - energy_resolution_outsc: 15.8790 - val_loss: 582.3402 - val_precision: 0.9201 - val_recall: 0.8782 - val_energy_resolution_insc: 7.3652 - val_energy_resolution_outsc: 17.7049\n",
      "Epoch 10/300\n",
      "10769/10769 [==============================] - 362s 34ms/step - loss: 727.3899 - precision: 0.9314 - recall: 0.8716 - energy_resolution_insc: 15.7172 - energy_resolution_outsc: 15.9722 - val_loss: 547.8574 - val_precision: 0.9417 - val_recall: 0.8704 - val_energy_resolution_insc: 6.9052 - val_energy_resolution_outsc: 16.5739\n",
      "Epoch 11/300\n",
      "10769/10769 [==============================] - 367s 34ms/step - loss: 690.2025 - precision: 0.9312 - recall: 0.8716 - energy_resolution_insc: 13.8344 - energy_resolution_outsc: 15.9689 - val_loss: 562.8960 - val_precision: 0.9378 - val_recall: 0.8705 - val_energy_resolution_insc: 7.7216 - val_energy_resolution_outsc: 15.8325\n",
      "Epoch 12/300\n",
      "10769/10769 [==============================] - 363s 34ms/step - loss: 1671.6948 - precision: 0.9300 - recall: 0.8660 - energy_resolution_insc: 62.4677 - energy_resolution_outsc: 15.9839 - val_loss: 598.9819 - val_precision: 0.9515 - val_recall: 0.8510 - val_energy_resolution_insc: 9.2576 - val_energy_resolution_outsc: 15.2848\n",
      "Epoch 13/300\n",
      "10769/10769 [==============================] - 365s 34ms/step - loss: 794.7342 - precision: 0.9309 - recall: 0.8718 - energy_resolution_insc: 19.0368 - energy_resolution_outsc: 15.9088 - val_loss: 545.0394 - val_precision: 0.9382 - val_recall: 0.8769 - val_energy_resolution_insc: 7.2536 - val_energy_resolution_outsc: 16.1231\n",
      "Epoch 14/300\n",
      "10769/10769 [==============================] - 366s 34ms/step - loss: 641.5521 - precision: 0.9297 - recall: 0.8663 - energy_resolution_insc: 11.0091 - energy_resolution_outsc: 15.9650 - val_loss: 569.0676 - val_precision: 0.9411 - val_recall: 0.8655 - val_energy_resolution_insc: 7.8283 - val_energy_resolution_outsc: 16.2681\n",
      "Epoch 15/300\n",
      "10769/10769 [==============================] - 363s 34ms/step - loss: 714.2280 - precision: 0.9306 - recall: 0.8621 - energy_resolution_insc: 14.4172 - energy_resolution_outsc: 15.4410 - val_loss: 544.8881 - val_precision: 0.9354 - val_recall: 0.8758 - val_energy_resolution_insc: 6.4060 - val_energy_resolution_outsc: 17.2777\n",
      "Epoch 16/300\n",
      "10769/10769 [==============================] - 366s 34ms/step - loss: 658.0239 - precision: 0.9297 - recall: 0.8725 - energy_resolution_insc: 12.0162 - energy_resolution_outsc: 15.8520 - val_loss: 642.3008 - val_precision: 0.9565 - val_recall: 0.8181 - val_energy_resolution_insc: 10.2863 - val_energy_resolution_outsc: 14.8095\n",
      "Epoch 17/300\n",
      "10769/10769 [==============================] - 363s 34ms/step - loss: 872.6307 - precision: 0.9274 - recall: 0.8633 - energy_resolution_insc: 21.5061 - energy_resolution_outsc: 15.8203 - val_loss: 566.0251 - val_precision: 0.9356 - val_recall: 0.8797 - val_energy_resolution_insc: 8.6473 - val_energy_resolution_outsc: 15.4665\n",
      "Epoch 18/300\n",
      "10769/10769 [==============================] - 365s 34ms/step - loss: 1644.6622 - precision: 0.9239 - recall: 0.8270 - energy_resolution_insc: 43.8673 - energy_resolution_outsc: 16.0300 - val_loss: 586.1760 - val_precision: 0.9143 - val_recall: 0.8815 - val_energy_resolution_insc: 7.1632 - val_energy_resolution_outsc: 17.7807\n",
      "Epoch 19/300\n",
      "10769/10769 [==============================] - 368s 34ms/step - loss: 998.0570 - precision: 0.9279 - recall: 0.8562 - energy_resolution_insc: 28.2229 - energy_resolution_outsc: 15.7839 - val_loss: 626.6650 - val_precision: 0.9518 - val_recall: 0.8475 - val_energy_resolution_insc: 11.8436 - val_energy_resolution_outsc: 13.1935\n",
      "Epoch 20/300\n",
      " 2482/10769 [=====>........................] - ETA: 3:02 - loss: 4521.1873 - precision: 0.8272 - recall: 0.8717 - energy_resolution_insc: 23.0212 - energy_resolution_outsc: 20.0968"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m       \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[0;34m(self, step, mode, size)\u001b[0m\n\u001b[1;32m    787\u001b[0m           self.callbacks._call_batch_hook(\n\u001b[0;32m--> 788\u001b[0;31m               mode, 'end', step, batch_logs)\n\u001b[0m\u001b[1;32m    789\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    238\u001b[0m       \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m       \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1696\u001b[0m       elif (not self._is_tracing and\n\u001b[0;32m-> 1697\u001b[0;31m             math_ops.equal(train_batches, self._profile_batch - 1)):\n\u001b[0m\u001b[1;32m   1698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mequal\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1304\u001b[0m   \"\"\"\n\u001b[0;32m-> 1305\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mequal\u001b[0;34m(x, y, incompatible_shape_error, name)\u001b[0m\n\u001b[1;32m   3222\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"incompatible_shape_error\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3223\u001b[0;31m         incompatible_shape_error)\n\u001b[0m\u001b[1;32m   3224\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e2e1e3b2f2b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntrain\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnval\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                       total_epochs=1)\n\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 397\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    769\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m       \u001b[0;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[1;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[1;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[0;32m-> 1055\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m       \u001b[0;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    }
   ],
   "source": [
    "if args.load:\n",
    "    #ensure model input size is known\n",
    "    for X, y in ds_train:\n",
    "        model(X)\n",
    "        break\n",
    "\n",
    "    model.load_weights(args.load)\n",
    "if args.nepochs > 0:\n",
    "    ret = model.fit(ds_train_r,\n",
    "        validation_data=ds_test_r, epochs=args.nepochs,\n",
    "        steps_per_epoch=args.ntrain//args.batch_size, validation_steps=args.nval//args.batch_size,\n",
    "        verbose=True,\n",
    "        callbacks=callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"args(models_path='/storage/ECAL/deepcluster/models/gcn_models_v5/', load=False, nepochs=300, ntrain=700000, nval=100000, nfeatures=13, n_seed_features=13, batch_size=65, lr_decay=0.96, lr=0.001, nhidden=128, distance_dim=128, num_conv=2, dropout=0.1, convlayer='ghconv', nclass_labels=2, opt='adam')\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
